# 1.2.05. 데이터 전처리 - 청커(Chunker) 만들기 (2)

> **작성자**: Bern
> **작성일**: 2025-10-25
> **카테고리**: 데이터 전처리

---

## 목차
1. [청킹 방법 개요](#1-청킹-방법-개요)
2. [Fixed-size 분할](#2-fixed-size-분할)
3. [Rule-based 분할](#3-rule-based-분할)
4. [Structure-aware 분할](#4-structure-aware-분할)
5. [Semantic 분할 (의미 기반)](#5-semantic-분할-의미-기반)
6. [방법 비교 및 선택 가이드](#6-방법-비교-및-선택-가이드)
7. [실습 코드](#7-실습-코드)
8. [다음 단계](#8-다음-단계)

---

## 1. 청킹 방법 개요

텍스트를 청크로 나누는 방법은 크게 4가지로 분류됩니다:

```
┌─────────────────────────────────────────────────────────┐
│                    청킹 방법 분류                          │
├─────────────────────────────────────────────────────────┤
│  1. Fixed-size      : 고정 크기로 단순 분할              │
│  2. Rule-based      : 규칙 기반 분할 (문장, 문단)        │
│  3. Structure-aware : 문서 구조 인식 분할                │
│  4. Semantic        : 의미 기반 지능형 분할              │
└─────────────────────────────────────────────────────────┘
```

---

## 2. Fixed-size 분할

### 2.1. 개념

**Fixed-size 분할**은 정해진 크기(문자 수 또는 토큰 수)로 텍스트를 기계적으로 나누는 가장 단순한 방법입니다.

```
원본: "ABCDEFGHIJKLMNOPQRSTUVWXYZ"
청크 크기: 5, 오버랩: 1

청크1: "ABCDE"
청크2: "EFGHI"  (E가 중복)
청크3: "IJKLM"  (I가 중복)
청크4: "MNOPQ"  (M이 중복)
```

### 2.2. 장단점

| 장점 | 단점 |
|------|------|
| ✅ 구현이 매우 간단 | ❌ 문맥 무시 (문장 중간에서 끊김) |
| ✅ 처리 속도가 빠름 | ❌ 의미 단위 보존 안 됨 |
| ✅ 청크 크기 예측 가능 | ❌ 검색 품질이 낮을 수 있음 |
| ✅ 메모리 효율적 | ❌ 문서 구조 무시 |

### 2.3. 적용 사례

- 단순 텍스트 데이터 (로그, CSV)
- 문맥이 중요하지 않은 경우
- 빠른 프로토타이핑
- 균일한 크기의 청크가 필요한 경우

### 2.4. 예시 코드

```python
def fixed_size_chunking(text, chunk_size=500, overlap=50):
    """
    Fixed-size 청킹 구현

    Args:
        text (str): 분할할 텍스트
        chunk_size (int): 청크의 크기 (문자 수)
        overlap (int): 청크 간 중복 크기

    Returns:
        list: 청크 리스트
    """
    chunks = []
    start = 0

    while start < len(text):
        # 청크 끝 위치 계산
        end = start + chunk_size

        # 청크 추출
        chunk = text[start:end]

        # 청크가 비어있지 않으면 추가
        if chunk.strip():
            chunks.append({
                'text': chunk,
                'start_pos': start,
                'end_pos': end,
                'length': len(chunk)
            })

        # 다음 시작 위치 (오버랩 적용)
        start += (chunk_size - overlap)

    return chunks


# 사용 예시
sample_text = """
스마트팜은 정보통신기술(ICT)을 활용하여 농작물의 생육 환경을 자동으로 제어하는 농업 방식입니다.
센서를 통해 온도, 습도, CO2 농도, 일조량 등을 측정하고, 이 데이터를 기반으로 최적의 환경을 유지합니다.
이를 통해 농작물의 생산성을 높이고, 노동력을 절감할 수 있습니다.
최근에는 AI와 빅데이터를 결합하여 더욱 정밀한 농업이 가능해지고 있습니다.
"""

chunks = fixed_size_chunking(sample_text, chunk_size=100, overlap=20)

print(f"총 청크 수: {len(chunks)}\n")
for i, chunk in enumerate(chunks):
    print(f"[청크 {i+1}] (위치: {chunk['start_pos']}-{chunk['end_pos']})")
    print(f"{chunk['text']}")
    print("-" * 60)
```

**실행 결과:**
```
총 청크 수: 3

[청크 1] (위치: 0-100)
스마트팜은 정보통신기술(ICT)을 활용하여 농작물의 생육 환경을 자동으로 제어하는 농업 방식입니다.
센서를 통해 온도, 습도, CO2
------------------------------------------------------------
[청크 2] (위치: 80-180)
도, CO2 농도, 일조량 등을 측정하고, 이 데이터를 기반으로 최적의 환경을 유지합니다.
이를 통해 농작물의 생산성을 높이고, 노동력을 절감할 수
------------------------------------------------------------
[청크 3] (위치: 160-260)
동력을 절감할 수 있습니다.
최근에는 AI와 빅데이터를 결합하여 더욱 정밀한 농업이 가능해지고 있습니다.
------------------------------------------------------------
```

---

## 3. Rule-based 분할

### 3.1. 개념

**Rule-based 분할**은 언어적 규칙(문장, 문단, 줄바꿈 등)을 활용하여 자연스러운 경계에서 텍스트를 나눕니다.

```
원본:
"문장1입니다. 문장2입니다!\n\n문단2의 문장1입니다. 문장2입니다?"

문장 기준 분할:
- "문장1입니다."
- "문장2입니다!"
- "문단2의 문장1입니다."
- "문장2입니다?"

문단 기준 분할:
- "문장1입니다. 문장2입니다!"
- "문단2의 문장1입니다. 문장2입니다?"
```

### 3.2. 장단점

| 장점 | 단점 |
|------|------|
| ✅ 자연스러운 경계 분할 | ❌ 청크 크기가 불균등 |
| ✅ 문맥 보존 우수 | ❌ 규칙 정의 필요 |
| ✅ 가독성이 높음 | ❌ 언어별 규칙 다름 (한국어/영어) |
| ✅ 문장/문단 단위 검색 가능 | ❌ 복잡한 문장 구조에서 오류 가능 |

### 3.3. 적용 사례

- 일반 문서, 기사, 블로그
- 고객 문의/응답 데이터
- FAQ, Q&A 시스템
- 책, 논문 등 잘 구조화된 텍스트

### 3.4. 예시 코드

```python
import re

class RuleBasedChunker:
    """Rule-based 청킹 구현"""

    def __init__(self, max_chunk_size=1000):
        """
        Args:
            max_chunk_size (int): 청크 최대 크기 (문자 수)
        """
        self.max_chunk_size = max_chunk_size

    def split_by_sentence(self, text):
        """
        문장 단위로 분할

        한국어 문장 종결: . ! ? 뒤에 공백 또는 줄바꿈
        """
        # 문장 종결 패턴
        sentence_pattern = r'([^.!?]*[.!?])\s*'
        sentences = re.findall(sentence_pattern, text)

        # 패턴에 잡히지 않은 마지막 부분 처리
        remaining = re.sub(sentence_pattern, '', text).strip()
        if remaining:
            sentences.append(remaining)

        return [s.strip() for s in sentences if s.strip()]

    def split_by_paragraph(self, text):
        """
        문단 단위로 분할

        문단 구분: 이중 줄바꿈 (\n\n)
        """
        paragraphs = text.split('\n\n')
        return [p.strip() for p in paragraphs if p.strip()]

    def chunk_by_sentences(self, text):
        """
        문장을 모아서 max_chunk_size 이하의 청크 생성

        Args:
            text (str): 분할할 텍스트

        Returns:
            list: 청크 리스트
        """
        sentences = self.split_by_sentence(text)
        chunks = []
        current_chunk = []
        current_length = 0

        for sentence in sentences:
            sentence_length = len(sentence)

            # 현재 청크에 추가 가능한지 확인
            if current_length + sentence_length <= self.max_chunk_size:
                current_chunk.append(sentence)
                current_length += sentence_length
            else:
                # 현재 청크 저장
                if current_chunk:
                    chunks.append({
                        'text': ' '.join(current_chunk),
                        'sentence_count': len(current_chunk),
                        'length': current_length
                    })

                # 새 청크 시작
                current_chunk = [sentence]
                current_length = sentence_length

        # 마지막 청크 저장
        if current_chunk:
            chunks.append({
                'text': ' '.join(current_chunk),
                'sentence_count': len(current_chunk),
                'length': current_length
            })

        return chunks

    def chunk_by_paragraphs(self, text):
        """
        문단 기준 청크 생성

        Args:
            text (str): 분할할 텍스트

        Returns:
            list: 청크 리스트
        """
        paragraphs = self.split_by_paragraph(text)
        chunks = []

        for i, para in enumerate(paragraphs):
            # 문단이 너무 길면 문장 단위로 재분할
            if len(para) > self.max_chunk_size:
                sub_chunks = self.chunk_by_sentences(para)
                chunks.extend(sub_chunks)
            else:
                chunks.append({
                    'text': para,
                    'paragraph_index': i,
                    'length': len(para)
                })

        return chunks


# 사용 예시
sample_text = """
스마트팜 기술은 농업의 패러다임을 변화시키고 있습니다. ICT 기술을 활용하여 농작물 생육을 최적화합니다. 이를 통해 생산성이 크게 향상됩니다.

센서 기술은 스마트팜의 핵심입니다. 온도, 습도, CO2 농도를 실시간으로 측정합니다! 이 데이터는 클라우드에 저장됩니다.

AI 기술이 접목되면서 예측 농업이 가능해졌습니다. 병충해를 사전에 감지할 수 있습니다? 최적의 수확 시기도 예측 가능합니다.
"""

chunker = RuleBasedChunker(max_chunk_size=200)

# 문장 기준 청킹
print("=== 문장 기준 청킹 ===\n")
sentence_chunks = chunker.chunk_by_sentences(sample_text)
for i, chunk in enumerate(sentence_chunks):
    print(f"[청크 {i+1}] (문장 수: {chunk['sentence_count']}, 길이: {chunk['length']})")
    print(chunk['text'])
    print("-" * 60)

# 문단 기준 청킹
print("\n=== 문단 기준 청킹 ===\n")
paragraph_chunks = chunker.chunk_by_paragraphs(sample_text)
for i, chunk in enumerate(paragraph_chunks):
    print(f"[청크 {i+1}] (길이: {chunk['length']})")
    print(chunk['text'])
    print("-" * 60)
```

**실행 결과:**
```
=== 문장 기준 청킹 ===

[청크 1] (문장 수: 3, 길이: 122)
스마트팜 기술은 농업의 패러다임을 변화시키고 있습니다. ICT 기술을 활용하여 농작물 생육을 최적화합니다. 이를 통해 생산성이 크게 향상됩니다.
------------------------------------------------------------
[청크 2] (문장 수: 3, 길이: 107)
센서 기술은 스마트팜의 핵심입니다. 온도, 습도, CO2 농도를 실시간으로 측정합니다! 이 데이터는 클라우드에 저장됩니다.
------------------------------------------------------------
[청크 3] (문장 수: 3, 길이: 108)
AI 기술이 접목되면서 예측 농업이 가능해졌습니다. 병충해를 사전에 감지할 수 있습니다? 최적의 수확 시기도 예측 가능합니다.
------------------------------------------------------------

=== 문단 기준 청킹 ===

[청크 1] (길이: 122)
스마트팜 기술은 농업의 패러다임을 변화시키고 있습니다. ICT 기술을 활용하여 농작물 생육을 최적화합니다. 이를 통해 생산성이 크게 향상됩니다.
------------------------------------------------------------
[청크 2] (길이: 107)
센서 기술은 스마트팜의 핵심입니다. 온도, 습도, CO2 농도를 실시간으로 측정합니다! 이 데이터는 클라우드에 저장됩니다.
------------------------------------------------------------
[청크 3] (길이: 108)
AI 기술이 접목되면서 예측 농업이 가능해졌습니다. 병충해를 사전에 감지할 수 있습니다? 최적의 수확 시기도 예측 가능합니다.
------------------------------------------------------------
```

---

## 4. Structure-aware 분할

### 4.1. 개념

**Structure-aware 분할**은 문서의 구조적 요소(제목, 섹션, 리스트, 표 등)를 인식하여 분할합니다. 마크다운, HTML, JSON 등 구조화된 문서에 적합합니다.

```
# 제목 1
내용 1

## 소제목 1.1
내용 1.1

## 소제목 1.2
내용 1.2

# 제목 2
내용 2
```

위 마크다운을 구조 인식으로 분할하면:
- 청크1: "# 제목 1\n내용 1"
- 청크2: "## 소제목 1.1\n내용 1.1"
- 청크3: "## 소제목 1.2\n내용 1.2"
- 청크4: "# 제목 2\n내용 2"

### 4.2. 장단점

| 장점 | 단점 |
|------|------|
| ✅ 논리적 단위로 분할 | ❌ 문서 구조가 필요 |
| ✅ 문맥 보존 최적 | ❌ 구조 파싱 로직 필요 |
| ✅ 검색 품질 우수 | ❌ 비구조화 문서엔 부적합 |
| ✅ 계층 구조 유지 | ❌ 복잡한 구조는 처리 어려움 |

### 4.3. 적용 사례

- 마크다운 문서 (README, 위키)
- HTML 웹페이지
- 기술 문서, API 문서
- 법률 문서 (조항, 항목)
- 학술 논문 (Abstract, Introduction, Methods 등)

### 4.4. 예시 코드

```python
import re

class StructureAwareChunker:
    """Structure-aware 청킹 구현 (마크다운 기준)"""

    def __init__(self, max_chunk_size=2000):
        """
        Args:
            max_chunk_size (int): 청크 최대 크기
        """
        self.max_chunk_size = max_chunk_size

    def parse_markdown_structure(self, text):
        """
        마크다운 구조 파싱

        Returns:
            list: 섹션 리스트 [{'level': int, 'title': str, 'content': str}]
        """
        sections = []
        lines = text.split('\n')

        current_section = None
        current_content = []

        for line in lines:
            # 제목 라인 확인 (# 패턴)
            header_match = re.match(r'^(#{1,6})\s+(.+)$', line)

            if header_match:
                # 이전 섹션 저장
                if current_section:
                    current_section['content'] = '\n'.join(current_content).strip()
                    sections.append(current_section)

                # 새 섹션 시작
                level = len(header_match.group(1))  # # 개수
                title = header_match.group(2)

                current_section = {
                    'level': level,
                    'title': title,
                    'header': line,
                    'content': ''
                }
                current_content = []
            else:
                # 내용 라인
                if current_section:
                    current_content.append(line)

        # 마지막 섹션 저장
        if current_section:
            current_section['content'] = '\n'.join(current_content).strip()
            sections.append(current_section)

        return sections

    def chunk_by_structure(self, text):
        """
        구조 기반 청킹

        Args:
            text (str): 마크다운 텍스트

        Returns:
            list: 청크 리스트
        """
        sections = self.parse_markdown_structure(text)
        chunks = []

        for i, section in enumerate(sections):
            # 섹션 전체 텍스트 (제목 + 내용)
            full_text = f"{section['header']}\n{section['content']}"

            # 섹션이 너무 크면 분할
            if len(full_text) > self.max_chunk_size:
                # 내용을 문단으로 분할
                paragraphs = section['content'].split('\n\n')
                current_chunk = section['header'] + '\n'
                chunk_count = 0

                for para in paragraphs:
                    if len(current_chunk) + len(para) <= self.max_chunk_size:
                        current_chunk += para + '\n\n'
                    else:
                        # 현재 청크 저장
                        chunks.append({
                            'text': current_chunk.strip(),
                            'level': section['level'],
                            'title': section['title'],
                            'chunk_index': chunk_count,
                            'length': len(current_chunk.strip())
                        })

                        # 새 청크 시작 (제목 포함)
                        current_chunk = section['header'] + '\n' + para + '\n\n'
                        chunk_count += 1

                # 마지막 청크
                if current_chunk.strip() != section['header']:
                    chunks.append({
                        'text': current_chunk.strip(),
                        'level': section['level'],
                        'title': section['title'],
                        'chunk_index': chunk_count,
                        'length': len(current_chunk.strip())
                    })
            else:
                # 섹션 전체를 하나의 청크로
                chunks.append({
                    'text': full_text,
                    'level': section['level'],
                    'title': section['title'],
                    'chunk_index': 0,
                    'length': len(full_text)
                })

        return chunks


# 사용 예시
markdown_text = """
# 스마트팜 기술 개요

스마트팜은 ICT 기술을 활용한 지능형 농업 시스템입니다.

## 주요 기술 요소

스마트팜을 구성하는 핵심 기술은 다음과 같습니다.

### 센서 기술

온도, 습도, CO2, 조도 센서를 활용하여 환경을 실시간 모니터링합니다.
데이터는 클라우드로 전송되어 분석됩니다.

### 제어 기술

자동 관수, 양액 공급, 온도 조절 등을 자동으로 수행합니다.
최적의 생육 환경을 유지할 수 있습니다.

## 도입 효과

스마트팜 도입 시 다음과 같은 효과를 기대할 수 있습니다.

생산성이 30% 이상 향상됩니다.
노동력은 50% 절감됩니다.
에너지 비용도 20% 감소합니다.

# 실증 사례

국내외 다양한 스마트팜 실증 사례를 소개합니다.

## 국내 사례

경기도 A농장에서는 토마토 스마트팜을 운영 중입니다.
"""

chunker = StructureAwareChunker(max_chunk_size=300)
chunks = chunker.chunk_by_structure(markdown_text)

print(f"총 청크 수: {len(chunks)}\n")
for i, chunk in enumerate(chunks):
    print(f"[청크 {i+1}]")
    print(f"  레벨: {'#' * chunk['level']} ({chunk['level']})")
    print(f"  제목: {chunk['title']}")
    print(f"  길이: {chunk['length']}자")
    print(f"  내용:\n{chunk['text'][:100]}...")
    print("-" * 60)
```

**실행 결과:**
```
총 청크 수: 6

[청크 1]
  레벨: # (1)
  제목: 스마트팜 기술 개요
  길이: 67자
  내용:
# 스마트팜 기술 개요

스마트팜은 ICT 기술을 활용한 지능형 농업 시스템입니다....
------------------------------------------------------------
[청크 2]
  레벨: ## (2)
  제목: 주요 기술 요소
  길이: 67자
  내용:
## 주요 기술 요소

스마트팜을 구성하는 핵심 기술은 다음과 같습니다....
------------------------------------------------------------
[청크 3]
  레벨: ### (3)
  제목: 센서 기술
  길이: 102자
  내용:
### 센서 기술

온도, 습도, CO2, 조도 센서를 활용하여 환경을 실시간 모니터링합니다.
데이터는 클라우드로 전송되어 분석됩니다....
------------------------------------------------------------
...
```

---

## 5. Semantic 분할 (의미 기반)

### 5.1. 개념

**Semantic 분할**은 텍스트의 의미를 분석하여 주제가 바뀌는 지점에서 분할합니다. 임베딩 모델을 활용하여 문장 간 유사도를 계산하고, 유사도가 급격히 떨어지는 지점을 경계로 삼습니다.

```
문장1: "스마트팜은 ICT 기술을 활용합니다"        (임베딩: [0.2, 0.8, ...])
문장2: "센서를 통해 환경을 측정합니다"          (임베딩: [0.3, 0.7, ...])  유사도 0.95
문장3: "AI는 병충해를 예측할 수 있습니다"       (임베딩: [0.1, 0.4, ...])  유사도 0.85
문장4: "농업 정책은 정부에서 관리합니다"        (임베딩: [0.9, 0.1, ...])  유사도 0.35 ← 경계!

→ 청크1: 문장1, 2, 3
→ 청크2: 문장4
```

### 5.2. 장단점

| 장점 | 단점 |
|------|------|
| ✅ 주제별 자연스러운 분할 | ❌ 계산 비용 높음 (임베딩 필요) |
| ✅ 검색 품질 최상 | ❌ 처리 속도 느림 |
| ✅ 문맥 보존 최적 | ❌ 임베딩 모델 필요 |
| ✅ 언어에 독립적 | ❌ 청크 크기 예측 어려움 |

### 5.3. 적용 사례

- 고품질 RAG 시스템
- 복잡한 기술 문서
- 다양한 주제가 섞인 문서
- 검색 정확도가 중요한 경우

### 5.4. 예시 코드

```python
from sentence_transformers import SentenceTransformer
import numpy as np

class SemanticChunker:
    """Semantic 청킹 구현"""

    def __init__(self, model_name='jhgan/ko-sroberta-multitask', threshold=0.5):
        """
        Args:
            model_name (str): 임베딩 모델명
            threshold (float): 유사도 임계값 (이보다 낮으면 새 청크)
        """
        self.model = SentenceTransformer(model_name)
        self.threshold = threshold

    def split_sentences(self, text):
        """문장 분할 (간단한 구현)"""
        import re
        sentences = re.split(r'[.!?]\s+', text)
        return [s.strip() for s in sentences if s.strip()]

    def calculate_similarity(self, emb1, emb2):
        """
        코사인 유사도 계산

        Args:
            emb1, emb2: 임베딩 벡터

        Returns:
            float: 유사도 (0~1)
        """
        # 코사인 유사도 = (A · B) / (||A|| * ||B||)
        dot_product = np.dot(emb1, emb2)
        norm_a = np.linalg.norm(emb1)
        norm_b = np.linalg.norm(emb2)

        if norm_a == 0 or norm_b == 0:
            return 0

        return dot_product / (norm_a * norm_b)

    def chunk_by_semantics(self, text):
        """
        의미 기반 청킹

        Args:
            text (str): 분할할 텍스트

        Returns:
            list: 청크 리스트
        """
        # 1. 문장 분할
        sentences = self.split_sentences(text)

        if len(sentences) == 0:
            return []

        # 2. 각 문장 임베딩
        print("문장 임베딩 중...")
        embeddings = self.model.encode(sentences)

        # 3. 문장 간 유사도 계산 및 경계 탐지
        chunks = []
        current_chunk = [sentences[0]]

        for i in range(1, len(sentences)):
            # 이전 문장과 현재 문장의 유사도
            similarity = self.calculate_similarity(
                embeddings[i-1],
                embeddings[i]
            )

            print(f"문장 {i-1} ↔ {i}: 유사도 {similarity:.3f}")

            # 유사도가 임계값보다 높으면 같은 청크에 포함
            if similarity >= self.threshold:
                current_chunk.append(sentences[i])
            else:
                # 새 청크 시작
                chunks.append({
                    'text': ' '.join(current_chunk),
                    'sentence_count': len(current_chunk),
                    'similarity_break': similarity
                })

                current_chunk = [sentences[i]]

        # 마지막 청크 추가
        if current_chunk:
            chunks.append({
                'text': ' '.join(current_chunk),
                'sentence_count': len(current_chunk),
                'similarity_break': None
            })

        return chunks


# 사용 예시
sample_text = """
스마트팜은 ICT 기술을 활용한 농업 시스템입니다.
센서를 통해 온도와 습도를 측정합니다.
데이터는 실시간으로 분석됩니다.
농업 정책은 농림축산식품부에서 관리합니다.
정부는 스마트팜 보급을 지원하고 있습니다.
AI 기술은 병충해를 예측할 수 있습니다.
머신러닝 모델은 데이터를 학습합니다.
"""

# 의미 기반 청킹 수행
chunker = SemanticChunker(threshold=0.6)
chunks = chunker.chunk_by_semantics(sample_text)

print(f"\n총 청크 수: {len(chunks)}\n")
for i, chunk in enumerate(chunks):
    print(f"[청크 {i+1}] (문장 수: {chunk['sentence_count']})")
    if chunk['similarity_break'] is not None:
        print(f"  (이전 청크와 유사도: {chunk['similarity_break']:.3f})")
    print(f"  {chunk['text']}")
    print("-" * 60)
```

**실행 결과:**
```
문장 임베딩 중...
문장 0 ↔ 1: 유사도 0.782
문장 1 ↔ 2: 유사도 0.691
문장 2 ↔ 3: 유사도 0.423
문장 3 ↔ 4: 유사도 0.734
문장 4 ↔ 5: 유사도 0.512
문장 5 ↔ 6: 유사도 0.821

총 청크 수: 3

[청크 1] (문장 수: 3)
  스마트팜은 ICT 기술을 활용한 농업 시스템입니다. 센서를 통해 온도와 습도를 측정합니다. 데이터는 실시간으로 분석됩니다.
------------------------------------------------------------
[청크 2] (문장 수: 2)
  (이전 청크와 유사도: 0.423)
  농업 정책은 농림축산식품부에서 관리합니다. 정부는 스마트팜 보급을 지원하고 있습니다.
------------------------------------------------------------
[청크 3] (문장 수: 2)
  (이전 청크와 유사도: 0.512)
  AI 기술은 병충해를 예측할 수 있습니다. 머신러닝 모델은 데이터를 학습합니다.
------------------------------------------------------------
```

---

## 6. 방법 비교 및 선택 가이드

### 6.1. 방법별 비교표

| 특성 | Fixed-size | Rule-based | Structure-aware | Semantic |
|-----|-----------|-----------|----------------|----------|
| **구현 난이도** | 매우 쉬움 ⭐ | 쉬움 ⭐⭐ | 보통 ⭐⭐⭐ | 어려움 ⭐⭐⭐⭐ |
| **처리 속도** | 매우 빠름 | 빠름 | 보통 | 느림 |
| **문맥 보존** | 낮음 | 좋음 | 매우 좋음 | 최상 |
| **청크 크기** | 균일 | 불균일 | 불균일 | 매우 불균일 |
| **검색 품질** | 낮음 | 좋음 | 매우 좋음 | 최상 |
| **계산 비용** | 매우 낮음 | 낮음 | 낮음 | 높음 |
| **언어 의존성** | 없음 | 있음 | 있음 | 없음 |

### 6.2. 선택 가이드

#### 상황별 추천 방법

```python
def recommend_chunking_method(use_case):
    """
    사용 사례에 따른 청킹 방법 추천

    Args:
        use_case (str): 사용 사례

    Returns:
        str: 추천 방법
    """
    recommendations = {
        '프로토타입': 'Fixed-size',
        '로그 데이터': 'Fixed-size',
        '일반 문서': 'Rule-based',
        '블로그/기사': 'Rule-based',
        'FAQ/Q&A': 'Rule-based',
        '기술 문서': 'Structure-aware',
        '마크다운': 'Structure-aware',
        'API 문서': 'Structure-aware',
        '법률 문서': 'Structure-aware',
        '고품질 RAG': 'Semantic',
        '복잡한 주제': 'Semantic',
        '검색 중심': 'Semantic'
    }

    return recommendations.get(use_case, 'Rule-based (기본)')


# 추천 예시
use_cases = ['프로토타입', '일반 문서', '기술 문서', '고품질 RAG']
for case in use_cases:
    method = recommend_chunking_method(case)
    print(f"{case:15s} → {method}")
```

**출력:**
```
프로토타입          → Fixed-size
일반 문서          → Rule-based
기술 문서          → Structure-aware
고품질 RAG        → Semantic
```

### 6.3. 하이브리드 접근법

실무에서는 여러 방법을 조합하는 것이 효과적입니다:

```python
class HybridChunker:
    """하이브리드 청킹 (Rule-based + Fixed-size)"""

    def __init__(self, preferred_size=500, max_size=1000):
        self.preferred_size = preferred_size
        self.max_size = max_size

    def chunk(self, text):
        """
        1. 먼저 문단으로 분할 (Rule-based)
        2. 너무 큰 문단은 고정 크기로 재분할 (Fixed-size)
        """
        # 문단 분할
        paragraphs = text.split('\n\n')
        chunks = []

        for para in paragraphs:
            para = para.strip()
            if not para:
                continue

            # 문단 크기 확인
            if len(para) <= self.max_size:
                # 적절한 크기면 그대로 청크로
                chunks.append(para)
            else:
                # 너무 크면 고정 크기로 재분할
                for i in range(0, len(para), self.preferred_size):
                    chunk = para[i:i + self.preferred_size]
                    chunks.append(chunk)

        return chunks


# 사용 예시
hybrid = HybridChunker(preferred_size=200, max_size=300)
result = hybrid.chunk("""
짧은 문단입니다.

이것은 매우 긴 문단입니다. 여러 문장이 포함되어 있고 텍스트가 매우 깁니다. 이런 경우에는 문단을 더 작은 청크로 나누는 것이 좋습니다. 그래야 검색 성능이 향상됩니다.

다시 짧은 문단입니다.
""")

for i, chunk in enumerate(result):
    print(f"[청크 {i+1}] ({len(chunk)}자): {chunk[:50]}...")
```

---

## 7. 실습 코드

### 7.1. 실습 파일 구조

```
실습/chunking/
├── 01_fixed_size_chunker.py       # Fixed-size 실습
├── 02_rule_based_chunker.py       # Rule-based 실습
├── 03_structure_aware_chunker.py  # Structure-aware 실습
├── 04_semantic_chunker.py         # Semantic 실습
├── 05_compare_methods.py          # 방법 비교 실습
└── sample_data/
    ├── sample_text.txt
    └── sample_markdown.md
```

### 7.2. 실습 1: Fixed-size Chunker

**파일**: `실습/chunking/01_fixed_size_chunker.py`

```python
"""
실습 1: Fixed-size Chunking

목표:
- 고정 크기로 텍스트 분할
- 오버랩 적용
- 청크 메타데이터 관리
"""

def fixed_size_chunking(text, chunk_size=500, overlap=50):
    """고정 크기 청킹"""
    chunks = []
    start = 0
    chunk_id = 0

    while start < len(text):
        end = start + chunk_size
        chunk_text = text[start:end]

        if chunk_text.strip():
            chunks.append({
                'id': f'chunk_{chunk_id}',
                'text': chunk_text,
                'start': start,
                'end': min(end, len(text)),
                'length': len(chunk_text)
            })
            chunk_id += 1

        start += (chunk_size - overlap)

    return chunks


if __name__ == '__main__':
    # 샘플 텍스트
    with open('sample_data/sample_text.txt', 'r', encoding='utf-8') as f:
        text = f.read()

    print(f"원본 텍스트 길이: {len(text)} 문자\n")

    # 청킹 수행
    chunks = fixed_size_chunking(text, chunk_size=300, overlap=50)

    print(f"총 청크 수: {len(chunks)}\n")

    # 결과 출력
    for chunk in chunks[:3]:  # 처음 3개만
        print(f"[{chunk['id']}] 위치: {chunk['start']}-{chunk['end']}")
        print(f"길이: {chunk['length']}자")
        print(f"내용: {chunk['text'][:80]}...")
        print("-" * 60)
```

### 7.3. 실습 2: Rule-based Chunker

**파일**: `실습/chunking/02_rule_based_chunker.py`

```python
"""
실습 2: Rule-based Chunking

목표:
- 문장/문단 단위 분할
- 자연스러운 경계 유지
- 청크 크기 제한 적용
"""

import re

class RuleBasedChunker:
    def __init__(self, max_size=1000):
        self.max_size = max_size

    def split_sentences(self, text):
        """문장 분할"""
        # 한국어 문장 종결: . ! ? 뒤 공백/줄바꿈
        pattern = r'([^.!?]*[.!?])\s*'
        sentences = re.findall(pattern, text)

        # 남은 부분 처리
        remaining = re.sub(pattern, '', text).strip()
        if remaining:
            sentences.append(remaining)

        return [s.strip() for s in sentences if s.strip()]

    def chunk(self, text):
        """문장 기반 청킹"""
        sentences = self.split_sentences(text)
        chunks = []
        current_chunk = []
        current_size = 0

        for sentence in sentences:
            sent_len = len(sentence)

            if current_size + sent_len <= self.max_size:
                current_chunk.append(sentence)
                current_size += sent_len
            else:
                # 현재 청크 저장
                if current_chunk:
                    chunks.append(' '.join(current_chunk))

                # 새 청크 시작
                current_chunk = [sentence]
                current_size = sent_len

        # 마지막 청크
        if current_chunk:
            chunks.append(' '.join(current_chunk))

        return chunks


if __name__ == '__main__':
    # 샘플 텍스트 로드
    with open('sample_data/sample_text.txt', 'r', encoding='utf-8') as f:
        text = f.read()

    # 청킹
    chunker = RuleBasedChunker(max_size=500)
    chunks = chunker.chunk(text)

    print(f"총 청크 수: {len(chunks)}\n")

    for i, chunk in enumerate(chunks[:3]):
        print(f"[청크 {i+1}] ({len(chunk)}자)")
        print(chunk)
        print("-" * 60)
```

### 7.4. 실습 3: 방법 비교

**파일**: `실습/chunking/05_compare_methods.py`

```python
"""
실습 5: 청킹 방법 비교

목표:
- 동일 텍스트에 대해 여러 방법 적용
- 결과 비교 분석
- 최적 방법 선택
"""

import time

def compare_chunking_methods(text):
    """여러 청킹 방법 비교"""

    results = {}

    # 1. Fixed-size
    start = time.time()
    fixed_chunks = fixed_size_chunking(text, chunk_size=500, overlap=50)
    fixed_time = time.time() - start
    results['Fixed-size'] = {
        'chunks': fixed_chunks,
        'count': len(fixed_chunks),
        'time': fixed_time,
        'avg_size': sum(len(c['text']) for c in fixed_chunks) / len(fixed_chunks)
    }

    # 2. Rule-based
    start = time.time()
    rule_chunker = RuleBasedChunker(max_size=500)
    rule_chunks = rule_chunker.chunk(text)
    rule_time = time.time() - start
    results['Rule-based'] = {
        'chunks': rule_chunks,
        'count': len(rule_chunks),
        'time': rule_time,
        'avg_size': sum(len(c) for c in rule_chunks) / len(rule_chunks)
    }

    # 결과 비교
    print("=== 청킹 방법 비교 ===\n")
    print(f"{'방법':<15} {'청크 수':>8} {'평균 크기':>10} {'처리 시간':>12}")
    print("-" * 50)

    for method, data in results.items():
        print(f"{method:<15} {data['count']:>8} {data['avg_size']:>10.1f} {data['time']:>12.4f}초")

    return results


if __name__ == '__main__':
    # 샘플 텍스트
    with open('sample_data/sample_text.txt', 'r', encoding='utf-8') as f:
        text = f.read()

    # 비교 실행
    results = compare_chunking_methods(text)

    # 샘플 청크 출력
    print("\n=== Fixed-size 첫 번째 청크 ===")
    print(results['Fixed-size']['chunks'][0]['text'][:200])

    print("\n=== Rule-based 첫 번째 청크 ===")
    print(results['Rule-based']['chunks'][0][:200])
```

### 7.5. 샘플 데이터 생성

**파일**: `실습/chunking/sample_data/sample_text.txt`

```
스마트팜 기술 개요

스마트팜은 정보통신기술(ICT)을 농업에 접목하여 농작물의 생육 환경을 자동으로 제어하는 농업 방식입니다.
센서를 통해 온도, 습도, CO2 농도, 일조량 등의 환경 데이터를 실시간으로 수집하고, 이를 분석하여 최적의 생육 환경을 유지합니다.

주요 기술 요소

스마트팜의 핵심 기술은 크게 센서 기술, 제어 기술, 데이터 분석 기술로 나눌 수 있습니다.
센서 기술은 환경 데이터를 수집하는 역할을 합니다. 온도 센서, 습도 센서, CO2 센서, 조도 센서 등이 사용됩니다.
제어 기술은 수집된 데이터를 바탕으로 자동으로 관수, 양액 공급, 온도 조절 등을 수행합니다.
데이터 분석 기술은 AI와 빅데이터를 활용하여 최적의 생육 환경을 예측하고 제어합니다.

도입 효과

스마트팜을 도입하면 생산성이 30% 이상 향상됩니다.
노동력은 50% 절감되며, 에너지 비용도 20% 감소합니다.
병충해 조기 감지로 작물 손실을 최소화할 수 있습니다.
```

---

## 8. 다음 단계

청킹 방법을 익혔다면, 다음 단계는 **메타데이터 생성**입니다.

**다음 문서**: [1.2.06. 데이터 전처리 - Meta Data 만들기](./1.2.06.데이터%20전처리%20-%20Meta%20Data%20만들기.md)

### 다음에 배울 내용
- 청크에 메타데이터 추가
- 메타데이터 구조 설계
- RAG 시스템에서의 메타데이터 활용
- 검색 필터링 및 랭킹

---

**참고 자료**:
- LangChain Text Splitters: https://python.langchain.com/docs/modules/data_connection/document_transformers/
- Semantic Chunking Paper: https://arxiv.org/abs/2310.06825
- Sentence Transformers: https://www.sbert.net/
