# 1.2.07. 임베딩 모델 활용하여 벡터화 시키기

> **학습 목표**: 임베딩 모델의 개념을 이해하고, Hugging Face 모델을 활용하여 텍스트를 벡터로 변환하는 방법을 습득합니다.

---

## 목차

1. [임베딩 모델이란](#1-임베딩-모델이란)
2. [벡터화란](#2-벡터화란)
3. [Hugging Face 모델 활용](#3-hugging-face-모델-활용)
4. [주의점](#4-주의점)
5. [실전 예시](#5-실전-예시)
6. [성능 최적화](#6-성능-최적화)

---

## 1. 임베딩 모델이란

### 1.1 임베딩(Embedding)의 정의

**임베딩(Embedding)**은 텍스트, 이미지 등의 데이터를 **고정된 길이의 숫자 벡터로 변환**하는 기술입니다.

```
텍스트: "벼 수확량이 증가했다"
    ↓ (임베딩 모델)
벡터: [0.23, -0.45, 0.12, ..., 0.89]  ← 768차원 벡터
```

### 1.2 왜 임베딩이 필요한가?

컴퓨터는 텍스트를 직접 이해하지 못합니다. 숫자로 변환해야 합니다.

#### 전통적인 방법의 한계

**1. One-Hot Encoding (원-핫 인코딩)**
```python
# 단어마다 하나의 인덱스만 1, 나머지는 0
vocabulary = ["벼", "수확", "증가", "감소"]

"벼"    → [1, 0, 0, 0]
"수확"  → [0, 1, 0, 0]
"증가"  → [0, 0, 1, 0]
"감소"  → [0, 0, 0, 1]
```

**문제점:**
- ❌ 단어 간 유사도를 표현할 수 없음
- ❌ "증가"와 "감소"가 반대 의미인지 모름
- ❌ 어휘 크기만큼 벡터 차원 증가 (100만 단어 = 100만 차원!)

**2. TF-IDF**
```python
# 단어 빈도 기반
"벼 수확량이 증가" → [0.5, 0.3, 0.8, ...]
```

**문제점:**
- ❌ 문맥을 고려하지 않음
- ❌ "사과" (과일)와 "사과" (사과하다)를 구분 못함
- ❌ 의미적 유사성 파악 불가

#### 임베딩의 장점

```python
# 임베딩: 의미가 비슷한 단어는 벡터 공간에서 가까움

"증가" → [0.8, 0.6, -0.2, ...]
"상승" → [0.7, 0.5, -0.1, ...]  # "증가"와 가까움 (유사도 높음)
"감소" → [-0.8, -0.6, 0.2, ...] # "증가"와 멈 (유사도 낮음)

# 코사인 유사도
similarity("증가", "상승") = 0.92  # 매우 유사
similarity("증가", "감소") = -0.85 # 반대 의미
```

**장점:**
- ✅ 의미적 유사성을 벡터 거리로 표현
- ✅ 고정된 차원 (보통 384, 768, 1024차원)
- ✅ 문맥을 고려한 표현
- ✅ 검색, 분류, 추천에 활용 가능

### 1.3 임베딩 모델의 종류

| 모델 유형 | 설명 | 예시 |
|----------|------|------|
| **Word Embedding** | 단어 단위 벡터화 | Word2Vec, GloVe, FastText |
| **Sentence Embedding** | 문장 단위 벡터화 | Sentence-BERT, SimCSE |
| **Document Embedding** | 문서 단위 벡터화 | Doc2Vec, Sentence-BERT |
| **Multilingual Embedding** | 다국어 지원 | mBERT, XLM-RoBERTa |

### 1.4 임베딩 모델의 작동 원리

```
┌─────────────────────────────────────────────────────┐
│                 임베딩 모델 구조                     │
├─────────────────────────────────────────────────────┤
│                                                     │
│  입력 텍스트: "2024년 벼 수확량이 15% 증가했다"      │
│       ↓                                             │
│  [토큰화]                                           │
│  ["2024년", "벼", "수확량", "이", "15%", ...]       │
│       ↓                                             │
│  [임베딩 레이어]                                    │
│  각 토큰을 초기 벡터로 변환                          │
│       ↓                                             │
│  [트랜스포머 레이어들]                              │
│  문맥을 고려하여 벡터 정제                           │
│  (Self-Attention으로 단어 간 관계 파악)             │
│       ↓                                             │
│  [풀링(Pooling)]                                    │
│  - Mean Pooling: 모든 토큰 벡터의 평균              │
│  - CLS Token: 특별 토큰의 벡터 사용                 │
│       ↓                                             │
│  출력 벡터: [0.23, -0.45, 0.12, ..., 0.89]          │
│  (768차원)                                          │
│                                                     │
└─────────────────────────────────────────────────────┘
```

---

## 2. 벡터화란

### 2.1 벡터화의 개념

**벡터화(Vectorization)**는 데이터를 수치형 벡터로 변환하는 과정입니다.

```python
# 1차원 벡터 (스칼라)
score = 85

# 2차원 벡터
point = [x, y] = [3, 5]

# 고차원 벡터 (임베딩)
text_embedding = [0.23, -0.45, 0.12, ..., 0.89]  # 768차원
```

### 2.2 벡터 공간과 유사도

벡터화의 핵심은 **의미적으로 유사한 텍스트는 벡터 공간에서 가깝게 위치**한다는 점입니다.

```
벡터 공간 (2D 시각화)
    ↑
    │    "증가" ●
    │           ● "상승"
    │
    │
    │
─────┼──────────────────→
    │
    │        ● "감소"
    │      ● "하락"
    │
```

실제로는 768차원 공간이지만, 원리는 같습니다.

### 2.3 벡터 간 유사도 측정

#### 코사인 유사도 (Cosine Similarity)

가장 많이 사용되는 유사도 측정 방법:

```python
import numpy as np

def cosine_similarity(vec1, vec2):
    """
    코사인 유사도 계산

    값 범위: -1 ~ 1
    - 1: 완전히 같은 방향 (매우 유사)
    - 0: 직각 (무관함)
    - -1: 정반대 방향 (반대 의미)
    """
    dot_product = np.dot(vec1, vec2)
    norm_a = np.linalg.norm(vec1)
    norm_b = np.linalg.norm(vec2)
    return dot_product / (norm_a * norm_b)

# 예시
vec_increase = [0.8, 0.6, -0.2]  # "증가"
vec_rise = [0.7, 0.5, -0.1]      # "상승"
vec_decrease = [-0.8, -0.6, 0.2] # "감소"

print(f"증가 vs 상승: {cosine_similarity(vec_increase, vec_rise):.2f}")     # 0.99 (매우 유사)
print(f"증가 vs 감소: {cosine_similarity(vec_increase, vec_decrease):.2f}") # -1.00 (정반대)
```

#### 유클리드 거리 (Euclidean Distance)

```python
def euclidean_distance(vec1, vec2):
    """
    유클리드 거리 계산

    값: 0 이상
    - 0: 완전히 같음
    - 클수록: 더 멀리 떨어짐
    """
    return np.linalg.norm(vec1 - vec2)
```

### 2.4 벡터화의 활용

#### 1. 유사 문서 검색

```python
# 쿼리와 가장 유사한 문서 찾기
query = "벼 수확량 증가"
query_vec = embedding_model.encode(query)

documents = [
    "2024년 벼 수확량이 15% 증가",
    "농업 기술 발전으로 생산성 향상",
    "날씨가 맑아 야외 활동 증가"
]

# 각 문서를 벡터화하고 유사도 계산
for doc in documents:
    doc_vec = embedding_model.encode(doc)
    similarity = cosine_similarity(query_vec, doc_vec)
    print(f"{doc}: {similarity:.2f}")

# 출력:
# 2024년 벼 수확량이 15% 증가: 0.89  ← 가장 관련성 높음
# 농업 기술 발전으로 생산성 향상: 0.65
# 날씨가 맑아 야외 활동 증가: 0.32
```

#### 2. 문서 클러스터링

```python
# 유사한 문서끼리 그룹화
from sklearn.cluster import KMeans

# 문서들을 벡터화
doc_vectors = [embedding_model.encode(doc) for doc in documents]

# K-Means 클러스터링
kmeans = KMeans(n_clusters=3)
clusters = kmeans.fit_predict(doc_vectors)

# 결과: 비슷한 주제의 문서들이 같은 클러스터로 묶임
```

#### 3. 의미 기반 검색 (Semantic Search)

```python
# 키워드가 정확히 일치하지 않아도 의미가 비슷하면 검색

query = "쌀 생산량"
# 벡터 검색으로 다음도 찾아줌:
# - "벼 수확량"  (쌀 ≈ 벼)
# - "곡물 생산"  (쌀 ⊂ 곡물)
```

---

## 3. Hugging Face 모델 활용

### 3.1 Hugging Face란?

**Hugging Face**는 오픈소스 AI 모델 라이브러리 및 커뮤니티 플랫폼입니다.

- 🤗 **모델 허브**: 수만 개의 사전 학습된 모델 제공
- 📚 **Transformers 라이브러리**: 모델을 쉽게 사용할 수 있는 API
- 🌍 **다국어 지원**: 한국어 포함 100개 이상 언어
- 🆓 **무료 사용**: 대부분 모델 무료 (오픈소스)

### 3.2 설치

```bash
# 필수 라이브러리 설치
pip install sentence-transformers transformers torch

# 선택 사항 (성능 향상)
pip install faiss-cpu  # 빠른 벡터 검색
```

### 3.3 추천 한국어 임베딩 모델

| 모델명 | 차원 | 특징 | 추천 용도 |
|-------|------|------|----------|
| **jhgan/ko-sroberta-multitask** | 768 | 한국어 최적화, 높은 성능 | 일반적인 한국어 텍스트 |
| **sentence-transformers/paraphrase-multilingual-mpnet-base-v2** | 768 | 다국어 지원, 안정적 | 한/영 혼용 문서 |
| **BM-K/KoSimCSE-roberta** | 768 | 문장 유사도 특화 | 유사 문장 검색 |
| **jhgan/ko-sbert-nli** | 768 | 의미 관계 분석 | 문장 간 관계 파악 |

### 3.4 기본 사용법

#### 예시 1: 문장 임베딩

```python
from sentence_transformers import SentenceTransformer

# 1. 모델 로드
model = SentenceTransformer('jhgan/ko-sroberta-multitask')

# 2. 텍스트 임베딩
sentences = [
    "벼 수확량이 증가했습니다.",
    "쌀 생산량이 늘어났습니다.",
    "오늘 날씨가 맑습니다."
]

embeddings = model.encode(sentences)

# 3. 결과 확인
print(f"임베딩 차원: {embeddings.shape}")  # (3, 768)
print(f"첫 문장 벡터 (일부): {embeddings[0][:5]}")  # [0.23, -0.45, ...]
```

#### 예시 2: 유사도 계산

```python
from sentence_transformers import SentenceTransformer, util

model = SentenceTransformer('jhgan/ko-sroberta-multitask')

# 문장 임베딩
sentences = [
    "벼 수확량이 증가했습니다.",
    "쌀 생산량이 늘어났습니다.",
    "오늘 날씨가 맑습니다."
]

embeddings = model.encode(sentences)

# 유사도 행렬 계산
similarity_matrix = util.cos_sim(embeddings, embeddings)

print("유사도 행렬:")
print(similarity_matrix)

# 출력:
# tensor([[1.0000, 0.8523, 0.1234],  # 문장1 vs [문장1, 문장2, 문장3]
#         [0.8523, 1.0000, 0.1456],  # 문장2 vs [문장1, 문장2, 문장3]
#         [0.1234, 0.1456, 1.0000]]) # 문장3 vs [문장1, 문장2, 문장3]
```

#### 예시 3: 의미 검색 (Semantic Search)

```python
from sentence_transformers import SentenceTransformer, util

model = SentenceTransformer('jhgan/ko-sroberta-multitask')

# 문서 데이터베이스
documents = [
    "농촌진흥청 2024년 벼 수확량 연구 보고서",
    "농업 기술 발전으로 생산성 15% 향상",
    "기후 변화에 따른 농작물 재배 방법 변화",
    "스마트팜 기술을 활용한 효율적 농업",
    "2024년 여름 무더위로 폭염 주의보 발령"
]

# 문서 벡터화 (한 번만 수행)
doc_embeddings = model.encode(documents)

# 사용자 쿼리
query = "쌀 생산량 증가"
query_embedding = model.encode(query)

# 유사도 계산
similarities = util.cos_sim(query_embedding, doc_embeddings)[0]

# 상위 3개 결과
top_k = 3
top_results = similarities.argsort(descending=True)[:top_k]

print(f"쿼리: '{query}'\n")
print("검색 결과:")
for idx in top_results:
    print(f"  {similarities[idx]:.4f} - {documents[idx]}")

# 출력:
# 쿼리: '쌀 생산량 증가'
#
# 검색 결과:
#   0.7234 - 농촌진흥청 2024년 벼 수확량 연구 보고서
#   0.6543 - 농업 기술 발전으로 생산성 15% 향상
#   0.4123 - 스마트팜 기술을 활용한 효율적 농업
```

### 3.5 다국어 모델 사용

```python
from sentence_transformers import SentenceTransformer

# 다국어 모델 로드
model = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2')

# 한국어, 영어 혼용 문장
sentences = [
    "벼 수확량이 증가했습니다.",
    "Rice production has increased.",  # 같은 의미의 영어
    "The weather is nice today."       # 다른 주제
]

embeddings = model.encode(sentences)

# 한-영 문장 간 유사도
from sentence_transformers import util
similarity = util.cos_sim(embeddings[0], embeddings[1])

print(f"한국어-영어 유사도: {similarity[0][0]:.4f}")  # 0.85 이상 (높은 유사도)
```

### 3.6 배치 처리

대량의 텍스트를 효율적으로 처리:

```python
from sentence_transformers import SentenceTransformer

model = SentenceTransformer('jhgan/ko-sroberta-multitask')

# 대량의 문서 (예: 10,000개)
documents = [f"문서 {i}의 내용입니다." for i in range(10000)]

# 배치 처리 (메모리 효율적)
batch_size = 32
embeddings = model.encode(
    documents,
    batch_size=batch_size,
    show_progress_bar=True  # 진행률 표시
)

print(f"총 {len(embeddings)}개 문서 벡터화 완료")
print(f"각 벡터 차원: {embeddings.shape[1]}")
```

---

## 4. 주의점

### 4.1 모델 선택 시 주의사항

#### 주의점 1: 언어별 최적화 모델 사용

```python
# ❌ 나쁜 예: 영어 전용 모델로 한국어 처리
model = SentenceTransformer('all-MiniLM-L6-v2')  # 영어 전용
text = "벼 수확량이 증가했습니다."
embedding = model.encode(text)  # 성능 낮음

# ✅ 좋은 예: 한국어 최적화 모델
model = SentenceTransformer('jhgan/ko-sroberta-multitask')
text = "벼 수확량이 증가했습니다."
embedding = model.encode(text)  # 성능 높음
```

#### 주의점 2: 모델 크기와 성능 균형

| 모델 크기 | 파라미터 수 | 속도 | 성능 | 추천 용도 |
|----------|-----------|------|------|----------|
| **Small** | ~60M | 빠름 | 낮음 | 실시간 서비스, 대량 처리 |
| **Base** | ~110M | 보통 | 보통 | 일반적인 용도 |
| **Large** | ~340M | 느림 | 높음 | 정확도가 중요한 경우 |

```python
# 속도 vs 정확도 트레이드오프
fast_model = SentenceTransformer('jhgan/ko-sbert-nli')      # 빠름, 성능 보통
accurate_model = SentenceTransformer('jhgan/ko-sroberta-multitask')  # 느림, 성능 높음

# 용도에 맞게 선택!
```

### 4.2 임베딩 차원 일관성

```python
# ⚠️ 주의: 모델마다 출력 차원이 다름

model_768 = SentenceTransformer('jhgan/ko-sroberta-multitask')  # 768차원
model_384 = SentenceTransformer('paraphrase-MiniLM-L3-v2')      # 384차원

# ❌ 다른 차원의 벡터 비교 불가
vec1 = model_768.encode("텍스트1")  # shape: (768,)
vec2 = model_384.encode("텍스트2")  # shape: (384,)
# similarity = cosine_similarity(vec1, vec2)  # 오류 발생!

# ✅ 같은 모델로 일관되게 사용
model = SentenceTransformer('jhgan/ko-sroberta-multitask')
vec1 = model.encode("텍스트1")
vec2 = model.encode("텍스트2")
similarity = cosine_similarity(vec1, vec2)  # 정상 작동
```

### 4.3 토큰 길이 제한

```python
# ⚠️ 대부분 모델은 최대 512 토큰 제한

long_text = "매우 긴 문서..." * 1000  # 토큰 제한 초과

# ❌ 나쁜 예: 긴 텍스트를 그대로 입력
embedding = model.encode(long_text)  # 512 토큰 이후 잘림

# ✅ 좋은 예 1: 텍스트를 청크로 나누기
def chunk_text(text, max_length=500):
    """텍스트를 여러 청크로 분할"""
    words = text.split()
    chunks = []
    current_chunk = []
    current_length = 0

    for word in words:
        if current_length + len(word) > max_length:
            chunks.append(' '.join(current_chunk))
            current_chunk = [word]
            current_length = len(word)
        else:
            current_chunk.append(word)
            current_length += len(word) + 1

    if current_chunk:
        chunks.append(' '.join(current_chunk))

    return chunks

# 청크별로 임베딩 후 평균
chunks = chunk_text(long_text)
chunk_embeddings = model.encode(chunks)
doc_embedding = chunk_embeddings.mean(axis=0)  # 평균 벡터

# ✅ 좋은 예 2: 요약 후 임베딩
summary = summarize(long_text)  # 요약 함수
embedding = model.encode(summary)
```

### 4.4 정규화(Normalization)

```python
# 코사인 유사도 계산 시 벡터 정규화 권장

import numpy as np

def normalize_embedding(embedding):
    """벡터를 단위 벡터로 정규화"""
    norm = np.linalg.norm(embedding)
    return embedding / norm

# 정규화하면 내적(dot product)만으로 코사인 유사도 계산 가능
vec1 = normalize_embedding(model.encode("텍스트1"))
vec2 = normalize_embedding(model.encode("텍스트2"))

# 코사인 유사도 = 내적
similarity = np.dot(vec1, vec2)  # 빠름!

# vs 정규화 없이
# similarity = np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))  # 느림
```

### 4.5 모델 캐싱

```python
# ⚠️ 모델을 반복 로드하면 메모리/시간 낭비

# ❌ 나쁜 예: 함수 호출마다 모델 로드
def embed_text(text):
    model = SentenceTransformer('jhgan/ko-sroberta-multitask')  # 매번 로드!
    return model.encode(text)

# ✅ 좋은 예: 모델을 한 번만 로드
class EmbeddingService:
    def __init__(self, model_name='jhgan/ko-sroberta-multitask'):
        self.model = SentenceTransformer(model_name)  # 한 번만 로드

    def embed(self, text):
        return self.model.encode(text)

# 사용
service = EmbeddingService()
emb1 = service.embed("텍스트1")
emb2 = service.embed("텍스트2")
```

### 4.6 GPU 활용

```python
# GPU가 있다면 활용하여 속도 향상

import torch

# GPU 사용 가능 확인
device = 'cuda' if torch.cuda.is_available() else 'cpu'
print(f"사용 디바이스: {device}")

# 모델을 GPU로 이동
model = SentenceTransformer('jhgan/ko-sroberta-multitask', device=device)

# 대량 처리 시 10~100배 빠름!
documents = ["문서 " + str(i) for i in range(10000)]
embeddings = model.encode(documents, batch_size=64, show_progress_bar=True)
```

---

## 5. 실전 예시

### 5.1 문서 검색 시스템 구축

```python
from sentence_transformers import SentenceTransformer, util
import numpy as np

class DocumentSearchSystem:
    """임베딩 기반 문서 검색 시스템"""

    def __init__(self, model_name='jhgan/ko-sroberta-multitask'):
        """
        Args:
            model_name: 사용할 임베딩 모델명
        """
        self.model = SentenceTransformer(model_name)
        self.documents = []
        self.embeddings = None

    def add_documents(self, documents: list):
        """
        문서 추가 및 임베딩

        Args:
            documents: 문서 리스트
        """
        self.documents.extend(documents)

        # 새 문서들 임베딩
        new_embeddings = self.model.encode(
            documents,
            batch_size=32,
            show_progress_bar=True,
            convert_to_numpy=True
        )

        # 기존 임베딩과 합치기
        if self.embeddings is None:
            self.embeddings = new_embeddings
        else:
            self.embeddings = np.vstack([self.embeddings, new_embeddings])

        print(f"총 {len(self.documents)}개 문서 인덱싱 완료")

    def search(self, query: str, top_k: int = 5):
        """
        쿼리와 가장 유사한 문서 검색

        Args:
            query: 검색 쿼리
            top_k: 반환할 결과 개수

        Returns:
            검색 결과 리스트 [(점수, 문서), ...]
        """
        if not self.documents:
            return []

        # 쿼리 임베딩
        query_embedding = self.model.encode(query, convert_to_numpy=True)

        # 코사인 유사도 계산
        similarities = util.cos_sim(query_embedding, self.embeddings)[0]

        # 상위 k개 결과
        top_results = similarities.argsort(descending=True)[:top_k]

        results = []
        for idx in top_results:
            results.append({
                'score': float(similarities[idx]),
                'document': self.documents[idx],
                'index': int(idx)
            })

        return results

    def save_index(self, file_path):
        """인덱스 저장"""
        np.savez(
            file_path,
            embeddings=self.embeddings,
            documents=self.documents
        )

    def load_index(self, file_path):
        """인덱스 로드"""
        data = np.load(file_path, allow_pickle=True)
        self.embeddings = data['embeddings']
        self.documents = data['documents'].tolist()


# 사용 예시
search_system = DocumentSearchSystem()

# 문서 추가
documents = [
    "농촌진흥청 2024년 벼 수확량 연구 보고서",
    "농업 기술 발전으로 생산성 15% 향상",
    "기후 변화에 따른 농작물 재배 방법 변화",
    "스마트팜 기술을 활용한 효율적 농업",
    "2024년 여름 무더위로 폭염 주의보 발령",
    "친환경 유기농 재배 기술 개발",
    "드론을 활용한 농작물 모니터링 시스템"
]

search_system.add_documents(documents)

# 검색
query = "쌀 생산량 증가"
results = search_system.search(query, top_k=3)

print(f"\n검색어: '{query}'\n")
for i, result in enumerate(results, 1):
    print(f"{i}. [점수: {result['score']:.4f}] {result['document']}")

# 인덱스 저장
search_system.save_index('document_index.npz')
```

### 5.2 청크 단위 임베딩

```python
from sentence_transformers import SentenceTransformer

def embed_document_with_chunks(
    text: str,
    model: SentenceTransformer,
    chunk_size: int = 500,
    overlap: int = 100
):
    """
    긴 문서를 청크로 나누어 임베딩

    Args:
        text: 원본 텍스트
        model: 임베딩 모델
        chunk_size: 청크 크기 (문자 수)
        overlap: 청크 간 중복 크기

    Returns:
        청크 정보 리스트
    """
    chunks = []
    start = 0

    while start < len(text):
        end = start + chunk_size

        # 청크 추출
        chunk_text = text[start:end]

        # 임베딩
        embedding = model.encode(chunk_text)

        chunks.append({
            'text': chunk_text,
            'embedding': embedding,
            'start': start,
            'end': end,
            'chunk_index': len(chunks)
        })

        # 다음 청크 시작 위치 (오버랩 고려)
        start += chunk_size - overlap

    return chunks


# 사용 예시
model = SentenceTransformer('jhgan/ko-sroberta-multitask')

long_document = """
농촌진흥청의 2024년 벼 수확량 연구에 따르면,
올해 벼 수확량이 전년 대비 15% 증가한 것으로 나타났습니다.
이는 신품종 개발과 스마트팜 기술의 도입으로
재배 효율성이 크게 향상된 결과입니다.
특히 기후 변화에 강한 품종이 개발되어
안정적인 생산이 가능해졌습니다.
""" * 10  # 긴 문서 시뮬레이션

chunks = embed_document_with_chunks(long_document, model)

print(f"총 {len(chunks)}개 청크 생성")
print(f"각 청크 임베딩 차원: {chunks[0]['embedding'].shape}")
```

### 5.3 메타데이터와 함께 벡터 저장

```python
from sentence_transformers import SentenceTransformer
import json
from datetime import datetime

class DocumentEmbeddingManager:
    """문서 임베딩과 메타데이터 관리"""

    def __init__(self, model_name='jhgan/ko-sroberta-multitask'):
        self.model = SentenceTransformer(model_name)
        self.documents = []

    def add_document(self, text: str, metadata: dict):
        """
        문서 추가 (임베딩 + 메타데이터)

        Args:
            text: 문서 텍스트
            metadata: 메타데이터
        """
        # 임베딩 생성
        embedding = self.model.encode(text)

        # 문서 정보 저장
        doc_info = {
            'text': text,
            'embedding': embedding.tolist(),  # numpy → list
            'metadata': metadata,
            'embedded_at': datetime.now().isoformat()
        }

        self.documents.append(doc_info)

    def search_with_filter(self, query: str, metadata_filter: dict = None, top_k: int = 5):
        """
        메타데이터 필터와 함께 검색

        Args:
            query: 검색 쿼리
            metadata_filter: 메타데이터 필터 조건
            top_k: 반환 결과 개수

        Returns:
            필터링된 검색 결과
        """
        # 쿼리 임베딩
        query_embedding = self.model.encode(query)

        # 메타데이터 필터 적용
        candidates = self.documents

        if metadata_filter:
            candidates = [
                doc for doc in self.documents
                if all(
                    doc['metadata'].get(key) == value
                    for key, value in metadata_filter.items()
                )
            ]

        if not candidates:
            return []

        # 유사도 계산
        import numpy as np
        from sentence_transformers import util

        candidate_embeddings = np.array([doc['embedding'] for doc in candidates])
        similarities = util.cos_sim(query_embedding, candidate_embeddings)[0]

        # 상위 k개 결과
        top_indices = similarities.argsort(descending=True)[:top_k]

        results = []
        for idx in top_indices:
            results.append({
                'score': float(similarities[idx]),
                'text': candidates[idx]['text'],
                'metadata': candidates[idx]['metadata']
            })

        return results

    def save(self, file_path):
        """JSON으로 저장"""
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(self.documents, f, ensure_ascii=False, indent=2)

    def load(self, file_path):
        """JSON에서 로드"""
        with open(file_path, 'r', encoding='utf-8') as f:
            self.documents = json.load(f)


# 사용 예시
manager = DocumentEmbeddingManager()

# 문서 추가 (메타데이터 포함)
manager.add_document(
    text="2024년 벼 수확량이 15% 증가했습니다.",
    metadata={
        'category': 'Research Report',
        'year': 2024,
        'department': 'RDA',
        'author': 'Bern'
    }
)

manager.add_document(
    text="스마트팜 기술로 생산성이 향상되었습니다.",
    metadata={
        'category': 'Technology Report',
        'year': 2024,
        'department': 'RDA',
        'author': 'Bern'
    }
)

manager.add_document(
    text="2023년 기후 변화 영향 분석 보고서",
    metadata={
        'category': 'Research Report',
        'year': 2023,
        'department': 'Environment',
        'author': 'Kim'
    }
)

# 검색 1: 메타데이터 필터 없이
results = manager.search_with_filter("생산량 증가", top_k=2)
print("일반 검색 결과:")
for r in results:
    print(f"  [{r['score']:.4f}] {r['text']}")

# 검색 2: 메타데이터 필터 적용
results = manager.search_with_filter(
    query="생산량 증가",
    metadata_filter={'year': 2024, 'department': 'RDA'},
    top_k=2
)
print("\n필터링된 검색 결과 (2024년 + RDA):")
for r in results:
    print(f"  [{r['score']:.4f}] {r['text']} (year: {r['metadata']['year']})")

# 저장
manager.save('embeddings_with_metadata.json')
```

---

## 6. 성능 최적화

### 6.1 벡터 데이터베이스 활용

대규모 벡터 검색을 위해 전용 데이터베이스 사용:

#### FAISS (Facebook AI Similarity Search)

```python
import faiss
import numpy as np
from sentence_transformers import SentenceTransformer

class FAISSSearchEngine:
    """FAISS를 활용한 고속 벡터 검색"""

    def __init__(self, model_name='jhgan/ko-sroberta-multitask'):
        self.model = SentenceTransformer(model_name)
        self.index = None
        self.documents = []
        self.dimension = 768  # 모델 출력 차원

    def build_index(self, documents: list):
        """
        문서 인덱스 구축

        Args:
            documents: 문서 리스트
        """
        self.documents = documents

        # 임베딩 생성
        print("문서 임베딩 중...")
        embeddings = self.model.encode(
            documents,
            batch_size=32,
            show_progress_bar=True,
            convert_to_numpy=True
        )

        # FAISS 인덱스 생성
        print("FAISS 인덱스 구축 중...")
        self.index = faiss.IndexFlatIP(self.dimension)  # 내적 (코사인 유사도)

        # 벡터 정규화 (코사인 유사도를 위해)
        faiss.normalize_L2(embeddings)

        # 인덱스에 추가
        self.index.add(embeddings)

        print(f"인덱스 구축 완료: {self.index.ntotal}개 벡터")

    def search(self, query: str, top_k: int = 5):
        """
        고속 검색

        Args:
            query: 검색 쿼리
            top_k: 반환 결과 개수

        Returns:
            검색 결과
        """
        # 쿼리 임베딩
        query_embedding = self.model.encode([query], convert_to_numpy=True)
        faiss.normalize_L2(query_embedding)

        # FAISS 검색
        scores, indices = self.index.search(query_embedding, top_k)

        # 결과 구성
        results = []
        for score, idx in zip(scores[0], indices[0]):
            if idx < len(self.documents):
                results.append({
                    'score': float(score),
                    'document': self.documents[idx],
                    'index': int(idx)
                })

        return results

    def save_index(self, index_path, docs_path):
        """인덱스 저장"""
        faiss.write_index(self.index, index_path)
        np.save(docs_path, self.documents)

    def load_index(self, index_path, docs_path):
        """인덱스 로드"""
        self.index = faiss.read_index(index_path)
        self.documents = np.load(docs_path, allow_pickle=True).tolist()


# 사용 예시
engine = FAISSSearchEngine()

# 대량의 문서 (예: 10만 개)
documents = [f"문서 {i}의 내용입니다." for i in range(100000)]

# 인덱스 구축
engine.build_index(documents)

# 검색 (매우 빠름!)
import time
start = time.time()
results = engine.search("특정 주제에 대한 문서", top_k=10)
end = time.time()

print(f"검색 시간: {(end - start) * 1000:.2f}ms")  # 수 ms 이내!

# 인덱스 저장
engine.save_index('faiss.index', 'documents.npy')
```

### 6.2 배치 처리 최적화

```python
from sentence_transformers import SentenceTransformer
from tqdm import tqdm

def batch_encode_documents(documents: list, model_name: str, batch_size: int = 32):
    """
    대량 문서의 효율적 배치 처리

    Args:
        documents: 문서 리스트
        model_name: 모델명
        batch_size: 배치 크기

    Returns:
        임베딩 배열
    """
    model = SentenceTransformer(model_name)

    embeddings = []

    # 배치별로 처리
    for i in tqdm(range(0, len(documents), batch_size)):
        batch = documents[i:i + batch_size]
        batch_embeddings = model.encode(batch, convert_to_numpy=True)
        embeddings.append(batch_embeddings)

    # 하나의 배열로 합치기
    import numpy as np
    return np.vstack(embeddings)
```

### 6.3 캐싱 전략

```python
from functools import lru_cache
import hashlib

class CachedEmbeddingModel:
    """캐싱 기능이 있는 임베딩 모델"""

    def __init__(self, model_name='jhgan/ko-sroberta-multitask', cache_size=1000):
        self.model = SentenceTransformer(model_name)
        self.cache = {}
        self.cache_size = cache_size

    def _get_cache_key(self, text: str) -> str:
        """텍스트의 해시 키 생성"""
        return hashlib.md5(text.encode()).hexdigest()

    def encode(self, text: str):
        """캐시를 활용한 임베딩"""
        cache_key = self._get_cache_key(text)

        # 캐시 히트
        if cache_key in self.cache:
            return self.cache[cache_key]

        # 캐시 미스 → 임베딩 생성
        embedding = self.model.encode(text)

        # 캐시 크기 제한
        if len(self.cache) >= self.cache_size:
            # 가장 오래된 항목 제거
            oldest = next(iter(self.cache))
            del self.cache[oldest]

        # 캐시 저장
        self.cache[cache_key] = embedding

        return embedding


# 사용 예시
cached_model = CachedEmbeddingModel()

# 첫 호출: 모델 실행
emb1 = cached_model.encode("벼 수확량")  # 느림

# 두 번째 호출: 캐시에서 반환
emb2 = cached_model.encode("벼 수확량")  # 매우 빠름!
```

---

## 정리

### 핵심 요약

1. **임베딩이란?**
   - 텍스트를 고정 길이의 숫자 벡터로 변환
   - 의미적으로 유사한 텍스트는 벡터 공간에서 가까움

2. **Hugging Face 활용**
   - 수만 개의 사전 학습된 모델 제공
   - `sentence-transformers` 라이브러리로 쉽게 사용
   - 한국어 추천 모델: `jhgan/ko-sroberta-multitask`

3. **주의사항**
   - 언어별 최적화 모델 사용
   - 토큰 길이 제한 (보통 512 토큰)
   - 모델 캐싱으로 성능 향상
   - GPU 활용 시 10~100배 빠름

4. **실전 활용**
   - 문서 검색 시스템
   - 유사 문서 추천
   - 메타데이터와 결합한 하이브리드 검색
   - FAISS로 대규모 벡터 검색

### 다음 단계

임베딩을 생성했다면, 이제 벡터 데이터베이스에 저장하고 실제 검색 시스템을 구축할 차례입니다.

---

**문서 버전**: 1.0
**최종 수정일**: 2025-10-24
**작성자**: Bern

## 📌 다음 단계

임베딩과 벡터화를 익혔다면, 다음은 **Elasticsearch를 활용한 벡터 서치**입니다.

**다음 문서**: [1.2.08. Elasticsearch를 활용한 벡터 서치](./1.2.08.Elasticsearch를%20활용한%20벡터%20서치.md)
