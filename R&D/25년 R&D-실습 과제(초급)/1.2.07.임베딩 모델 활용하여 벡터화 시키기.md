# 1.2.07. ì„ë² ë”© ëª¨ë¸ í™œìš©í•˜ì—¬ ë²¡í„°í™” ì‹œí‚¤ê¸°

> **í•™ìŠµ ëª©í‘œ**: ì„ë² ë”© ëª¨ë¸ì˜ ê°œë…ì„ ì´í•´í•˜ê³ , Hugging Face ëª¨ë¸ì„ í™œìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•˜ëŠ” ë°©ë²•ì„ ìŠµë“í•©ë‹ˆë‹¤.

---

## ëª©ì°¨

1. [ì„ë² ë”© ëª¨ë¸ì´ë€](#1-ì„ë² ë”©-ëª¨ë¸ì´ë€)
2. [ë²¡í„°í™”ë€](#2-ë²¡í„°í™”ë€)
3. [Hugging Face ëª¨ë¸ í™œìš©](#3-hugging-face-ëª¨ë¸-í™œìš©)
4. [ì£¼ì˜ì ](#4-ì£¼ì˜ì )
5. [ì‹¤ì „ ì˜ˆì‹œ](#5-ì‹¤ì „-ì˜ˆì‹œ)
6. [ì„±ëŠ¥ ìµœì í™”](#6-ì„±ëŠ¥-ìµœì í™”)

---

## 1. ì„ë² ë”© ëª¨ë¸ì´ë€

### 1.1 ì„ë² ë”©(Embedding)ì˜ ì •ì˜

**ì„ë² ë”©(Embedding)**ì€ í…ìŠ¤íŠ¸, ì´ë¯¸ì§€ ë“±ì˜ ë°ì´í„°ë¥¼ **ê³ ì •ëœ ê¸¸ì´ì˜ ìˆ«ì ë²¡í„°ë¡œ ë³€í™˜**í•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤.

```
í…ìŠ¤íŠ¸: "ë²¼ ìˆ˜í™•ëŸ‰ì´ ì¦ê°€í–ˆë‹¤"
    â†“ (ì„ë² ë”© ëª¨ë¸)
ë²¡í„°: [0.23, -0.45, 0.12, ..., 0.89]  â† 768ì°¨ì› ë²¡í„°
```

### 1.2 ì™œ ì„ë² ë”©ì´ í•„ìš”í•œê°€?

ì»´í“¨í„°ëŠ” í…ìŠ¤íŠ¸ë¥¼ ì§ì ‘ ì´í•´í•˜ì§€ ëª»í•©ë‹ˆë‹¤. ìˆ«ìë¡œ ë³€í™˜í•´ì•¼ í•©ë‹ˆë‹¤.

#### ì „í†µì ì¸ ë°©ë²•ì˜ í•œê³„

**1. One-Hot Encoding (ì›-í•« ì¸ì½”ë”©)**
```python
# ë‹¨ì–´ë§ˆë‹¤ í•˜ë‚˜ì˜ ì¸ë±ìŠ¤ë§Œ 1, ë‚˜ë¨¸ì§€ëŠ” 0
vocabulary = ["ë²¼", "ìˆ˜í™•", "ì¦ê°€", "ê°ì†Œ"]

"ë²¼"    â†’ [1, 0, 0, 0]
"ìˆ˜í™•"  â†’ [0, 1, 0, 0]
"ì¦ê°€"  â†’ [0, 0, 1, 0]
"ê°ì†Œ"  â†’ [0, 0, 0, 1]
```

**ë¬¸ì œì :**
- âŒ ë‹¨ì–´ ê°„ ìœ ì‚¬ë„ë¥¼ í‘œí˜„í•  ìˆ˜ ì—†ìŒ
- âŒ "ì¦ê°€"ì™€ "ê°ì†Œ"ê°€ ë°˜ëŒ€ ì˜ë¯¸ì¸ì§€ ëª¨ë¦„
- âŒ ì–´íœ˜ í¬ê¸°ë§Œí¼ ë²¡í„° ì°¨ì› ì¦ê°€ (100ë§Œ ë‹¨ì–´ = 100ë§Œ ì°¨ì›!)

**2. TF-IDF**
```python
# ë‹¨ì–´ ë¹ˆë„ ê¸°ë°˜
"ë²¼ ìˆ˜í™•ëŸ‰ì´ ì¦ê°€" â†’ [0.5, 0.3, 0.8, ...]
```

**ë¬¸ì œì :**
- âŒ ë¬¸ë§¥ì„ ê³ ë ¤í•˜ì§€ ì•ŠìŒ
- âŒ "ì‚¬ê³¼" (ê³¼ì¼)ì™€ "ì‚¬ê³¼" (ì‚¬ê³¼í•˜ë‹¤)ë¥¼ êµ¬ë¶„ ëª»í•¨
- âŒ ì˜ë¯¸ì  ìœ ì‚¬ì„± íŒŒì•… ë¶ˆê°€

#### ì„ë² ë”©ì˜ ì¥ì 

```python
# ì„ë² ë”©: ì˜ë¯¸ê°€ ë¹„ìŠ·í•œ ë‹¨ì–´ëŠ” ë²¡í„° ê³µê°„ì—ì„œ ê°€ê¹Œì›€

"ì¦ê°€" â†’ [0.8, 0.6, -0.2, ...]
"ìƒìŠ¹" â†’ [0.7, 0.5, -0.1, ...]  # "ì¦ê°€"ì™€ ê°€ê¹Œì›€ (ìœ ì‚¬ë„ ë†’ìŒ)
"ê°ì†Œ" â†’ [-0.8, -0.6, 0.2, ...] # "ì¦ê°€"ì™€ ë©ˆ (ìœ ì‚¬ë„ ë‚®ìŒ)

# ì½”ì‚¬ì¸ ìœ ì‚¬ë„
similarity("ì¦ê°€", "ìƒìŠ¹") = 0.92  # ë§¤ìš° ìœ ì‚¬
similarity("ì¦ê°€", "ê°ì†Œ") = -0.85 # ë°˜ëŒ€ ì˜ë¯¸
```

**ì¥ì :**
- âœ… ì˜ë¯¸ì  ìœ ì‚¬ì„±ì„ ë²¡í„° ê±°ë¦¬ë¡œ í‘œí˜„
- âœ… ê³ ì •ëœ ì°¨ì› (ë³´í†µ 384, 768, 1024ì°¨ì›)
- âœ… ë¬¸ë§¥ì„ ê³ ë ¤í•œ í‘œí˜„
- âœ… ê²€ìƒ‰, ë¶„ë¥˜, ì¶”ì²œì— í™œìš© ê°€ëŠ¥

### 1.3 ì„ë² ë”© ëª¨ë¸ì˜ ì¢…ë¥˜

| ëª¨ë¸ ìœ í˜• | ì„¤ëª… | ì˜ˆì‹œ |
|----------|------|------|
| **Word Embedding** | ë‹¨ì–´ ë‹¨ìœ„ ë²¡í„°í™” | Word2Vec, GloVe, FastText |
| **Sentence Embedding** | ë¬¸ì¥ ë‹¨ìœ„ ë²¡í„°í™” | Sentence-BERT, SimCSE |
| **Document Embedding** | ë¬¸ì„œ ë‹¨ìœ„ ë²¡í„°í™” | Doc2Vec, Sentence-BERT |
| **Multilingual Embedding** | ë‹¤êµ­ì–´ ì§€ì› | mBERT, XLM-RoBERTa |

### 1.4 ì„ë² ë”© ëª¨ë¸ì˜ ì‘ë™ ì›ë¦¬

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 ì„ë² ë”© ëª¨ë¸ êµ¬ì¡°                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                     â”‚
â”‚  ì…ë ¥ í…ìŠ¤íŠ¸: "2024ë…„ ë²¼ ìˆ˜í™•ëŸ‰ì´ 15% ì¦ê°€í–ˆë‹¤"      â”‚
â”‚       â†“                                             â”‚
â”‚  [í† í°í™”]                                           â”‚
â”‚  ["2024ë…„", "ë²¼", "ìˆ˜í™•ëŸ‰", "ì´", "15%", ...]       â”‚
â”‚       â†“                                             â”‚
â”‚  [ì„ë² ë”© ë ˆì´ì–´]                                    â”‚
â”‚  ê° í† í°ì„ ì´ˆê¸° ë²¡í„°ë¡œ ë³€í™˜                          â”‚
â”‚       â†“                                             â”‚
â”‚  [íŠ¸ëœìŠ¤í¬ë¨¸ ë ˆì´ì–´ë“¤]                              â”‚
â”‚  ë¬¸ë§¥ì„ ê³ ë ¤í•˜ì—¬ ë²¡í„° ì •ì œ                           â”‚
â”‚  (Self-Attentionìœ¼ë¡œ ë‹¨ì–´ ê°„ ê´€ê³„ íŒŒì•…)             â”‚
â”‚       â†“                                             â”‚
â”‚  [í’€ë§(Pooling)]                                    â”‚
â”‚  - Mean Pooling: ëª¨ë“  í† í° ë²¡í„°ì˜ í‰ê·               â”‚
â”‚  - CLS Token: íŠ¹ë³„ í† í°ì˜ ë²¡í„° ì‚¬ìš©                 â”‚
â”‚       â†“                                             â”‚
â”‚  ì¶œë ¥ ë²¡í„°: [0.23, -0.45, 0.12, ..., 0.89]          â”‚
â”‚  (768ì°¨ì›)                                          â”‚
â”‚                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 2. ë²¡í„°í™”ë€

### 2.1 ë²¡í„°í™”ì˜ ê°œë…

**ë²¡í„°í™”(Vectorization)**ëŠ” ë°ì´í„°ë¥¼ ìˆ˜ì¹˜í˜• ë²¡í„°ë¡œ ë³€í™˜í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤.

```python
# 1ì°¨ì› ë²¡í„° (ìŠ¤ì¹¼ë¼)
score = 85

# 2ì°¨ì› ë²¡í„°
point = [x, y] = [3, 5]

# ê³ ì°¨ì› ë²¡í„° (ì„ë² ë”©)
text_embedding = [0.23, -0.45, 0.12, ..., 0.89]  # 768ì°¨ì›
```

### 2.2 ë²¡í„° ê³µê°„ê³¼ ìœ ì‚¬ë„

ë²¡í„°í™”ì˜ í•µì‹¬ì€ **ì˜ë¯¸ì ìœ¼ë¡œ ìœ ì‚¬í•œ í…ìŠ¤íŠ¸ëŠ” ë²¡í„° ê³µê°„ì—ì„œ ê°€ê¹ê²Œ ìœ„ì¹˜**í•œë‹¤ëŠ” ì ì…ë‹ˆë‹¤.

```
ë²¡í„° ê³µê°„ (2D ì‹œê°í™”)
    â†‘
    â”‚    "ì¦ê°€" â—
    â”‚           â— "ìƒìŠ¹"
    â”‚
    â”‚
    â”‚
â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’
    â”‚
    â”‚        â— "ê°ì†Œ"
    â”‚      â— "í•˜ë½"
    â”‚
```

ì‹¤ì œë¡œëŠ” 768ì°¨ì› ê³µê°„ì´ì§€ë§Œ, ì›ë¦¬ëŠ” ê°™ìŠµë‹ˆë‹¤.

### 2.3 ë²¡í„° ê°„ ìœ ì‚¬ë„ ì¸¡ì •

#### ì½”ì‚¬ì¸ ìœ ì‚¬ë„ (Cosine Similarity)

ê°€ì¥ ë§ì´ ì‚¬ìš©ë˜ëŠ” ìœ ì‚¬ë„ ì¸¡ì • ë°©ë²•:

```python
import numpy as np

def cosine_similarity(vec1, vec2):
    """
    ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°

    ê°’ ë²”ìœ„: -1 ~ 1
    - 1: ì™„ì „íˆ ê°™ì€ ë°©í–¥ (ë§¤ìš° ìœ ì‚¬)
    - 0: ì§ê° (ë¬´ê´€í•¨)
    - -1: ì •ë°˜ëŒ€ ë°©í–¥ (ë°˜ëŒ€ ì˜ë¯¸)
    """
    dot_product = np.dot(vec1, vec2)
    norm_a = np.linalg.norm(vec1)
    norm_b = np.linalg.norm(vec2)
    return dot_product / (norm_a * norm_b)

# ì˜ˆì‹œ
vec_increase = [0.8, 0.6, -0.2]  # "ì¦ê°€"
vec_rise = [0.7, 0.5, -0.1]      # "ìƒìŠ¹"
vec_decrease = [-0.8, -0.6, 0.2] # "ê°ì†Œ"

print(f"ì¦ê°€ vs ìƒìŠ¹: {cosine_similarity(vec_increase, vec_rise):.2f}")     # 0.99 (ë§¤ìš° ìœ ì‚¬)
print(f"ì¦ê°€ vs ê°ì†Œ: {cosine_similarity(vec_increase, vec_decrease):.2f}") # -1.00 (ì •ë°˜ëŒ€)
```

#### ìœ í´ë¦¬ë“œ ê±°ë¦¬ (Euclidean Distance)

```python
def euclidean_distance(vec1, vec2):
    """
    ìœ í´ë¦¬ë“œ ê±°ë¦¬ ê³„ì‚°

    ê°’: 0 ì´ìƒ
    - 0: ì™„ì „íˆ ê°™ìŒ
    - í´ìˆ˜ë¡: ë” ë©€ë¦¬ ë–¨ì–´ì§
    """
    return np.linalg.norm(vec1 - vec2)
```

### 2.4 ë²¡í„°í™”ì˜ í™œìš©

#### 1. ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰

```python
# ì¿¼ë¦¬ì™€ ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œ ì°¾ê¸°
query = "ë²¼ ìˆ˜í™•ëŸ‰ ì¦ê°€"
query_vec = embedding_model.encode(query)

documents = [
    "2024ë…„ ë²¼ ìˆ˜í™•ëŸ‰ì´ 15% ì¦ê°€",
    "ë†ì—… ê¸°ìˆ  ë°œì „ìœ¼ë¡œ ìƒì‚°ì„± í–¥ìƒ",
    "ë‚ ì”¨ê°€ ë§‘ì•„ ì•¼ì™¸ í™œë™ ì¦ê°€"
]

# ê° ë¬¸ì„œë¥¼ ë²¡í„°í™”í•˜ê³  ìœ ì‚¬ë„ ê³„ì‚°
for doc in documents:
    doc_vec = embedding_model.encode(doc)
    similarity = cosine_similarity(query_vec, doc_vec)
    print(f"{doc}: {similarity:.2f}")

# ì¶œë ¥:
# 2024ë…„ ë²¼ ìˆ˜í™•ëŸ‰ì´ 15% ì¦ê°€: 0.89  â† ê°€ì¥ ê´€ë ¨ì„± ë†’ìŒ
# ë†ì—… ê¸°ìˆ  ë°œì „ìœ¼ë¡œ ìƒì‚°ì„± í–¥ìƒ: 0.65
# ë‚ ì”¨ê°€ ë§‘ì•„ ì•¼ì™¸ í™œë™ ì¦ê°€: 0.32
```

#### 2. ë¬¸ì„œ í´ëŸ¬ìŠ¤í„°ë§

```python
# ìœ ì‚¬í•œ ë¬¸ì„œë¼ë¦¬ ê·¸ë£¹í™”
from sklearn.cluster import KMeans

# ë¬¸ì„œë“¤ì„ ë²¡í„°í™”
doc_vectors = [embedding_model.encode(doc) for doc in documents]

# K-Means í´ëŸ¬ìŠ¤í„°ë§
kmeans = KMeans(n_clusters=3)
clusters = kmeans.fit_predict(doc_vectors)

# ê²°ê³¼: ë¹„ìŠ·í•œ ì£¼ì œì˜ ë¬¸ì„œë“¤ì´ ê°™ì€ í´ëŸ¬ìŠ¤í„°ë¡œ ë¬¶ì„
```

#### 3. ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰ (Semantic Search)

```python
# í‚¤ì›Œë“œê°€ ì •í™•íˆ ì¼ì¹˜í•˜ì§€ ì•Šì•„ë„ ì˜ë¯¸ê°€ ë¹„ìŠ·í•˜ë©´ ê²€ìƒ‰

query = "ìŒ€ ìƒì‚°ëŸ‰"
# ë²¡í„° ê²€ìƒ‰ìœ¼ë¡œ ë‹¤ìŒë„ ì°¾ì•„ì¤Œ:
# - "ë²¼ ìˆ˜í™•ëŸ‰"  (ìŒ€ â‰ˆ ë²¼)
# - "ê³¡ë¬¼ ìƒì‚°"  (ìŒ€ âŠ‚ ê³¡ë¬¼)
```

---

## 3. Hugging Face ëª¨ë¸ í™œìš©

### 3.1 Hugging Faceë€?

**Hugging Face**ëŠ” ì˜¤í”ˆì†ŒìŠ¤ AI ëª¨ë¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë° ì»¤ë®¤ë‹ˆí‹° í”Œë«í¼ì…ë‹ˆë‹¤.

- ğŸ¤— **ëª¨ë¸ í—ˆë¸Œ**: ìˆ˜ë§Œ ê°œì˜ ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ ì œê³µ
- ğŸ“š **Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬**: ëª¨ë¸ì„ ì‰½ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” API
- ğŸŒ **ë‹¤êµ­ì–´ ì§€ì›**: í•œêµ­ì–´ í¬í•¨ 100ê°œ ì´ìƒ ì–¸ì–´
- ğŸ†“ **ë¬´ë£Œ ì‚¬ìš©**: ëŒ€ë¶€ë¶„ ëª¨ë¸ ë¬´ë£Œ (ì˜¤í”ˆì†ŒìŠ¤)

### 3.2 ì„¤ì¹˜

```bash
# í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
pip install sentence-transformers transformers torch

# ì„ íƒ ì‚¬í•­ (ì„±ëŠ¥ í–¥ìƒ)
pip install faiss-cpu  # ë¹ ë¥¸ ë²¡í„° ê²€ìƒ‰
```

### 3.3 ì¶”ì²œ í•œêµ­ì–´ ì„ë² ë”© ëª¨ë¸

| ëª¨ë¸ëª… | ì°¨ì› | íŠ¹ì§• | ì¶”ì²œ ìš©ë„ |
|-------|------|------|----------|
| **jhgan/ko-sroberta-multitask** | 768 | í•œêµ­ì–´ ìµœì í™”, ë†’ì€ ì„±ëŠ¥ | ì¼ë°˜ì ì¸ í•œêµ­ì–´ í…ìŠ¤íŠ¸ |
| **sentence-transformers/paraphrase-multilingual-mpnet-base-v2** | 768 | ë‹¤êµ­ì–´ ì§€ì›, ì•ˆì •ì  | í•œ/ì˜ í˜¼ìš© ë¬¸ì„œ |
| **BM-K/KoSimCSE-roberta** | 768 | ë¬¸ì¥ ìœ ì‚¬ë„ íŠ¹í™” | ìœ ì‚¬ ë¬¸ì¥ ê²€ìƒ‰ |
| **jhgan/ko-sbert-nli** | 768 | ì˜ë¯¸ ê´€ê³„ ë¶„ì„ | ë¬¸ì¥ ê°„ ê´€ê³„ íŒŒì•… |

### 3.4 ê¸°ë³¸ ì‚¬ìš©ë²•

#### ì˜ˆì‹œ 1: ë¬¸ì¥ ì„ë² ë”©

```python
from sentence_transformers import SentenceTransformer

# 1. ëª¨ë¸ ë¡œë“œ
model = SentenceTransformer('jhgan/ko-sroberta-multitask')

# 2. í…ìŠ¤íŠ¸ ì„ë² ë”©
sentences = [
    "ë²¼ ìˆ˜í™•ëŸ‰ì´ ì¦ê°€í–ˆìŠµë‹ˆë‹¤.",
    "ìŒ€ ìƒì‚°ëŸ‰ì´ ëŠ˜ì–´ë‚¬ìŠµë‹ˆë‹¤.",
    "ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ë§‘ìŠµë‹ˆë‹¤."
]

embeddings = model.encode(sentences)

# 3. ê²°ê³¼ í™•ì¸
print(f"ì„ë² ë”© ì°¨ì›: {embeddings.shape}")  # (3, 768)
print(f"ì²« ë¬¸ì¥ ë²¡í„° (ì¼ë¶€): {embeddings[0][:5]}")  # [0.23, -0.45, ...]
```

#### ì˜ˆì‹œ 2: ìœ ì‚¬ë„ ê³„ì‚°

```python
from sentence_transformers import SentenceTransformer, util

model = SentenceTransformer('jhgan/ko-sroberta-multitask')

# ë¬¸ì¥ ì„ë² ë”©
sentences = [
    "ë²¼ ìˆ˜í™•ëŸ‰ì´ ì¦ê°€í–ˆìŠµë‹ˆë‹¤.",
    "ìŒ€ ìƒì‚°ëŸ‰ì´ ëŠ˜ì–´ë‚¬ìŠµë‹ˆë‹¤.",
    "ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ë§‘ìŠµë‹ˆë‹¤."
]

embeddings = model.encode(sentences)

# ìœ ì‚¬ë„ í–‰ë ¬ ê³„ì‚°
similarity_matrix = util.cos_sim(embeddings, embeddings)

print("ìœ ì‚¬ë„ í–‰ë ¬:")
print(similarity_matrix)

# ì¶œë ¥:
# tensor([[1.0000, 0.8523, 0.1234],  # ë¬¸ì¥1 vs [ë¬¸ì¥1, ë¬¸ì¥2, ë¬¸ì¥3]
#         [0.8523, 1.0000, 0.1456],  # ë¬¸ì¥2 vs [ë¬¸ì¥1, ë¬¸ì¥2, ë¬¸ì¥3]
#         [0.1234, 0.1456, 1.0000]]) # ë¬¸ì¥3 vs [ë¬¸ì¥1, ë¬¸ì¥2, ë¬¸ì¥3]
```

#### ì˜ˆì‹œ 3: ì˜ë¯¸ ê²€ìƒ‰ (Semantic Search)

```python
from sentence_transformers import SentenceTransformer, util

model = SentenceTransformer('jhgan/ko-sroberta-multitask')

# ë¬¸ì„œ ë°ì´í„°ë² ì´ìŠ¤
documents = [
    "ë†ì´Œì§„í¥ì²­ 2024ë…„ ë²¼ ìˆ˜í™•ëŸ‰ ì—°êµ¬ ë³´ê³ ì„œ",
    "ë†ì—… ê¸°ìˆ  ë°œì „ìœ¼ë¡œ ìƒì‚°ì„± 15% í–¥ìƒ",
    "ê¸°í›„ ë³€í™”ì— ë”°ë¥¸ ë†ì‘ë¬¼ ì¬ë°° ë°©ë²• ë³€í™”",
    "ìŠ¤ë§ˆíŠ¸íŒœ ê¸°ìˆ ì„ í™œìš©í•œ íš¨ìœ¨ì  ë†ì—…",
    "2024ë…„ ì—¬ë¦„ ë¬´ë”ìœ„ë¡œ í­ì—¼ ì£¼ì˜ë³´ ë°œë ¹"
]

# ë¬¸ì„œ ë²¡í„°í™” (í•œ ë²ˆë§Œ ìˆ˜í–‰)
doc_embeddings = model.encode(documents)

# ì‚¬ìš©ì ì¿¼ë¦¬
query = "ìŒ€ ìƒì‚°ëŸ‰ ì¦ê°€"
query_embedding = model.encode(query)

# ìœ ì‚¬ë„ ê³„ì‚°
similarities = util.cos_sim(query_embedding, doc_embeddings)[0]

# ìƒìœ„ 3ê°œ ê²°ê³¼
top_k = 3
top_results = similarities.argsort(descending=True)[:top_k]

print(f"ì¿¼ë¦¬: '{query}'\n")
print("ê²€ìƒ‰ ê²°ê³¼:")
for idx in top_results:
    print(f"  {similarities[idx]:.4f} - {documents[idx]}")

# ì¶œë ¥:
# ì¿¼ë¦¬: 'ìŒ€ ìƒì‚°ëŸ‰ ì¦ê°€'
#
# ê²€ìƒ‰ ê²°ê³¼:
#   0.7234 - ë†ì´Œì§„í¥ì²­ 2024ë…„ ë²¼ ìˆ˜í™•ëŸ‰ ì—°êµ¬ ë³´ê³ ì„œ
#   0.6543 - ë†ì—… ê¸°ìˆ  ë°œì „ìœ¼ë¡œ ìƒì‚°ì„± 15% í–¥ìƒ
#   0.4123 - ìŠ¤ë§ˆíŠ¸íŒœ ê¸°ìˆ ì„ í™œìš©í•œ íš¨ìœ¨ì  ë†ì—…
```

### 3.5 ë‹¤êµ­ì–´ ëª¨ë¸ ì‚¬ìš©

```python
from sentence_transformers import SentenceTransformer

# ë‹¤êµ­ì–´ ëª¨ë¸ ë¡œë“œ
model = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2')

# í•œêµ­ì–´, ì˜ì–´ í˜¼ìš© ë¬¸ì¥
sentences = [
    "ë²¼ ìˆ˜í™•ëŸ‰ì´ ì¦ê°€í–ˆìŠµë‹ˆë‹¤.",
    "Rice production has increased.",  # ê°™ì€ ì˜ë¯¸ì˜ ì˜ì–´
    "The weather is nice today."       # ë‹¤ë¥¸ ì£¼ì œ
]

embeddings = model.encode(sentences)

# í•œ-ì˜ ë¬¸ì¥ ê°„ ìœ ì‚¬ë„
from sentence_transformers import util
similarity = util.cos_sim(embeddings[0], embeddings[1])

print(f"í•œêµ­ì–´-ì˜ì–´ ìœ ì‚¬ë„: {similarity[0][0]:.4f}")  # 0.85 ì´ìƒ (ë†’ì€ ìœ ì‚¬ë„)
```

### 3.6 ë°°ì¹˜ ì²˜ë¦¬

ëŒ€ëŸ‰ì˜ í…ìŠ¤íŠ¸ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì²˜ë¦¬:

```python
from sentence_transformers import SentenceTransformer

model = SentenceTransformer('jhgan/ko-sroberta-multitask')

# ëŒ€ëŸ‰ì˜ ë¬¸ì„œ (ì˜ˆ: 10,000ê°œ)
documents = [f"ë¬¸ì„œ {i}ì˜ ë‚´ìš©ì…ë‹ˆë‹¤." for i in range(10000)]

# ë°°ì¹˜ ì²˜ë¦¬ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )
batch_size = 32
embeddings = model.encode(
    documents,
    batch_size=batch_size,
    show_progress_bar=True  # ì§„í–‰ë¥  í‘œì‹œ
)

print(f"ì´ {len(embeddings)}ê°œ ë¬¸ì„œ ë²¡í„°í™” ì™„ë£Œ")
print(f"ê° ë²¡í„° ì°¨ì›: {embeddings.shape[1]}")
```

---

## 4. ì£¼ì˜ì 

### 4.1 ëª¨ë¸ ì„ íƒ ì‹œ ì£¼ì˜ì‚¬í•­

#### ì£¼ì˜ì  1: ì–¸ì–´ë³„ ìµœì í™” ëª¨ë¸ ì‚¬ìš©

```python
# âŒ ë‚˜ìœ ì˜ˆ: ì˜ì–´ ì „ìš© ëª¨ë¸ë¡œ í•œêµ­ì–´ ì²˜ë¦¬
model = SentenceTransformer('all-MiniLM-L6-v2')  # ì˜ì–´ ì „ìš©
text = "ë²¼ ìˆ˜í™•ëŸ‰ì´ ì¦ê°€í–ˆìŠµë‹ˆë‹¤."
embedding = model.encode(text)  # ì„±ëŠ¥ ë‚®ìŒ

# âœ… ì¢‹ì€ ì˜ˆ: í•œêµ­ì–´ ìµœì í™” ëª¨ë¸
model = SentenceTransformer('jhgan/ko-sroberta-multitask')
text = "ë²¼ ìˆ˜í™•ëŸ‰ì´ ì¦ê°€í–ˆìŠµë‹ˆë‹¤."
embedding = model.encode(text)  # ì„±ëŠ¥ ë†’ìŒ
```

#### ì£¼ì˜ì  2: ëª¨ë¸ í¬ê¸°ì™€ ì„±ëŠ¥ ê· í˜•

| ëª¨ë¸ í¬ê¸° | íŒŒë¼ë¯¸í„° ìˆ˜ | ì†ë„ | ì„±ëŠ¥ | ì¶”ì²œ ìš©ë„ |
|----------|-----------|------|------|----------|
| **Small** | ~60M | ë¹ ë¦„ | ë‚®ìŒ | ì‹¤ì‹œê°„ ì„œë¹„ìŠ¤, ëŒ€ëŸ‰ ì²˜ë¦¬ |
| **Base** | ~110M | ë³´í†µ | ë³´í†µ | ì¼ë°˜ì ì¸ ìš©ë„ |
| **Large** | ~340M | ëŠë¦¼ | ë†’ìŒ | ì •í™•ë„ê°€ ì¤‘ìš”í•œ ê²½ìš° |

```python
# ì†ë„ vs ì •í™•ë„ íŠ¸ë ˆì´ë“œì˜¤í”„
fast_model = SentenceTransformer('jhgan/ko-sbert-nli')      # ë¹ ë¦„, ì„±ëŠ¥ ë³´í†µ
accurate_model = SentenceTransformer('jhgan/ko-sroberta-multitask')  # ëŠë¦¼, ì„±ëŠ¥ ë†’ìŒ

# ìš©ë„ì— ë§ê²Œ ì„ íƒ!
```

### 4.2 ì„ë² ë”© ì°¨ì› ì¼ê´€ì„±

```python
# âš ï¸ ì£¼ì˜: ëª¨ë¸ë§ˆë‹¤ ì¶œë ¥ ì°¨ì›ì´ ë‹¤ë¦„

model_768 = SentenceTransformer('jhgan/ko-sroberta-multitask')  # 768ì°¨ì›
model_384 = SentenceTransformer('paraphrase-MiniLM-L3-v2')      # 384ì°¨ì›

# âŒ ë‹¤ë¥¸ ì°¨ì›ì˜ ë²¡í„° ë¹„êµ ë¶ˆê°€
vec1 = model_768.encode("í…ìŠ¤íŠ¸1")  # shape: (768,)
vec2 = model_384.encode("í…ìŠ¤íŠ¸2")  # shape: (384,)
# similarity = cosine_similarity(vec1, vec2)  # ì˜¤ë¥˜ ë°œìƒ!

# âœ… ê°™ì€ ëª¨ë¸ë¡œ ì¼ê´€ë˜ê²Œ ì‚¬ìš©
model = SentenceTransformer('jhgan/ko-sroberta-multitask')
vec1 = model.encode("í…ìŠ¤íŠ¸1")
vec2 = model.encode("í…ìŠ¤íŠ¸2")
similarity = cosine_similarity(vec1, vec2)  # ì •ìƒ ì‘ë™
```

### 4.3 í† í° ê¸¸ì´ ì œí•œ

```python
# âš ï¸ ëŒ€ë¶€ë¶„ ëª¨ë¸ì€ ìµœëŒ€ 512 í† í° ì œí•œ

long_text = "ë§¤ìš° ê¸´ ë¬¸ì„œ..." * 1000  # í† í° ì œí•œ ì´ˆê³¼

# âŒ ë‚˜ìœ ì˜ˆ: ê¸´ í…ìŠ¤íŠ¸ë¥¼ ê·¸ëŒ€ë¡œ ì…ë ¥
embedding = model.encode(long_text)  # 512 í† í° ì´í›„ ì˜ë¦¼

# âœ… ì¢‹ì€ ì˜ˆ 1: í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë‚˜ëˆ„ê¸°
def chunk_text(text, max_length=500):
    """í…ìŠ¤íŠ¸ë¥¼ ì—¬ëŸ¬ ì²­í¬ë¡œ ë¶„í• """
    words = text.split()
    chunks = []
    current_chunk = []
    current_length = 0

    for word in words:
        if current_length + len(word) > max_length:
            chunks.append(' '.join(current_chunk))
            current_chunk = [word]
            current_length = len(word)
        else:
            current_chunk.append(word)
            current_length += len(word) + 1

    if current_chunk:
        chunks.append(' '.join(current_chunk))

    return chunks

# ì²­í¬ë³„ë¡œ ì„ë² ë”© í›„ í‰ê· 
chunks = chunk_text(long_text)
chunk_embeddings = model.encode(chunks)
doc_embedding = chunk_embeddings.mean(axis=0)  # í‰ê·  ë²¡í„°

# âœ… ì¢‹ì€ ì˜ˆ 2: ìš”ì•½ í›„ ì„ë² ë”©
summary = summarize(long_text)  # ìš”ì•½ í•¨ìˆ˜
embedding = model.encode(summary)
```

### 4.4 ì •ê·œí™”(Normalization)

```python
# ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° ì‹œ ë²¡í„° ì •ê·œí™” ê¶Œì¥

import numpy as np

def normalize_embedding(embedding):
    """ë²¡í„°ë¥¼ ë‹¨ìœ„ ë²¡í„°ë¡œ ì •ê·œí™”"""
    norm = np.linalg.norm(embedding)
    return embedding / norm

# ì •ê·œí™”í•˜ë©´ ë‚´ì (dot product)ë§Œìœ¼ë¡œ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° ê°€ëŠ¥
vec1 = normalize_embedding(model.encode("í…ìŠ¤íŠ¸1"))
vec2 = normalize_embedding(model.encode("í…ìŠ¤íŠ¸2"))

# ì½”ì‚¬ì¸ ìœ ì‚¬ë„ = ë‚´ì 
similarity = np.dot(vec1, vec2)  # ë¹ ë¦„!

# vs ì •ê·œí™” ì—†ì´
# similarity = np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))  # ëŠë¦¼
```

### 4.5 ëª¨ë¸ ìºì‹±

```python
# âš ï¸ ëª¨ë¸ì„ ë°˜ë³µ ë¡œë“œí•˜ë©´ ë©”ëª¨ë¦¬/ì‹œê°„ ë‚­ë¹„

# âŒ ë‚˜ìœ ì˜ˆ: í•¨ìˆ˜ í˜¸ì¶œë§ˆë‹¤ ëª¨ë¸ ë¡œë“œ
def embed_text(text):
    model = SentenceTransformer('jhgan/ko-sroberta-multitask')  # ë§¤ë²ˆ ë¡œë“œ!
    return model.encode(text)

# âœ… ì¢‹ì€ ì˜ˆ: ëª¨ë¸ì„ í•œ ë²ˆë§Œ ë¡œë“œ
class EmbeddingService:
    def __init__(self, model_name='jhgan/ko-sroberta-multitask'):
        self.model = SentenceTransformer(model_name)  # í•œ ë²ˆë§Œ ë¡œë“œ

    def embed(self, text):
        return self.model.encode(text)

# ì‚¬ìš©
service = EmbeddingService()
emb1 = service.embed("í…ìŠ¤íŠ¸1")
emb2 = service.embed("í…ìŠ¤íŠ¸2")
```

### 4.6 GPU í™œìš©

```python
# GPUê°€ ìˆë‹¤ë©´ í™œìš©í•˜ì—¬ ì†ë„ í–¥ìƒ

import torch

# GPU ì‚¬ìš© ê°€ëŠ¥ í™•ì¸
device = 'cuda' if torch.cuda.is_available() else 'cpu'
print(f"ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")

# ëª¨ë¸ì„ GPUë¡œ ì´ë™
model = SentenceTransformer('jhgan/ko-sroberta-multitask', device=device)

# ëŒ€ëŸ‰ ì²˜ë¦¬ ì‹œ 10~100ë°° ë¹ ë¦„!
documents = ["ë¬¸ì„œ " + str(i) for i in range(10000)]
embeddings = model.encode(documents, batch_size=64, show_progress_bar=True)
```

---

## 5. ì‹¤ì „ ì˜ˆì‹œ

### 5.1 ë¬¸ì„œ ê²€ìƒ‰ ì‹œìŠ¤í…œ êµ¬ì¶•

```python
from sentence_transformers import SentenceTransformer, util
import numpy as np

class DocumentSearchSystem:
    """ì„ë² ë”© ê¸°ë°˜ ë¬¸ì„œ ê²€ìƒ‰ ì‹œìŠ¤í…œ"""

    def __init__(self, model_name='jhgan/ko-sroberta-multitask'):
        """
        Args:
            model_name: ì‚¬ìš©í•  ì„ë² ë”© ëª¨ë¸ëª…
        """
        self.model = SentenceTransformer(model_name)
        self.documents = []
        self.embeddings = None

    def add_documents(self, documents: list):
        """
        ë¬¸ì„œ ì¶”ê°€ ë° ì„ë² ë”©

        Args:
            documents: ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        """
        self.documents.extend(documents)

        # ìƒˆ ë¬¸ì„œë“¤ ì„ë² ë”©
        new_embeddings = self.model.encode(
            documents,
            batch_size=32,
            show_progress_bar=True,
            convert_to_numpy=True
        )

        # ê¸°ì¡´ ì„ë² ë”©ê³¼ í•©ì¹˜ê¸°
        if self.embeddings is None:
            self.embeddings = new_embeddings
        else:
            self.embeddings = np.vstack([self.embeddings, new_embeddings])

        print(f"ì´ {len(self.documents)}ê°œ ë¬¸ì„œ ì¸ë±ì‹± ì™„ë£Œ")

    def search(self, query: str, top_k: int = 5):
        """
        ì¿¼ë¦¬ì™€ ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œ ê²€ìƒ‰

        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            top_k: ë°˜í™˜í•  ê²°ê³¼ ê°œìˆ˜

        Returns:
            ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ [(ì ìˆ˜, ë¬¸ì„œ), ...]
        """
        if not self.documents:
            return []

        # ì¿¼ë¦¬ ì„ë² ë”©
        query_embedding = self.model.encode(query, convert_to_numpy=True)

        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        similarities = util.cos_sim(query_embedding, self.embeddings)[0]

        # ìƒìœ„ kê°œ ê²°ê³¼
        top_results = similarities.argsort(descending=True)[:top_k]

        results = []
        for idx in top_results:
            results.append({
                'score': float(similarities[idx]),
                'document': self.documents[idx],
                'index': int(idx)
            })

        return results

    def save_index(self, file_path):
        """ì¸ë±ìŠ¤ ì €ì¥"""
        np.savez(
            file_path,
            embeddings=self.embeddings,
            documents=self.documents
        )

    def load_index(self, file_path):
        """ì¸ë±ìŠ¤ ë¡œë“œ"""
        data = np.load(file_path, allow_pickle=True)
        self.embeddings = data['embeddings']
        self.documents = data['documents'].tolist()


# ì‚¬ìš© ì˜ˆì‹œ
search_system = DocumentSearchSystem()

# ë¬¸ì„œ ì¶”ê°€
documents = [
    "ë†ì´Œì§„í¥ì²­ 2024ë…„ ë²¼ ìˆ˜í™•ëŸ‰ ì—°êµ¬ ë³´ê³ ì„œ",
    "ë†ì—… ê¸°ìˆ  ë°œì „ìœ¼ë¡œ ìƒì‚°ì„± 15% í–¥ìƒ",
    "ê¸°í›„ ë³€í™”ì— ë”°ë¥¸ ë†ì‘ë¬¼ ì¬ë°° ë°©ë²• ë³€í™”",
    "ìŠ¤ë§ˆíŠ¸íŒœ ê¸°ìˆ ì„ í™œìš©í•œ íš¨ìœ¨ì  ë†ì—…",
    "2024ë…„ ì—¬ë¦„ ë¬´ë”ìœ„ë¡œ í­ì—¼ ì£¼ì˜ë³´ ë°œë ¹",
    "ì¹œí™˜ê²½ ìœ ê¸°ë† ì¬ë°° ê¸°ìˆ  ê°œë°œ",
    "ë“œë¡ ì„ í™œìš©í•œ ë†ì‘ë¬¼ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ"
]

search_system.add_documents(documents)

# ê²€ìƒ‰
query = "ìŒ€ ìƒì‚°ëŸ‰ ì¦ê°€"
results = search_system.search(query, top_k=3)

print(f"\nê²€ìƒ‰ì–´: '{query}'\n")
for i, result in enumerate(results, 1):
    print(f"{i}. [ì ìˆ˜: {result['score']:.4f}] {result['document']}")

# ì¸ë±ìŠ¤ ì €ì¥
search_system.save_index('document_index.npz')
```

### 5.2 ì²­í¬ ë‹¨ìœ„ ì„ë² ë”©

```python
from sentence_transformers import SentenceTransformer

def embed_document_with_chunks(
    text: str,
    model: SentenceTransformer,
    chunk_size: int = 500,
    overlap: int = 100
):
    """
    ê¸´ ë¬¸ì„œë¥¼ ì²­í¬ë¡œ ë‚˜ëˆ„ì–´ ì„ë² ë”©

    Args:
        text: ì›ë³¸ í…ìŠ¤íŠ¸
        model: ì„ë² ë”© ëª¨ë¸
        chunk_size: ì²­í¬ í¬ê¸° (ë¬¸ì ìˆ˜)
        overlap: ì²­í¬ ê°„ ì¤‘ë³µ í¬ê¸°

    Returns:
        ì²­í¬ ì •ë³´ ë¦¬ìŠ¤íŠ¸
    """
    chunks = []
    start = 0

    while start < len(text):
        end = start + chunk_size

        # ì²­í¬ ì¶”ì¶œ
        chunk_text = text[start:end]

        # ì„ë² ë”©
        embedding = model.encode(chunk_text)

        chunks.append({
            'text': chunk_text,
            'embedding': embedding,
            'start': start,
            'end': end,
            'chunk_index': len(chunks)
        })

        # ë‹¤ìŒ ì²­í¬ ì‹œì‘ ìœ„ì¹˜ (ì˜¤ë²„ë© ê³ ë ¤)
        start += chunk_size - overlap

    return chunks


# ì‚¬ìš© ì˜ˆì‹œ
model = SentenceTransformer('jhgan/ko-sroberta-multitask')

long_document = """
ë†ì´Œì§„í¥ì²­ì˜ 2024ë…„ ë²¼ ìˆ˜í™•ëŸ‰ ì—°êµ¬ì— ë”°ë¥´ë©´,
ì˜¬í•´ ë²¼ ìˆ˜í™•ëŸ‰ì´ ì „ë…„ ëŒ€ë¹„ 15% ì¦ê°€í•œ ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ìŠµë‹ˆë‹¤.
ì´ëŠ” ì‹ í’ˆì¢… ê°œë°œê³¼ ìŠ¤ë§ˆíŠ¸íŒœ ê¸°ìˆ ì˜ ë„ì…ìœ¼ë¡œ
ì¬ë°° íš¨ìœ¨ì„±ì´ í¬ê²Œ í–¥ìƒëœ ê²°ê³¼ì…ë‹ˆë‹¤.
íŠ¹íˆ ê¸°í›„ ë³€í™”ì— ê°•í•œ í’ˆì¢…ì´ ê°œë°œë˜ì–´
ì•ˆì •ì ì¸ ìƒì‚°ì´ ê°€ëŠ¥í•´ì¡ŒìŠµë‹ˆë‹¤.
""" * 10  # ê¸´ ë¬¸ì„œ ì‹œë®¬ë ˆì´ì…˜

chunks = embed_document_with_chunks(long_document, model)

print(f"ì´ {len(chunks)}ê°œ ì²­í¬ ìƒì„±")
print(f"ê° ì²­í¬ ì„ë² ë”© ì°¨ì›: {chunks[0]['embedding'].shape}")
```

### 5.3 ë©”íƒ€ë°ì´í„°ì™€ í•¨ê»˜ ë²¡í„° ì €ì¥

```python
from sentence_transformers import SentenceTransformer
import json
from datetime import datetime

class DocumentEmbeddingManager:
    """ë¬¸ì„œ ì„ë² ë”©ê³¼ ë©”íƒ€ë°ì´í„° ê´€ë¦¬"""

    def __init__(self, model_name='jhgan/ko-sroberta-multitask'):
        self.model = SentenceTransformer(model_name)
        self.documents = []

    def add_document(self, text: str, metadata: dict):
        """
        ë¬¸ì„œ ì¶”ê°€ (ì„ë² ë”© + ë©”íƒ€ë°ì´í„°)

        Args:
            text: ë¬¸ì„œ í…ìŠ¤íŠ¸
            metadata: ë©”íƒ€ë°ì´í„°
        """
        # ì„ë² ë”© ìƒì„±
        embedding = self.model.encode(text)

        # ë¬¸ì„œ ì •ë³´ ì €ì¥
        doc_info = {
            'text': text,
            'embedding': embedding.tolist(),  # numpy â†’ list
            'metadata': metadata,
            'embedded_at': datetime.now().isoformat()
        }

        self.documents.append(doc_info)

    def search_with_filter(self, query: str, metadata_filter: dict = None, top_k: int = 5):
        """
        ë©”íƒ€ë°ì´í„° í•„í„°ì™€ í•¨ê»˜ ê²€ìƒ‰

        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            metadata_filter: ë©”íƒ€ë°ì´í„° í•„í„° ì¡°ê±´
            top_k: ë°˜í™˜ ê²°ê³¼ ê°œìˆ˜

        Returns:
            í•„í„°ë§ëœ ê²€ìƒ‰ ê²°ê³¼
        """
        # ì¿¼ë¦¬ ì„ë² ë”©
        query_embedding = self.model.encode(query)

        # ë©”íƒ€ë°ì´í„° í•„í„° ì ìš©
        candidates = self.documents

        if metadata_filter:
            candidates = [
                doc for doc in self.documents
                if all(
                    doc['metadata'].get(key) == value
                    for key, value in metadata_filter.items()
                )
            ]

        if not candidates:
            return []

        # ìœ ì‚¬ë„ ê³„ì‚°
        import numpy as np
        from sentence_transformers import util

        candidate_embeddings = np.array([doc['embedding'] for doc in candidates])
        similarities = util.cos_sim(query_embedding, candidate_embeddings)[0]

        # ìƒìœ„ kê°œ ê²°ê³¼
        top_indices = similarities.argsort(descending=True)[:top_k]

        results = []
        for idx in top_indices:
            results.append({
                'score': float(similarities[idx]),
                'text': candidates[idx]['text'],
                'metadata': candidates[idx]['metadata']
            })

        return results

    def save(self, file_path):
        """JSONìœ¼ë¡œ ì €ì¥"""
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(self.documents, f, ensure_ascii=False, indent=2)

    def load(self, file_path):
        """JSONì—ì„œ ë¡œë“œ"""
        with open(file_path, 'r', encoding='utf-8') as f:
            self.documents = json.load(f)


# ì‚¬ìš© ì˜ˆì‹œ
manager = DocumentEmbeddingManager()

# ë¬¸ì„œ ì¶”ê°€ (ë©”íƒ€ë°ì´í„° í¬í•¨)
manager.add_document(
    text="2024ë…„ ë²¼ ìˆ˜í™•ëŸ‰ì´ 15% ì¦ê°€í–ˆìŠµë‹ˆë‹¤.",
    metadata={
        'category': 'Research Report',
        'year': 2024,
        'department': 'RDA',
        'author': 'Bern'
    }
)

manager.add_document(
    text="ìŠ¤ë§ˆíŠ¸íŒœ ê¸°ìˆ ë¡œ ìƒì‚°ì„±ì´ í–¥ìƒë˜ì—ˆìŠµë‹ˆë‹¤.",
    metadata={
        'category': 'Technology Report',
        'year': 2024,
        'department': 'RDA',
        'author': 'Bern'
    }
)

manager.add_document(
    text="2023ë…„ ê¸°í›„ ë³€í™” ì˜í–¥ ë¶„ì„ ë³´ê³ ì„œ",
    metadata={
        'category': 'Research Report',
        'year': 2023,
        'department': 'Environment',
        'author': 'Kim'
    }
)

# ê²€ìƒ‰ 1: ë©”íƒ€ë°ì´í„° í•„í„° ì—†ì´
results = manager.search_with_filter("ìƒì‚°ëŸ‰ ì¦ê°€", top_k=2)
print("ì¼ë°˜ ê²€ìƒ‰ ê²°ê³¼:")
for r in results:
    print(f"  [{r['score']:.4f}] {r['text']}")

# ê²€ìƒ‰ 2: ë©”íƒ€ë°ì´í„° í•„í„° ì ìš©
results = manager.search_with_filter(
    query="ìƒì‚°ëŸ‰ ì¦ê°€",
    metadata_filter={'year': 2024, 'department': 'RDA'},
    top_k=2
)
print("\ní•„í„°ë§ëœ ê²€ìƒ‰ ê²°ê³¼ (2024ë…„ + RDA):")
for r in results:
    print(f"  [{r['score']:.4f}] {r['text']} (year: {r['metadata']['year']})")

# ì €ì¥
manager.save('embeddings_with_metadata.json')
```

---

## 6. ì„±ëŠ¥ ìµœì í™”

### 6.1 ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ í™œìš©

ëŒ€ê·œëª¨ ë²¡í„° ê²€ìƒ‰ì„ ìœ„í•´ ì „ìš© ë°ì´í„°ë² ì´ìŠ¤ ì‚¬ìš©:

#### FAISS (Facebook AI Similarity Search)

```python
import faiss
import numpy as np
from sentence_transformers import SentenceTransformer

class FAISSSearchEngine:
    """FAISSë¥¼ í™œìš©í•œ ê³ ì† ë²¡í„° ê²€ìƒ‰"""

    def __init__(self, model_name='jhgan/ko-sroberta-multitask'):
        self.model = SentenceTransformer(model_name)
        self.index = None
        self.documents = []
        self.dimension = 768  # ëª¨ë¸ ì¶œë ¥ ì°¨ì›

    def build_index(self, documents: list):
        """
        ë¬¸ì„œ ì¸ë±ìŠ¤ êµ¬ì¶•

        Args:
            documents: ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        """
        self.documents = documents

        # ì„ë² ë”© ìƒì„±
        print("ë¬¸ì„œ ì„ë² ë”© ì¤‘...")
        embeddings = self.model.encode(
            documents,
            batch_size=32,
            show_progress_bar=True,
            convert_to_numpy=True
        )

        # FAISS ì¸ë±ìŠ¤ ìƒì„±
        print("FAISS ì¸ë±ìŠ¤ êµ¬ì¶• ì¤‘...")
        self.index = faiss.IndexFlatIP(self.dimension)  # ë‚´ì  (ì½”ì‚¬ì¸ ìœ ì‚¬ë„)

        # ë²¡í„° ì •ê·œí™” (ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ìœ„í•´)
        faiss.normalize_L2(embeddings)

        # ì¸ë±ìŠ¤ì— ì¶”ê°€
        self.index.add(embeddings)

        print(f"ì¸ë±ìŠ¤ êµ¬ì¶• ì™„ë£Œ: {self.index.ntotal}ê°œ ë²¡í„°")

    def search(self, query: str, top_k: int = 5):
        """
        ê³ ì† ê²€ìƒ‰

        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            top_k: ë°˜í™˜ ê²°ê³¼ ê°œìˆ˜

        Returns:
            ê²€ìƒ‰ ê²°ê³¼
        """
        # ì¿¼ë¦¬ ì„ë² ë”©
        query_embedding = self.model.encode([query], convert_to_numpy=True)
        faiss.normalize_L2(query_embedding)

        # FAISS ê²€ìƒ‰
        scores, indices = self.index.search(query_embedding, top_k)

        # ê²°ê³¼ êµ¬ì„±
        results = []
        for score, idx in zip(scores[0], indices[0]):
            if idx < len(self.documents):
                results.append({
                    'score': float(score),
                    'document': self.documents[idx],
                    'index': int(idx)
                })

        return results

    def save_index(self, index_path, docs_path):
        """ì¸ë±ìŠ¤ ì €ì¥"""
        faiss.write_index(self.index, index_path)
        np.save(docs_path, self.documents)

    def load_index(self, index_path, docs_path):
        """ì¸ë±ìŠ¤ ë¡œë“œ"""
        self.index = faiss.read_index(index_path)
        self.documents = np.load(docs_path, allow_pickle=True).tolist()


# ì‚¬ìš© ì˜ˆì‹œ
engine = FAISSSearchEngine()

# ëŒ€ëŸ‰ì˜ ë¬¸ì„œ (ì˜ˆ: 10ë§Œ ê°œ)
documents = [f"ë¬¸ì„œ {i}ì˜ ë‚´ìš©ì…ë‹ˆë‹¤." for i in range(100000)]

# ì¸ë±ìŠ¤ êµ¬ì¶•
engine.build_index(documents)

# ê²€ìƒ‰ (ë§¤ìš° ë¹ ë¦„!)
import time
start = time.time()
results = engine.search("íŠ¹ì • ì£¼ì œì— ëŒ€í•œ ë¬¸ì„œ", top_k=10)
end = time.time()

print(f"ê²€ìƒ‰ ì‹œê°„: {(end - start) * 1000:.2f}ms")  # ìˆ˜ ms ì´ë‚´!

# ì¸ë±ìŠ¤ ì €ì¥
engine.save_index('faiss.index', 'documents.npy')
```

### 6.2 ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”

```python
from sentence_transformers import SentenceTransformer
from tqdm import tqdm

def batch_encode_documents(documents: list, model_name: str, batch_size: int = 32):
    """
    ëŒ€ëŸ‰ ë¬¸ì„œì˜ íš¨ìœ¨ì  ë°°ì¹˜ ì²˜ë¦¬

    Args:
        documents: ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        model_name: ëª¨ë¸ëª…
        batch_size: ë°°ì¹˜ í¬ê¸°

    Returns:
        ì„ë² ë”© ë°°ì—´
    """
    model = SentenceTransformer(model_name)

    embeddings = []

    # ë°°ì¹˜ë³„ë¡œ ì²˜ë¦¬
    for i in tqdm(range(0, len(documents), batch_size)):
        batch = documents[i:i + batch_size]
        batch_embeddings = model.encode(batch, convert_to_numpy=True)
        embeddings.append(batch_embeddings)

    # í•˜ë‚˜ì˜ ë°°ì—´ë¡œ í•©ì¹˜ê¸°
    import numpy as np
    return np.vstack(embeddings)
```

### 6.3 ìºì‹± ì „ëµ

```python
from functools import lru_cache
import hashlib

class CachedEmbeddingModel:
    """ìºì‹± ê¸°ëŠ¥ì´ ìˆëŠ” ì„ë² ë”© ëª¨ë¸"""

    def __init__(self, model_name='jhgan/ko-sroberta-multitask', cache_size=1000):
        self.model = SentenceTransformer(model_name)
        self.cache = {}
        self.cache_size = cache_size

    def _get_cache_key(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ì˜ í•´ì‹œ í‚¤ ìƒì„±"""
        return hashlib.md5(text.encode()).hexdigest()

    def encode(self, text: str):
        """ìºì‹œë¥¼ í™œìš©í•œ ì„ë² ë”©"""
        cache_key = self._get_cache_key(text)

        # ìºì‹œ íˆíŠ¸
        if cache_key in self.cache:
            return self.cache[cache_key]

        # ìºì‹œ ë¯¸ìŠ¤ â†’ ì„ë² ë”© ìƒì„±
        embedding = self.model.encode(text)

        # ìºì‹œ í¬ê¸° ì œí•œ
        if len(self.cache) >= self.cache_size:
            # ê°€ì¥ ì˜¤ë˜ëœ í•­ëª© ì œê±°
            oldest = next(iter(self.cache))
            del self.cache[oldest]

        # ìºì‹œ ì €ì¥
        self.cache[cache_key] = embedding

        return embedding


# ì‚¬ìš© ì˜ˆì‹œ
cached_model = CachedEmbeddingModel()

# ì²« í˜¸ì¶œ: ëª¨ë¸ ì‹¤í–‰
emb1 = cached_model.encode("ë²¼ ìˆ˜í™•ëŸ‰")  # ëŠë¦¼

# ë‘ ë²ˆì§¸ í˜¸ì¶œ: ìºì‹œì—ì„œ ë°˜í™˜
emb2 = cached_model.encode("ë²¼ ìˆ˜í™•ëŸ‰")  # ë§¤ìš° ë¹ ë¦„!
```

---

## ì •ë¦¬

### í•µì‹¬ ìš”ì•½

1. **ì„ë² ë”©ì´ë€?**
   - í…ìŠ¤íŠ¸ë¥¼ ê³ ì • ê¸¸ì´ì˜ ìˆ«ì ë²¡í„°ë¡œ ë³€í™˜
   - ì˜ë¯¸ì ìœ¼ë¡œ ìœ ì‚¬í•œ í…ìŠ¤íŠ¸ëŠ” ë²¡í„° ê³µê°„ì—ì„œ ê°€ê¹Œì›€

2. **Hugging Face í™œìš©**
   - ìˆ˜ë§Œ ê°œì˜ ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ ì œê³µ
   - `sentence-transformers` ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ ì‰½ê²Œ ì‚¬ìš©
   - í•œêµ­ì–´ ì¶”ì²œ ëª¨ë¸: `jhgan/ko-sroberta-multitask`

3. **ì£¼ì˜ì‚¬í•­**
   - ì–¸ì–´ë³„ ìµœì í™” ëª¨ë¸ ì‚¬ìš©
   - í† í° ê¸¸ì´ ì œí•œ (ë³´í†µ 512 í† í°)
   - ëª¨ë¸ ìºì‹±ìœ¼ë¡œ ì„±ëŠ¥ í–¥ìƒ
   - GPU í™œìš© ì‹œ 10~100ë°° ë¹ ë¦„

4. **ì‹¤ì „ í™œìš©**
   - ë¬¸ì„œ ê²€ìƒ‰ ì‹œìŠ¤í…œ
   - ìœ ì‚¬ ë¬¸ì„œ ì¶”ì²œ
   - ë©”íƒ€ë°ì´í„°ì™€ ê²°í•©í•œ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
   - FAISSë¡œ ëŒ€ê·œëª¨ ë²¡í„° ê²€ìƒ‰

### ë‹¤ìŒ ë‹¨ê³„

ì„ë² ë”©ì„ ìƒì„±í–ˆë‹¤ë©´, ì´ì œ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•˜ê³  ì‹¤ì œ ê²€ìƒ‰ ì‹œìŠ¤í…œì„ êµ¬ì¶•í•  ì°¨ë¡€ì…ë‹ˆë‹¤.

---

**ë¬¸ì„œ ë²„ì „**: 1.0
**ìµœì¢… ìˆ˜ì •ì¼**: 2025-10-24
**ì‘ì„±ì**: Bern

## ğŸ“Œ ë‹¤ìŒ ë‹¨ê³„

ì„ë² ë”©ê³¼ ë²¡í„°í™”ë¥¼ ìµí˜”ë‹¤ë©´, ë‹¤ìŒì€ **Elasticsearchë¥¼ í™œìš©í•œ ë²¡í„° ì„œì¹˜**ì…ë‹ˆë‹¤.

**ë‹¤ìŒ ë¬¸ì„œ**: [1.2.08. Elasticsearchë¥¼ í™œìš©í•œ ë²¡í„° ì„œì¹˜](./1.2.08.Elasticsearchë¥¼%20í™œìš©í•œ%20ë²¡í„°%20ì„œì¹˜.md)
