# 1.2.11. Agent, RAG 개발 - LLM 호출

> **작성자**: Bern
> **작성일**: 2025-10-25
> **카테고리**: LLM / API

---

## 목차
1. [LLM 호출 개요](#1-llm-호출-개요)
2. [OpenAI API 호출](#2-openai-api-호출)
3. [HuggingFace Transformers 모델 호출](#3-huggingface-transformers-모델-호출)
4. [Config 파일 관리](#4-config-파일-관리)
5. [실전 예시 및 실행 결과](#5-실전-예시-및-실행-결과)
6. [다음 단계](#6-다음-단계)

---

## 1. LLM 호출 개요

### 1.1. LLM(Large Language Model)이란?

**LLM**은 대규모 텍스트 데이터로 학습된 언어 모델로, 텍스트 생성, 질문 응답, 요약 등 다양한 자연어 처리 작업을 수행할 수 있습니다.

### 1.2. 주요 LLM 서비스

| 서비스 | 설명 | 특징 |
|--------|------|------|
| **OpenAI** | GPT-4, GPT-3.5 등 | 강력한 성능, API 제공 |
| **HuggingFace** | 오픈소스 모델 허브 | 무료, 로컬 실행 가능 |
| **Anthropic** | Claude | 안전성 중심 |
| **Google** | Gemini | 멀티모달 지원 |

### 1.3. 호출 방식

```
1. API 호출: 클라우드 서비스 (OpenAI, Anthropic 등)
   - 장점: 설정 간단, 강력한 성능
   - 단점: 비용 발생, 인터넷 필요

2. 로컬 모델: HuggingFace Transformers
   - 장점: 무료, 데이터 보안
   - 단점: 하드웨어 요구사항, 성능 제한
```

---

## 2. OpenAI API 호출

### 2.1. 라이브러리 설치

```bash
pip install openai
```

### 2.2. 기본 호출

```python
from openai import OpenAI

# API 키 설정 (환경 변수 사용 권장)
import os
api_key = os.getenv('OPENAI_API_KEY')  # 또는 직접 입력: "your-api-key-here"

# OpenAI 클라이언트 생성
client = OpenAI(api_key=api_key)

# 기본 채팅 완성 (Chat Completion)
response = client.chat.completions.create(
    model="gpt-3.5-turbo",  # 모델 선택
    messages=[
        {"role": "system", "content": "당신은 농업 전문가입니다."},
        {"role": "user", "content": "스마트팜의 주요 이점은 무엇인가요?"}
    ],
    temperature=0.7,  # 창의성 (0~2, 낮을수록 일관성↑)
    max_tokens=500    # 최대 토큰 수
)

# 응답 추출
answer = response.choices[0].message.content
print(f"답변: {answer}")

# 사용 토큰 수 확인
print(f"\n토큰 사용량:")
print(f"  입력: {response.usage.prompt_tokens}")
print(f"  출력: {response.usage.completion_tokens}")
print(f"  총합: {response.usage.total_tokens}")
```

**실행 결과**:
```
답변: 스마트팜의 주요 이점은 다음과 같습니다:

1. 생산성 향상: ICT 기술을 활용하여 작물의 생육 환경을 최적화함으로써
   생산량이 30% 이상 증가할 수 있습니다.

2. 노동력 절감: 자동화된 시스템으로 관수, 온도 조절 등이 자동으로 이루어져
   노동력을 50% 이상 절감할 수 있습니다.

3. 에너지 효율: 정밀한 환경 제어로 에너지 비용을 20% 정도 절감할 수 있습니다.

4. 품질 향상: 일정한 환경 유지로 농산물의 품질이 균일하게 향상됩니다.

토큰 사용량:
  입력: 45
  출력: 180
  총합: 225
```

### 2.3. 스트리밍 응답

```python
# 스트리밍으로 실시간 응답 받기
print("스트리밍 응답:\n")

stream = client.chat.completions.create(
    model="gpt-3.5-turbo",
    messages=[
        {"role": "user", "content": "스마트팜 센서의 종류를 설명해주세요."}
    ],
    stream=True  # 스트리밍 활성화
)

# 실시간으로 출력
for chunk in stream:
    if chunk.choices[0].delta.content is not None:
        print(chunk.choices[0].delta.content, end="", flush=True)

print("\n")
```

### 2.4. 함수 호출(Function Calling)

```python
# 함수 정의
functions = [
    {
        "name": "get_weather",
        "description": "특정 지역의 날씨 정보를 조회합니다",
        "parameters": {
            "type": "object",
            "properties": {
                "location": {
                    "type": "string",
                    "description": "지역명 (예: 서울, 부산)"
                }
            },
            "required": ["location"]
        }
    }
]

# 함수 호출 포함 요청
response = client.chat.completions.create(
    model="gpt-3.5-turbo",
    messages=[
        {"role": "user", "content": "서울의 날씨를 알려주세요"}
    ],
    functions=functions,
    function_call="auto"
)

# 함수 호출 확인
message = response.choices[0].message

if message.function_call:
    function_name = message.function_call.name
    function_args = message.function_call.arguments
    print(f"호출할 함수: {function_name}")
    print(f"인자: {function_args}")
```

---

## 3. HuggingFace Transformers 모델 호출

### 3.1. 라이브러리 설치

```bash
pip install transformers torch
```

### 3.2. 텍스트 생성 모델

```python
from transformers import pipeline

# 텍스트 생성 파이프라인 (한국어 모델)
generator = pipeline(
    'text-generation',
    model='skt/kogpt2-base-v2'  # SKT KoGPT-2
)

# 텍스트 생성
prompt = "스마트팜은"
result = generator(
    prompt,
    max_length=100,      # 최대 길이
    num_return_sequences=1,  # 생성 개수
    temperature=0.8,     # 창의성
    top_p=0.9,          # Nucleus sampling
    do_sample=True      # 샘플링 활성화
)

print(f"입력: {prompt}")
print(f"생성: {result[0]['generated_text']}")
```

**실행 결과**:
```
입력: 스마트팜은
생성: 스마트팜은 정보통신기술(ICT)을 활용하여 농작물의 생육 환경을
자동으로 제어하는 첨단 농업 시스템입니다. 센서를 통해 온도, 습도,
CO2 농도 등을 실시간으로 측정하고 최적의 환경을 유지합니다.
```

### 3.3. 질문 응답 모델

```python
from transformers import pipeline

# 질문 응답 파이프라인
qa_pipeline = pipeline(
    'question-answering',
    model='klue/roberta-base'  # KLUE RoBERTa
)

# 컨텍스트와 질문
context = """
스마트팜은 ICT 기술을 활용하여 농작물의 생육 환경을 자동으로 제어하는
농업 방식입니다. 센서를 통해 온도, 습도, CO2 농도, 일조량 등을 측정하고,
이 데이터를 기반으로 최적의 환경을 유지합니다. 이를 통해 농작물의
생산성을 높이고 노동력을 절감할 수 있습니다.
"""

question = "스마트팜에서 측정하는 환경 요소는?"

# 질문 응답
result = qa_pipeline(question=question, context=context)

print(f"질문: {question}")
print(f"답변: {result['answer']}")
print(f"신뢰도: {result['score']:.2%}")
```

**실행 결과**:
```
질문: 스마트팜에서 측정하는 환경 요소는?
답변: 온도, 습도, CO2 농도, 일조량
신뢰도: 94.23%
```

### 3.4. 임베딩 생성

```python
from transformers import AutoTokenizer, AutoModel
import torch

# 모델 및 토크나이저 로드
model_name = 'jhgan/ko-sroberta-multitask'
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name)

# 텍스트 임베딩 생성
def get_embedding(text):
    """텍스트 임베딩 생성"""
    # 토큰화
    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)

    # 모델 실행 (gradient 계산 불필요)
    with torch.no_grad():
        outputs = model(**inputs)

    # [CLS] 토큰의 임베딩 사용
    embedding = outputs.last_hidden_state[:, 0, :].numpy()[0]

    return embedding

# 임베딩 생성
text = "스마트팜은 ICT 기술을 활용한 농업입니다."
embedding = get_embedding(text)

print(f"텍스트: {text}")
print(f"임베딩 차원: {embedding.shape}")
print(f"임베딩 (일부): {embedding[:5]}")
```

**실행 결과**:
```
텍스트: 스마트팜은 ICT 기술을 활용한 농업입니다.
임베딩 차원: (768,)
임베딩 (일부): [ 0.234 -0.567  0.891 -0.123  0.456]
```

---

## 4. Config 파일 관리

### 4.1. YAML Config 파일 생성

**파일**: `config/llm_config.yaml`

```yaml
# LLM 설정 파일

openai:
  api_key: "${OPENAI_API_KEY}"  # 환경 변수에서 로드
  model: "gpt-3.5-turbo"
  temperature: 0.7
  max_tokens: 500

huggingface:
  text_generation:
    model: "skt/kogpt2-base-v2"
    max_length: 100
    temperature: 0.8

  question_answering:
    model: "klue/roberta-base"

  embedding:
    model: "jhgan/ko-sroberta-multitask"

rag:
  top_k: 3  # 검색할 문서 수
  chunk_size: 500
  overlap: 50
```

### 4.2. Config 로더 클래스

```python
import yaml
from typing import Dict, Any

class ConfigLoader:
    """Config 파일 로더"""

    def __init__(self, config_path: str):
        """
        Args:
            config_path (str): Config 파일 경로
        """
        self.config_path = config_path
        self.config = self.load_config()

    def load_config(self) -> Dict[str, Any]:
        """Config 파일 로드"""
        with open(self.config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        return config

    def get(self, key_path: str, default=None):
        """
        중첩된 키 경로로 값 조회

        Args:
            key_path (str): 키 경로 (예: "openai.model")
            default: 기본값

        Returns:
            설정 값
        """
        keys = key_path.split('.')
        value = self.config

        for key in keys:
            if isinstance(value, dict) and key in value:
                value = value[key]
            else:
                return default

        return value

    def get_openai_config(self) -> Dict[str, Any]:
        """OpenAI 설정 조회"""
        return self.config.get('openai', {})

    def get_huggingface_config(self) -> Dict[str, Any]:
        """HuggingFace 설정 조회"""
        return self.config.get('huggingface', {})


# 사용 예시
config = ConfigLoader('config/llm_config.yaml')

# 설정 조회
api_key = config.get('openai.api_key')
model = config.get('openai.model')
temperature = config.get('openai.temperature')

print(f"OpenAI 모델: {model}")
print(f"Temperature: {temperature}")

# 전체 설정 조회
openai_config = config.get_openai_config()
print(f"OpenAI 설정: {openai_config}")
```

### 4.3. 환경 변수 관리 (.env)

보안을 위해 API 키는 환경 변수로 관리할 수 있습니다.

**파일**: `.env`

```
OPENAI_API_KEY=your-openai-api-key-here
```

**Python 코드**:

```python
import os
from dotenv import load_dotenv

# .env 파일 로드
load_dotenv()

# 환경 변수에서 API 키 가져오기
api_key = os.getenv('OPENAI_API_KEY')

# OpenAI 클라이언트 생성
from openai import OpenAI
client = OpenAI(api_key=api_key)
```

---

## 5. 실전 예시 및 실행 결과

### 5.1. RAG 시스템 통합 예시

```python
from openai import OpenAI
import yaml

class RAGSystem:
    """RAG 시스템 (Retrieval-Augmented Generation)"""

    def __init__(self, config_path: str):
        """
        Args:
            config_path (str): Config 파일 경로
        """
        # Config 로드
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)

        # OpenAI 클라이언트 초기화
        self.client = OpenAI(api_key=config['openai']['api_key'])
        self.model = config['openai']['model']
        self.temperature = config['openai']['temperature']
        self.max_tokens = config['openai']['max_tokens']

    def retrieve_documents(self, query: str, top_k: int = 3):
        """
        문서 검색 (Elasticsearch 등에서)

        Args:
            query (str): 검색 질문
            top_k (int): 반환 문서 수

        Returns:
            list: 검색된 문서 리스트
        """
        # 실제로는 Elasticsearch에서 검색
        # 여기서는 예시 문서 반환
        mock_documents = [
            "스마트팜은 ICT 기술을 활용하여 농작물 생육을 자동으로 제어합니다.",
            "센서를 통해 온도, 습도, CO2 농도를 실시간으로 측정합니다.",
            "AI 기술은 병충해를 조기에 감지하고 예측할 수 있습니다."
        ]
        return mock_documents[:top_k]

    def generate_context(self, documents: list) -> str:
        """
        문서 리스트를 컨텍스트 문자열로 변환

        Args:
            documents (list): 문서 리스트

        Returns:
            str: 컨텍스트 문자열
        """
        context = "다음은 관련 정보입니다:\n\n"
        for i, doc in enumerate(documents):
            context += f"[문서 {i+1}] {doc}\n"
        return context

    def answer_question(self, question: str, top_k: int = 3):
        """
        질문에 답변 생성 (RAG)

        Args:
            question (str): 사용자 질문
            top_k (int): 검색 문서 수

        Returns:
            str: 생성된 답변
        """
        # 1. 문서 검색 (Retrieval)
        documents = self.retrieve_documents(question, top_k=top_k)

        # 2. 컨텍스트 생성
        context = self.generate_context(documents)

        # 3. 프롬프트 구성
        prompt = f"""{context}

위 정보를 바탕으로 다음 질문에 답변해주세요.

질문: {question}

답변:"""

        # 4. LLM 호출 (Generation)
        response = self.client.chat.completions.create(
            model=self.model,
            messages=[
                {"role": "system", "content": "당신은 스마트팜 전문가입니다. 제공된 정보를 바탕으로 정확하게 답변하세요."},
                {"role": "user", "content": prompt}
            ],
            temperature=self.temperature,
            max_tokens=self.max_tokens
        )

        answer = response.choices[0].message.content

        return {
            'question': question,
            'context_documents': documents,
            'answer': answer,
            'tokens_used': response.usage.total_tokens
        }


# RAG 시스템 사용
rag = RAGSystem('config/llm_config.yaml')

# 질문
question = "스마트팜에서 센서의 역할은 무엇인가요?"

# 답변 생성
result = rag.answer_question(question, top_k=3)

# 결과 출력
print(f"질문: {result['question']}\n")
print("참조 문서:")
for i, doc in enumerate(result['context_documents']):
    print(f"  {i+1}. {doc}")

print(f"\n답변:\n{result['answer']}")
print(f"\n사용 토큰: {result['tokens_used']}")
```

**실행 결과**:
```
질문: 스마트팜에서 센서의 역할은 무엇인가요?

참조 문서:
  1. 스마트팜은 ICT 기술을 활용하여 농작물 생육을 자동으로 제어합니다.
  2. 센서를 통해 온도, 습도, CO2 농도를 실시간으로 측정합니다.
  3. AI 기술은 병충해를 조기에 감지하고 예측할 수 있습니다.

답변:
스마트팜에서 센서는 매우 중요한 역할을 합니다. 제공된 정보에 따르면,
센서는 온도, 습도, CO2 농도와 같은 농작물 생육 환경을 실시간으로
측정하는 역할을 합니다.

이렇게 수집된 데이터는 ICT 기술과 결합되어 농작물의 생육 환경을
자동으로 제어하는 데 활용됩니다. 또한 AI 기술과 연계하여 병충해를
조기에 감지하고 예측하는 데에도 활용될 수 있습니다.

결론적으로, 센서는 스마트팜의 '눈'과 '귀' 역할을 하며, 정확한
환경 데이터를 제공하여 최적의 농작물 생육 환경을 유지하는 데
핵심적인 기능을 수행합니다.

사용 토큰: 387
```

### 5.2. 대화형 챗봇 예시

```python
class ChatBot:
    """대화형 챗봇"""

    def __init__(self, api_key: str, model: str = "gpt-3.5-turbo"):
        self.client = OpenAI(api_key=api_key)
        self.model = model
        self.conversation_history = []

    def chat(self, user_message: str) -> str:
        """
        대화

        Args:
            user_message (str): 사용자 메시지

        Returns:
            str: 봇 응답
        """
        # 사용자 메시지 추가
        self.conversation_history.append({
            "role": "user",
            "content": user_message
        })

        # LLM 호출
        response = self.client.chat.completions.create(
            model=self.model,
            messages=[
                {"role": "system", "content": "당신은 친절한 스마트팜 상담사입니다."}
            ] + self.conversation_history
        )

        # 봇 응답 추출
        bot_message = response.choices[0].message.content

        # 대화 기록에 추가
        self.conversation_history.append({
            "role": "assistant",
            "content": bot_message
        })

        return bot_message

    def reset(self):
        """대화 기록 초기화"""
        self.conversation_history = []


# 챗봇 사용
import os
chatbot = ChatBot(
    api_key=os.getenv('OPENAI_API_KEY')  # 환경 변수에서 로드
)

# 대화 시작
print("=== 스마트팜 상담 챗봇 ===\n")

user_input = "스마트팜이 뭔가요?"
print(f"사용자: {user_input}")
response = chatbot.chat(user_input)
print(f"챗봇: {response}\n")

user_input = "도입 비용은 얼마나 드나요?"
print(f"사용자: {user_input}")
response = chatbot.chat(user_input)
print(f"챗봇: {response}\n")
```

**실행 결과**:
```
=== 스마트팜 상담 챗봇 ===

사용자: 스마트팜이 뭔가요?
챗봇: 스마트팜은 ICT(정보통신기술)를 농업에 접목하여 작물의 생육 환경을
자동으로 관리하는 첨단 농업 시스템입니다. 센서, 자동화 장비, 데이터 분석
기술을 활용하여 온도, 습도, 조도, CO2 농도 등을 최적으로 제어합니다.

사용자: 도입 비용은 얼마나 드나요?
챗봇: 스마트팜 도입 비용은 규모와 시스템에 따라 다릅니다. 일반적으로:

- 소규모 (100평 미만): 2,000~5,000만원
- 중규모 (100~500평): 5,000만원~2억원
- 대규모 (500평 이상): 2억원 이상

정부 지원 사업을 활용하면 초기 비용의 50~70%를 지원받을 수 있습니다.
```

---

## 6. 다음 단계

LLM 호출 방법을 익혔다면, 다음은 **프롬프트 엔지니어링**입니다.

**다음 문서**: [1.2.12. 프롬프트 엔지니어링](./1.2.12.프롬프트%20엔지니어링.md)

### 다음에 배울 내용
- 프롬프트 엔지니어링의 정의
- 핵심 개념 (Few-shot, Chain-of-Thought 등)
- 하이퍼파라미터 제어
- 유형별 프롬프트 예시

---

**참고 자료**:
- OpenAI API Documentation: https://platform.openai.com/docs/api-reference
- HuggingFace Transformers: https://huggingface.co/docs/transformers/
- Python dotenv: https://pypi.org/project/python-dotenv/
