# 1.2.09. Keylook API 활용한 인덱싱

> **작성자**: Bern
> **작성일**: 2025-10-25
> **카테고리**: 데이터 수집 / 인덱싱

---

## 목차
1. [Keylook API 소개](#1-keylook-api-소개)
2. [대체 API 가이드](#2-대체-api-가이드)
3. [REST API 기반 인덱싱 구현](#3-rest-api-기반-인덱싱-구현)
4. [실전 예시: 문서 자동 수집](#4-실전-예시-문서-자동-수집)
5. [다음 단계](#5-다음-단계)

---

## 1. Keylook API 소개

### 1.1. Keylook API란?

**Keylook API**는 CNS(농촌진흥청 등) 내부 시스템에서 제공하는 것으로 추정되는 문서 검색/수집 API입니다.

> **참고**: Keylook API는 공개 API가 아닐 수 있으며, 접근 권한이 필요할 수 있습니다.
> 본 가이드에서는 일반적인 REST API 패턴을 기반으로 설명하며, 실제 환경에 맞게 조정이 필요합니다.

### 1.2. 일반적인 API 구조 (추정)

```python
# Keylook API 예상 구조
base_url = "https://api.keylook.cns.go.kr/v1"

endpoints = {
    "search": "/search",          # 문서 검색
    "document": "/document/{id}", # 문서 상세 조회
    "categories": "/categories"   # 카테고리 목록
}
```

---

## 2. 대체 API 가이드

Keylook API를 사용할 수 없는 경우, 다음 대체 API를 활용할 수 있습니다.

### 2.1. 대체 API 옵션

| API | 설명 | 사용 사례 |
|-----|------|----------|
| **공공데이터포털 API** | 대한민국 공공데이터 제공 | 농업 통계, 정책 문서 |
| **국립농업과학원 API** | 농업 연구 데이터 | 농업 기술, 연구 보고서 |
| **Custom REST API** | 자체 구축 API | 사내 문서 시스템 |
| **웹 크롤링** | 웹사이트 데이터 수집 | 공개 웹사이트 |

### 2.2. 공공데이터포털 API 예시

#### 2.2.1. API 키 발급

1. [공공데이터포털](https://www.data.go.kr/) 접속
2. 회원가입 및 로그인
3. 데이터 검색 → 활용 신청
4. API 키 발급 (일반 인증키/디코딩키)

#### 2.2.2. API 호출 예시

```python
import requests

class PublicDataAPI:
    """공공데이터포털 API 클라이언트"""

    def __init__(self, api_key):
        """
        Args:
            api_key (str): 공공데이터포털 API 키
        """
        self.api_key = api_key
        self.base_url = "http://apis.data.go.kr"

    def search_agricultural_data(self, keyword, num_of_rows=10):
        """
        농업 데이터 검색 (예시)

        Args:
            keyword (str): 검색 키워드
            num_of_rows (int): 조회 건수

        Returns:
            dict: API 응답
        """
        endpoint = f"{self.base_url}/service/SampleService/search"

        params = {
            'serviceKey': self.api_key,
            'keyword': keyword,
            'numOfRows': num_of_rows,
            'pageNo': 1,
            '_type': 'json'  # 응답 형식 (json/xml)
        }

        try:
            response = requests.get(endpoint, params=params, timeout=10)
            response.raise_for_status()

            data = response.json()
            return data

        except requests.exceptions.RequestException as e:
            print(f"API 요청 실패: {e}")
            return None


# 사용 예시
# api_key = "YOUR_API_KEY_HERE"
# api_client = PublicDataAPI(api_key)
# results = api_client.search_agricultural_data("스마트팜", num_of_rows=10)
```

### 2.3. 자체 REST API 구현 (Mock)

실습을 위해 Mock API를 구현할 수 있습니다.

```python
from flask import Flask, jsonify, request

app = Flask(__name__)

# 샘플 문서 데이터
MOCK_DOCUMENTS = [
    {
        "id": "doc_001",
        "title": "스마트팜 기술 개요",
        "content": "스마트팜은 ICT 기술을 활용하여 농작물 생육을 최적화합니다.",
        "category": "기술",
        "author": "RDA",
        "created_at": "2024-10-20"
    },
    {
        "id": "doc_002",
        "title": "센서 기술 가이드",
        "content": "온도, 습도, CO2 센서를 활용한 환경 모니터링 방법을 소개합니다.",
        "category": "기술",
        "author": "RDA",
        "created_at": "2024-10-21"
    },
    {
        "id": "doc_003",
        "title": "스마트팜 도입 사례",
        "content": "경기도 A농장의 스마트팜 도입 성공 사례입니다.",
        "category": "사례",
        "author": "RDA",
        "created_at": "2024-10-22"
    }
]


@app.route('/api/v1/search', methods=['GET'])
def search_documents():
    """문서 검색 API"""
    keyword = request.args.get('keyword', '')
    category = request.args.get('category', '')
    limit = int(request.args.get('limit', 10))

    # 필터링
    results = MOCK_DOCUMENTS

    if keyword:
        results = [doc for doc in results
                   if keyword in doc['title'] or keyword in doc['content']]

    if category:
        results = [doc for doc in results if doc['category'] == category]

    # 제한
    results = results[:limit]

    return jsonify({
        "status": "success",
        "count": len(results),
        "data": results
    })


@app.route('/api/v1/document/<doc_id>', methods=['GET'])
def get_document(doc_id):
    """문서 상세 조회 API"""
    doc = next((d for d in MOCK_DOCUMENTS if d['id'] == doc_id), None)

    if doc:
        return jsonify({
            "status": "success",
            "data": doc
        })
    else:
        return jsonify({
            "status": "error",
            "message": "Document not found"
        }), 404


if __name__ == '__main__':
    # Mock API 서버 실행
    # python mock_api.py
    # 접속: http://localhost:5000/api/v1/search?keyword=스마트팜
    app.run(debug=True, port=5000)
```

**Mock API 실행**:
```bash
# 터미널에서 실행
python mock_api.py

# 다른 터미널에서 테스트
curl "http://localhost:5000/api/v1/search?keyword=스마트팜"
```

---

## 3. REST API 기반 인덱싱 구현

### 3.1. API 클라이언트 클래스

```python
import requests
from typing import List, Dict, Optional

class DocumentAPIClient:
    """문서 수집 API 클라이언트"""

    def __init__(self, base_url, api_key=None):
        """
        Args:
            base_url (str): API 베이스 URL
            api_key (str, optional): API 인증 키
        """
        self.base_url = base_url
        self.api_key = api_key
        self.session = requests.Session()

        # API 키가 있으면 헤더에 추가
        if api_key:
            self.session.headers.update({
                'Authorization': f'Bearer {api_key}'
            })

    def search(self, keyword: str, category: Optional[str] = None,
               limit: int = 10) -> List[Dict]:
        """
        문서 검색

        Args:
            keyword (str): 검색 키워드
            category (str, optional): 카테고리 필터
            limit (int): 조회 건수

        Returns:
            list: 문서 리스트
        """
        endpoint = f"{self.base_url}/search"

        params = {
            'keyword': keyword,
            'limit': limit
        }

        if category:
            params['category'] = category

        try:
            response = self.session.get(endpoint, params=params, timeout=10)
            response.raise_for_status()

            data = response.json()
            return data.get('data', [])

        except requests.exceptions.RequestException as e:
            print(f"검색 실패: {e}")
            return []

    def get_document(self, doc_id: str) -> Optional[Dict]:
        """
        문서 상세 조회

        Args:
            doc_id (str): 문서 ID

        Returns:
            dict: 문서 데이터
        """
        endpoint = f"{self.base_url}/document/{doc_id}"

        try:
            response = self.session.get(endpoint, timeout=10)
            response.raise_for_status()

            data = response.json()
            return data.get('data')

        except requests.exceptions.RequestException as e:
            print(f"문서 조회 실패: {e}")
            return None

    def search_all(self, keywords: List[str], category: Optional[str] = None) -> List[Dict]:
        """
        여러 키워드로 검색

        Args:
            keywords (list): 검색 키워드 리스트
            category (str, optional): 카테고리 필터

        Returns:
            list: 중복 제거된 문서 리스트
        """
        all_docs = []
        seen_ids = set()

        for keyword in keywords:
            docs = self.search(keyword, category=category, limit=100)

            for doc in docs:
                doc_id = doc['id']
                if doc_id not in seen_ids:
                    all_docs.append(doc)
                    seen_ids.add(doc_id)

        return all_docs


# 사용 예시
api_client = DocumentAPIClient(
    base_url="http://localhost:5000/api/v1",
    api_key=None  # Mock API는 인증 불필요
)

# 단일 키워드 검색
results = api_client.search("스마트팜", limit=5)
print(f"검색 결과: {len(results)}개")

for doc in results:
    print(f"  - {doc['title']}")

# 여러 키워드 검색
keywords = ["스마트팜", "센서", "AI"]
all_results = api_client.search_all(keywords)
print(f"\n전체 검색 결과: {len(all_results)}개")
```

### 3.2. Elasticsearch 인덱싱 파이프라인

```python
from elasticsearch import Elasticsearch
from elasticsearch.helpers import bulk
from sentence_transformers import SentenceTransformer

class APIToElasticsearchPipeline:
    """API → Elasticsearch 인덱싱 파이프라인"""

    def __init__(self, api_client, es_client, index_name, embedding_model):
        """
        Args:
            api_client: API 클라이언트
            es_client: Elasticsearch 클라이언트
            index_name (str): 인덱스 이름
            embedding_model: 임베딩 모델
        """
        self.api_client = api_client
        self.es = es_client
        self.index_name = index_name
        self.model = embedding_model

    def collect_and_index(self, keywords: List[str], category: Optional[str] = None):
        """
        API에서 문서 수집 후 Elasticsearch에 인덱싱

        Args:
            keywords (list): 검색 키워드 리스트
            category (str, optional): 카테고리 필터

        Returns:
            dict: 인덱싱 결과 통계
        """
        print(f"API에서 문서 수집 중... (키워드: {keywords})")

        # 1. API에서 문서 수집
        documents = self.api_client.search_all(keywords, category=category)
        print(f"수집된 문서: {len(documents)}개")

        if not documents:
            return {"success": 0, "failed": 0}

        # 2. 임베딩 생성
        print("임베딩 생성 중...")
        texts = [f"{doc['title']} {doc['content']}" for doc in documents]
        embeddings = self.model.encode(texts, show_progress_bar=True)

        # 3. Elasticsearch 벌크 인덱싱
        print("Elasticsearch에 저장 중...")

        def generate_docs():
            for i, doc in enumerate(documents):
                yield {
                    "_index": self.index_name,
                    "_id": doc['id'],
                    "_source": {
                        "doc_id": doc['id'],
                        "title": doc['title'],
                        "content": doc['content'],
                        "text": f"{doc['title']} {doc['content']}",  # 검색용
                        "embedding": embeddings[i].tolist(),
                        "metadata": {
                            "category": doc.get('category'),
                            "author": doc.get('author'),
                            "created_at": doc.get('created_at'),
                            "source": "API"
                        }
                    }
                }

        success, failed = bulk(self.es, generate_docs())
        self.es.indices.refresh(index=self.index_name)

        print(f"인덱싱 완료: 성공 {success}개, 실패 {len(failed)}개")

        return {
            "success": success,
            "failed": len(failed),
            "total": len(documents)
        }


# 사용 예시
# Elasticsearch 연결
es = Elasticsearch(['http://localhost:9200'])

# 임베딩 모델 로드
model = SentenceTransformer('jhgan/ko-sroberta-multitask')

# 파이프라인 생성
pipeline = APIToElasticsearchPipeline(
    api_client=api_client,
    es_client=es,
    index_name='api_documents',
    embedding_model=model
)

# 문서 수집 및 인덱싱
keywords = ["스마트팜", "센서", "AI"]
result = pipeline.collect_and_index(keywords)

print(f"\n최종 결과:")
print(f"  총 문서 수: {result['total']}")
print(f"  인덱싱 성공: {result['success']}")
print(f"  인덱싱 실패: {result['failed']}")
```

---

## 4. 실전 예시: 문서 자동 수집

### 4.1. 스케줄링 자동화

```python
import schedule
import time
from datetime import datetime

class AutoIndexingScheduler:
    """자동 인덱싱 스케줄러"""

    def __init__(self, pipeline, keywords, category=None):
        """
        Args:
            pipeline: 인덱싱 파이프라인
            keywords (list): 검색 키워드
            category (str, optional): 카테고리
        """
        self.pipeline = pipeline
        self.keywords = keywords
        self.category = category

    def run_indexing(self):
        """인덱싱 작업 실행"""
        print(f"\n[{datetime.now()}] 자동 인덱싱 시작")

        try:
            result = self.pipeline.collect_and_index(
                keywords=self.keywords,
                category=self.category
            )

            print(f"[{datetime.now()}] 인덱싱 완료: {result['success']}개 성공")

        except Exception as e:
            print(f"[{datetime.now()}] 인덱싱 실패: {e}")

    def start(self, interval_hours=24):
        """
        스케줄러 시작

        Args:
            interval_hours (int): 실행 간격 (시간)
        """
        # 즉시 1회 실행
        self.run_indexing()

        # 주기적 실행 스케줄 설정
        schedule.every(interval_hours).hours.do(self.run_indexing)

        print(f"\n스케줄러 시작: {interval_hours}시간마다 실행")
        print("종료하려면 Ctrl+C를 누르세요.\n")

        # 스케줄 실행 루프
        try:
            while True:
                schedule.run_pending()
                time.sleep(60)  # 1분마다 스케줄 확인

        except KeyboardInterrupt:
            print("\n스케줄러 종료")


# 사용 예시
scheduler = AutoIndexingScheduler(
    pipeline=pipeline,
    keywords=["스마트팜", "센서", "AI"],
    category="기술"
)

# 24시간마다 자동 인덱싱
# scheduler.start(interval_hours=24)
```

### 4.2. 증분 업데이트 (Incremental Update)

```python
class IncrementalIndexer:
    """증분 인덱싱 (변경된 문서만 업데이트)"""

    def __init__(self, pipeline):
        self.pipeline = pipeline
        self.last_indexed_at = None

    def index_new_documents(self, keywords, category=None):
        """
        마지막 인덱싱 이후 새로운 문서만 수집

        Args:
            keywords (list): 검색 키워드
            category (str, optional): 카테고리
        """
        # 1. 모든 문서 수집
        all_docs = self.pipeline.api_client.search_all(keywords, category)

        # 2. 새 문서 필터링 (created_at 기준)
        if self.last_indexed_at:
            new_docs = [
                doc for doc in all_docs
                if doc.get('created_at', '') > self.last_indexed_at
            ]
        else:
            new_docs = all_docs

        print(f"새 문서: {len(new_docs)}개")

        if not new_docs:
            print("업데이트할 문서가 없습니다.")
            return

        # 3. 새 문서만 인덱싱
        # (파이프라인 로직 재사용)
        texts = [f"{doc['title']} {doc['content']}" for doc in new_docs]
        embeddings = self.pipeline.model.encode(texts)

        # Elasticsearch 저장
        def generate_docs():
            for i, doc in enumerate(new_docs):
                yield {
                    "_index": self.pipeline.index_name,
                    "_id": doc['id'],
                    "_source": {
                        "doc_id": doc['id'],
                        "title": doc['title'],
                        "content": doc['content'],
                        "text": f"{doc['title']} {doc['content']}",
                        "embedding": embeddings[i].tolist(),
                        "metadata": {
                            "category": doc.get('category'),
                            "author": doc.get('author'),
                            "created_at": doc.get('created_at'),
                            "source": "API"
                        }
                    }
                }

        from elasticsearch.helpers import bulk
        success, failed = bulk(self.pipeline.es, generate_docs())

        print(f"인덱싱 완료: {success}개")

        # 4. 마지막 인덱싱 시간 업데이트
        self.last_indexed_at = datetime.now().isoformat()


# 사용 예시
incremental = IncrementalIndexer(pipeline)

# 첫 번째 실행 - 전체 인덱싱
incremental.index_new_documents(["스마트팜"])

# 두 번째 실행 - 새 문서만 인덱싱
# incremental.index_new_documents(["스마트팜"])
```

### 4.3. 모니터링 및 로깅

```python
import logging
from datetime import datetime

# 로깅 설정
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[
        logging.FileHandler('indexing.log'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)


class MonitoredPipeline:
    """모니터링 기능이 추가된 파이프라인"""

    def __init__(self, pipeline):
        self.pipeline = pipeline
        self.stats = {
            'total_runs': 0,
            'total_indexed': 0,
            'total_failed': 0,
            'last_run': None
        }

    def run(self, keywords, category=None):
        """모니터링과 함께 인덱싱 실행"""
        logger.info(f"인덱싱 시작: keywords={keywords}, category={category}")

        try:
            result = self.pipeline.collect_and_index(keywords, category)

            # 통계 업데이트
            self.stats['total_runs'] += 1
            self.stats['total_indexed'] += result['success']
            self.stats['total_failed'] += result['failed']
            self.stats['last_run'] = datetime.now().isoformat()

            logger.info(f"인덱싱 완료: {result['success']}개 성공, {result['failed']}개 실패")

            return result

        except Exception as e:
            logger.error(f"인덱싱 오류: {e}", exc_info=True)
            raise

    def get_stats(self):
        """통계 조회"""
        return self.stats


# 사용 예시
monitored = MonitoredPipeline(pipeline)

# 인덱싱 실행
monitored.run(["스마트팜", "센서"])

# 통계 확인
stats = monitored.get_stats()
print(f"\n=== 인덱싱 통계 ===")
print(f"총 실행 횟수: {stats['total_runs']}")
print(f"총 인덱싱 문서: {stats['total_indexed']}")
print(f"총 실패 문서: {stats['total_failed']}")
print(f"마지막 실행: {stats['last_run']}")
```

---

## 5. 다음 단계

API 기반 인덱싱을 익혔다면, 다음은 **Elasticsearch 기본 활용법**입니다.

**다음 문서**: [1.2.10. Elasticsearch 기본 활용법](./1.2.10.Elasticsearch%20기본%20활용법.md)

### 다음에 배울 내용
- Python에서 Elasticsearch 활용
- CRUD 작업 (생성, 조회, 수정, 삭제)
- 다양한 검색 쿼리
- 집계(Aggregation) 활용
- 실전 예시 코드

---

**참고 자료**:
- 공공데이터포털: https://www.data.go.kr/
- Elasticsearch Python Client: https://elasticsearch-py.readthedocs.io/
- Requests 라이브러리: https://requests.readthedocs.io/
- Schedule 라이브러리: https://schedule.readthedocs.io/
