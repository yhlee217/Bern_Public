# TXT 파일 파서(Parser) 가이드

## 목차

1. [TXT 파싱의 개념](#1-txt-파싱의-개념)
2. [TXT 파일 구조 이해](#2-txt-파일-구조-이해)
3. [Python 내장 함수를 이용한 파싱](#3-python-내장-함수를-이용한-파싱)
4. [정규식을 이용한 고급 파싱](#4-정규식을-이용한-고급-파싱)
5. [구조화된 TXT 파싱 (구분자 기반)](#5-구조화된-txt-파싱-구분자-기반)
6. [대용량 TXT 파일 처리](#6-대용량-txt-파일-처리)
7. [실습 예제](#7-실습-예제)
8. [트러블슈팅](#8-트러블슈팅)
9. [베스트 프랙티스](#9-베스트-프랙티스)

---

## 1. TXT 파싱의 개념

### 📘 정의

**TXT 파싱**은 일반 텍스트 파일(.txt)에서 필요한 정보를 추출하고, 구조화된 데이터로 변환하는 과정입니다.

### 🎯 TXT 파일의 특징

- **인코딩**: UTF-8, CP949, EUC-KR 등 다양한 인코딩 방식 존재
- **구조**: 정형화되지 않은 자유 텍스트 또는 구분자 기반 구조
- **크기**: 수 KB부터 수 GB까지 다양
- **용도**: 로그 파일, 설정 파일, 데이터 덤프, 보고서 등

### 📌 왜 TXT 파싱이 중요한가?

1. **데이터 전처리**: 머신러닝 모델 학습을 위한 데이터 준비
2. **로그 분석**: 시스템 로그, 애플리케이션 로그 분석
3. **레거시 데이터 처리**: 오래된 시스템의 데이터 마이그레이션
4. **설정 파일 관리**: 애플리케이션 설정 값 읽기/쓰기

---

## 2. TXT 파일 구조 이해

### TXT 파일의 주요 유형

| 유형 | 설명 | 예시 |
|------|------|------|
| **자유 텍스트** | 구조화되지 않은 일반 텍스트 | 소설, 에세이, 일반 문서 |
| **라인 기반** | 줄 단위로 의미 있는 데이터 | 로그 파일, 목록 파일 |
| **구분자 기반** | 특정 문자로 필드 구분 | CSV, TSV, 파이프 구분 |
| **고정 폭** | 각 필드가 고정된 너비 | 메인프레임 데이터 |
| **키-값 쌍** | 설정 파일 형식 | .ini, .conf 파일 |
| **계층적 구조** | 들여쓰기로 구조 표현 | YAML 스타일 텍스트 |

### 📋 예시: 다양한 TXT 파일 구조

#### 1. 로그 파일
```text
2025-10-23 14:30:45 [INFO] Application started
2025-10-23 14:30:46 [DEBUG] Loading configuration
2025-10-23 14:30:47 [ERROR] Database connection failed
```

#### 2. 구분자 기반 파일
```text
홍길동|30|서울시 강남구|010-1234-5678
김철수|25|부산시 해운대구|010-2345-6789
이영희|28|대구시 중구|010-3456-7890
```

#### 3. 키-값 설정 파일
```text
database.host=localhost
database.port=5432
database.name=mydb
max_connections=100
```

---

## 3. 필요한 라이브러리 및 설치

### 📦 기본 라이브러리 (Python 내장)

TXT 파일 파싱의 기본 기능은 Python 내장 라이브러리만으로 충분합니다:

- `open()` - 파일 읽기/쓰기
- `re` - 정규식 처리
- `os` - 파일 시스템 작업
- `io` - 입출력 스트림 처리

**별도 설치 불필요!** Python만 설치되어 있으면 바로 사용 가능합니다.

### 📦 선택적 라이브러리 (추가 기능)

더 강력한 기능이 필요한 경우 설치:

```bash
# 인코딩 자동 감지
pip install chardet

# 구조화된 데이터 처리 (CSV, 테이블 등)
pip install pandas

# 병렬 처리 (대용량 파일)
# Python 내장 multiprocessing 사용 가능
```

### 설치 확인

```python
# 기본 라이브러리 확인 (별도 설치 불필요)
import re
import os
print("✅ Python 내장 라이브러리 사용 가능")

# 선택적 라이브러리 확인
try:
    import chardet
    print("✅ chardet 사용 가능 (인코딩 자동 감지)")
except ImportError:
    print("⚠️ chardet 미설치 (선택사항)")

try:
    import pandas
    print("✅ pandas 사용 가능 (데이터 분석)")
except ImportError:
    print("⚠️ pandas 미설치 (선택사항)")
```

### 라이브러리 역할

| 라이브러리 | 용도 | 필수 여부 | 설치 명령 |
|-----------|------|----------|----------|
| **Python 내장** | 기본 파일 처리 | 필수 | - |
| **chardet** | 인코딩 자동 감지 | 선택 | `pip install chardet` |
| **pandas** | 데이터프레임 변환 | 선택 | `pip install pandas` |
| **regex** | 고급 정규식 | 선택 | `pip install regex` |

---

## 4. Python 내장 함수를 이용한 파싱

### 4.1 기본적인 파일 읽기

```python
# 방법 1: 전체 파일을 한 번에 읽기
with open('file.txt', 'r', encoding='utf-8') as f:
    content = f.read()
    print(content)

# 방법 2: 줄 단위로 읽기 (리스트)
with open('file.txt', 'r', encoding='utf-8') as f:
    lines = f.readlines()
    for line in lines:
        print(line.strip())  # 줄바꿈 제거

# 방법 3: 줄 단위 순회 (메모리 효율적)
with open('file.txt', 'r', encoding='utf-8') as f:
    for line in f:
        print(line.strip())
```

### 3.2 인코딩 처리

```python
import chardet

def detect_encoding(file_path):
    """
    파일의 인코딩을 자동으로 감지
    """
    with open(file_path, 'rb') as f:
        raw_data = f.read()
        result = chardet.detect(raw_data)
        return result['encoding']

# 사용 예시
encoding = detect_encoding('file.txt')
print(f"감지된 인코딩: {encoding}")

with open('file.txt', 'r', encoding=encoding) as f:
    content = f.read()
```

### 3.3 라인 필터링

```python
def filter_lines(file_path, keyword):
    """
    특정 키워드를 포함하는 라인만 추출
    """
    filtered_lines = []

    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            if keyword in line:
                filtered_lines.append(line.strip())

    return filtered_lines

# 사용 예시
error_logs = filter_lines('app.log', 'ERROR')
for log in error_logs:
    print(log)
```

---

## 4. 정규식을 이용한 고급 파싱

### 4.1 정규식 기본

```python
import re

# 패턴 매칭 예시
text = "전화번호: 010-1234-5678, 이메일: user@example.com"

# 전화번호 추출
phone_pattern = r'\d{3}-\d{4}-\d{4}'
phone = re.search(phone_pattern, text)
if phone:
    print(f"전화번호: {phone.group()}")

# 이메일 추출
email_pattern = r'[\w\.-]+@[\w\.-]+'
email = re.search(email_pattern, text)
if email:
    print(f"이메일: {email.group()}")
```

### 4.2 로그 파일 파싱

```python
import re
from datetime import datetime

class LogParser:
    """
    정규식 기반 로그 파서
    """

    def __init__(self):
        # 로그 패턴: [날짜] [시간] [레벨] 메시지
        self.pattern = re.compile(
            r'(?P<date>\d{4}-\d{2}-\d{2})\s+'
            r'(?P<time>\d{2}:\d{2}:\d{2})\s+'
            r'\[(?P<level>\w+)\]\s+'
            r'(?P<message>.*)'
        )

    def parse_line(self, line):
        """
        한 줄의 로그를 파싱

        Args:
            line (str): 로그 라인

        Returns:
            dict or None: 파싱된 로그 정보
        """
        match = self.pattern.match(line.strip())

        if match:
            result = match.groupdict()
            # 날짜/시간 변환
            result['timestamp'] = datetime.strptime(
                f"{result['date']} {result['time']}",
                "%Y-%m-%d %H:%M:%S"
            )
            return result

        return None

    def parse_file(self, file_path):
        """
        전체 로그 파일을 파싱

        Args:
            file_path (str): 로그 파일 경로

        Returns:
            list: 파싱된 로그 리스트
        """
        logs = []

        with open(file_path, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                parsed = self.parse_line(line)

                if parsed:
                    parsed['line_number'] = line_num
                    logs.append(parsed)
                else:
                    print(f"경고: {line_num}번째 줄 파싱 실패")

        return logs

# 사용 예시
parser = LogParser()
logs = parser.parse_file('application.log')

# 에러 로그만 필터링
error_logs = [log for log in logs if log['level'] == 'ERROR']
print(f"총 에러 수: {len(error_logs)}")
```

### 4.3 다양한 패턴 추출

```python
import re

class TextExtractor:
    """
    텍스트에서 다양한 패턴을 추출하는 유틸리티
    """

    @staticmethod
    def extract_urls(text):
        """URL 추출"""
        pattern = r'https?://(?:[-\w.]|(?:%[\da-fA-F]{2}))+'
        return re.findall(pattern, text)

    @staticmethod
    def extract_emails(text):
        """이메일 추출"""
        pattern = r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'
        return re.findall(pattern, text)

    @staticmethod
    def extract_phone_numbers(text):
        """한국 전화번호 추출"""
        patterns = [
            r'\d{2,3}-\d{3,4}-\d{4}',  # 02-1234-5678
            r'\d{3}-\d{4}-\d{4}',       # 010-1234-5678
            r'\(\d{2,3}\)\s?\d{3,4}-\d{4}'  # (02) 1234-5678
        ]

        phones = []
        for pattern in patterns:
            phones.extend(re.findall(pattern, text))

        return phones

    @staticmethod
    def extract_dates(text):
        """날짜 추출 (YYYY-MM-DD, YYYY/MM/DD)"""
        pattern = r'\d{4}[-/]\d{2}[-/]\d{2}'
        return re.findall(pattern, text)

    @staticmethod
    def extract_numbers(text):
        """숫자 추출 (정수, 실수)"""
        pattern = r'-?\d+\.?\d*'
        return [float(n) if '.' in n else int(n)
                for n in re.findall(pattern, text)]

# 사용 예시
text = """
문의사항은 support@example.com 으로 연락주세요.
전화: 02-1234-5678 또는 010-9876-5432
웹사이트: https://www.example.com
날짜: 2025-10-23
가격: 15,000원 (할인가: 12,500원)
"""

extractor = TextExtractor()
print("URL:", extractor.extract_urls(text))
print("이메일:", extractor.extract_emails(text))
print("전화번호:", extractor.extract_phone_numbers(text))
print("날짜:", extractor.extract_dates(text))
```

---

## 5. 구조화된 TXT 파싱 (구분자 기반)

### 5.1 CSV 스타일 파싱

```python
import csv

class DelimitedParser:
    """
    구분자 기반 텍스트 파일 파서
    """

    def __init__(self, delimiter='|', has_header=True):
        """
        Args:
            delimiter (str): 필드 구분자
            has_header (bool): 헤더 행 존재 여부
        """
        self.delimiter = delimiter
        self.has_header = has_header

    def parse_file(self, file_path):
        """
        구분자 기반 파일을 딕셔너리 리스트로 변환

        Args:
            file_path (str): 파일 경로

        Returns:
            list: 파싱된 데이터 리스트
        """
        data = []

        with open(file_path, 'r', encoding='utf-8') as f:
            if self.has_header:
                # 헤더 읽기
                header = f.readline().strip().split(self.delimiter)

                # 데이터 읽기
                for line in f:
                    values = line.strip().split(self.delimiter)

                    # 헤더와 값을 매핑
                    row = dict(zip(header, values))
                    data.append(row)
            else:
                # 헤더 없이 리스트로 저장
                for line in f:
                    values = line.strip().split(self.delimiter)
                    data.append(values)

        return data

    def parse_to_dataframe(self, file_path):
        """
        pandas DataFrame으로 변환
        """
        import pandas as pd

        return pd.read_csv(
            file_path,
            delimiter=self.delimiter,
            header=0 if self.has_header else None
        )

# 사용 예시
parser = DelimitedParser(delimiter='|', has_header=True)
data = parser.parse_file('users.txt')

for user in data:
    print(f"이름: {user['이름']}, 나이: {user['나이']}")
```

### 5.2 고정 폭 파싱

```python
class FixedWidthParser:
    """
    고정 폭 텍스트 파일 파서
    """

    def __init__(self, field_specs):
        """
        Args:
            field_specs (list): [(필드명, 시작위치, 끝위치), ...]
        """
        self.field_specs = field_specs

    def parse_line(self, line):
        """
        한 줄을 파싱하여 딕셔너리로 반환

        Args:
            line (str): 텍스트 라인

        Returns:
            dict: 파싱된 필드
        """
        result = {}

        for field_name, start, end in self.field_specs:
            value = line[start:end].strip()
            result[field_name] = value

        return result

    def parse_file(self, file_path, skip_lines=0):
        """
        파일 전체를 파싱

        Args:
            file_path (str): 파일 경로
            skip_lines (int): 건너뛸 줄 수 (헤더 등)

        Returns:
            list: 파싱된 데이터
        """
        data = []

        with open(file_path, 'r', encoding='utf-8') as f:
            # 헤더 건너뛰기
            for _ in range(skip_lines):
                next(f)

            # 데이터 파싱
            for line in f:
                if line.strip():  # 빈 줄 무시
                    parsed = self.parse_line(line)
                    data.append(parsed)

        return data

# 사용 예시
# 형식: 이름(10자), 나이(3자), 주소(20자), 전화번호(13자)
field_specs = [
    ('이름', 0, 10),
    ('나이', 10, 13),
    ('주소', 13, 33),
    ('전화번호', 33, 46)
]

parser = FixedWidthParser(field_specs)
data = parser.parse_file('fixed_width_data.txt', skip_lines=1)

for record in data:
    print(record)
```

---

## 6. 대용량 TXT 파일 처리

### 6.1 청크 단위 처리

```python
def process_large_file_in_chunks(file_path, chunk_size=1000):
    """
    대용량 파일을 청크 단위로 처리

    Args:
        file_path (str): 파일 경로
        chunk_size (int): 한 번에 처리할 라인 수

    Yields:
        list: 라인 청크
    """
    with open(file_path, 'r', encoding='utf-8') as f:
        chunk = []

        for line in f:
            chunk.append(line.strip())

            if len(chunk) >= chunk_size:
                yield chunk
                chunk = []

        # 남은 데이터 처리
        if chunk:
            yield chunk

# 사용 예시
total_lines = 0
for chunk in process_large_file_in_chunks('large_file.txt', chunk_size=1000):
    total_lines += len(chunk)
    # 청크 단위로 처리
    print(f"처리된 라인: {total_lines}")
```

### 6.2 메모리 효율적 파싱

```python
class MemoryEfficientParser:
    """
    메모리 효율적인 대용량 파일 파서
    """

    def __init__(self, file_path):
        self.file_path = file_path

    def parse_with_filter(self, filter_func):
        """
        필터 조건에 맞는 라인만 반환 (제너레이터)

        Args:
            filter_func (callable): 필터 함수

        Yields:
            str: 필터링된 라인
        """
        with open(self.file_path, 'r', encoding='utf-8') as f:
            for line in f:
                if filter_func(line):
                    yield line.strip()

    def count_lines(self):
        """
        전체 라인 수 계산 (메모리 효율적)
        """
        with open(self.file_path, 'r', encoding='utf-8') as f:
            return sum(1 for _ in f)

    def get_sample_lines(self, n=10):
        """
        파일에서 n개의 샘플 라인 추출
        """
        samples = []

        with open(self.file_path, 'r', encoding='utf-8') as f:
            for i, line in enumerate(f):
                if i >= n:
                    break
                samples.append(line.strip())

        return samples

# 사용 예시
parser = MemoryEfficientParser('large_log.txt')

# 에러 로그만 추출
error_filter = lambda line: 'ERROR' in line
for error_line in parser.parse_with_filter(error_filter):
    print(error_line)

# 파일 정보 확인
print(f"전체 라인 수: {parser.count_lines()}")
print(f"샘플 라인:\n" + '\n'.join(parser.get_sample_lines(5)))
```

### 6.3 병렬 처리

```python
from concurrent.futures import ProcessPoolExecutor
import os

def process_chunk(chunk_data):
    """
    청크 데이터를 처리하는 함수

    Args:
        chunk_data (tuple): (청크 번호, 라인 리스트)

    Returns:
        dict: 처리 결과
    """
    chunk_num, lines = chunk_data

    # 예시: 특정 키워드 카운트
    error_count = sum(1 for line in lines if 'ERROR' in line)
    warning_count = sum(1 for line in lines if 'WARNING' in line)

    return {
        'chunk': chunk_num,
        'errors': error_count,
        'warnings': warning_count
    }

def parallel_process_file(file_path, num_workers=4, chunk_size=1000):
    """
    대용량 파일을 병렬로 처리

    Args:
        file_path (str): 파일 경로
        num_workers (int): 워커 프로세스 수
        chunk_size (int): 청크 크기

    Returns:
        list: 처리 결과 리스트
    """
    chunks = []

    # 파일을 청크로 분할
    with open(file_path, 'r', encoding='utf-8') as f:
        chunk = []
        chunk_num = 0

        for line in f:
            chunk.append(line.strip())

            if len(chunk) >= chunk_size:
                chunks.append((chunk_num, chunk))
                chunk = []
                chunk_num += 1

        if chunk:
            chunks.append((chunk_num, chunk))

    # 병렬 처리
    with ProcessPoolExecutor(max_workers=num_workers) as executor:
        results = list(executor.map(process_chunk, chunks))

    return results

# 사용 예시
results = parallel_process_file('large_log.txt', num_workers=4)

total_errors = sum(r['errors'] for r in results)
total_warnings = sum(r['warnings'] for r in results)

print(f"총 에러: {total_errors}, 총 경고: {total_warnings}")
```

---

## 7. 실습 예제

실습 예제는 `실습/txt_parsing/` 디렉토리에서 확인할 수 있습니다:

1. **[기본 TXT 파서](실습/txt_parsing/01_basic_txt_parser.py)**: 파일 읽기 기초
2. **[로그 파서](실습/txt_parsing/02_log_parser.py)**: 정규식 기반 로그 분석
3. **[구분자 파서](실습/txt_parsing/03_delimited_parser.py)**: CSV/TSV 스타일 파싱
4. **[대용량 파서](실습/txt_parsing/04_large_file_parser.py)**: 메모리 효율적 처리
5. **[통합 예제](실습/txt_parsing/05_comprehensive_example.py)**: 실제 데이터 처리 시나리오

### 테스트 데이터

- [샘플 로그 파일](실습/txt_parsing/data/sample_application.log)
- [구분자 기반 데이터](실습/txt_parsing/data/sample_users.txt)
- [고정 폭 데이터](실습/txt_parsing/data/sample_fixed_width.txt)
- [대용량 더미 데이터](실습/txt_parsing/data/generate_large_file.py)

---

## 8. 트러블슈팅

### 8.1 인코딩 문제

#### 문제: UnicodeDecodeError

```python
# ❌ 잘못된 방법
with open('file.txt', 'r') as f:
    content = f.read()  # UnicodeDecodeError 발생 가능

# ✅ 올바른 방법 1: 인코딩 명시
with open('file.txt', 'r', encoding='utf-8') as f:
    content = f.read()

# ✅ 올바른 방법 2: 에러 처리
with open('file.txt', 'r', encoding='utf-8', errors='ignore') as f:
    content = f.read()

# ✅ 올바른 방법 3: 자동 감지
import chardet

with open('file.txt', 'rb') as f:
    raw_data = f.read()
    detected = chardet.detect(raw_data)
    encoding = detected['encoding']

with open('file.txt', 'r', encoding=encoding) as f:
    content = f.read()
```

### 8.2 메모리 부족

#### 문제: 대용량 파일 읽기 시 MemoryError

```python
# ❌ 잘못된 방법
with open('huge_file.txt', 'r') as f:
    lines = f.readlines()  # 전체를 메모리에 로드

# ✅ 올바른 방법: 라인별 처리
with open('huge_file.txt', 'r') as f:
    for line in f:  # 제너레이터로 한 줄씩 읽기
        process_line(line)
```

### 8.3 줄바꿈 문제

#### 문제: Windows/Linux 줄바꿈 차이

```python
# ✅ 해결 방법: universal newlines 사용
with open('file.txt', 'r', newline='') as f:
    content = f.read()  # \r\n, \n 모두 자동 처리
```

### 8.4 빈 줄 처리

```python
# 빈 줄 제거
with open('file.txt', 'r') as f:
    lines = [line.strip() for line in f if line.strip()]

# 빈 줄 유지하면서 처리
with open('file.txt', 'r') as f:
    for line in f:
        if not line.strip():
            print("빈 줄 발견")
        else:
            print(f"내용: {line.strip()}")
```

### 8.5 특수 문자 처리

```python
import unicodedata

def normalize_text(text):
    """
    텍스트 정규화 (특수 문자, 공백 등)
    """
    # NFD 정규화 (한글 자모 분리)
    text = unicodedata.normalize('NFC', text)

    # 제어 문자 제거
    text = ''.join(ch for ch in text if unicodedata.category(ch)[0] != 'C')

    # 연속된 공백을 하나로
    text = ' '.join(text.split())

    return text

# 사용 예시
raw_text = "안녕하세요\u200b\u200b   여러   공백"
clean_text = normalize_text(raw_text)
print(clean_text)  # "안녕하세요 여러 공백"
```

---

## 9. 베스트 프랙티스

### ✅ TXT 파싱 설계 원칙

1. **인코딩 명시**
   ```python
   # 항상 인코딩을 명시하세요
   with open('file.txt', 'r', encoding='utf-8') as f:
       content = f.read()
   ```

2. **컨텍스트 매니저 사용**
   ```python
   # ✅ 권장: 자동으로 파일 닫힘
   with open('file.txt', 'r') as f:
       content = f.read()

   # ❌ 비권장: 수동으로 파일 닫아야 함
   f = open('file.txt', 'r')
   content = f.read()
   f.close()
   ```

3. **에러 처리**
   ```python
   def safe_parse_file(file_path):
       try:
           with open(file_path, 'r', encoding='utf-8') as f:
               return f.read()
       except FileNotFoundError:
           print(f"파일을 찾을 수 없음: {file_path}")
           return None
       except UnicodeDecodeError:
           print("인코딩 오류 발생, CP949로 재시도")
           with open(file_path, 'r', encoding='cp949') as f:
               return f.read()
       except Exception as e:
           print(f"예상치 못한 오류: {e}")
           return None
   ```

4. **메모리 효율성**
   ```python
   # 대용량 파일은 제너레이터 사용
   def read_large_file(file_path):
       with open(file_path, 'r') as f:
           for line in f:
               yield line.strip()
   ```

5. **로깅 추가**
   ```python
   import logging

   logging.basicConfig(level=logging.INFO)
   logger = logging.getLogger(__name__)

   def parse_file_with_logging(file_path):
       logger.info(f"파싱 시작: {file_path}")

       try:
           with open(file_path, 'r') as f:
               lines = f.readlines()

           logger.info(f"파싱 완료: {len(lines)}줄 읽음")
           return lines

       except Exception as e:
           logger.error(f"파싱 실패: {e}")
           return None
   ```

### 🚫 피해야 할 사항

- ❌ 인코딩 미지정으로 인한 UnicodeError
- ❌ 대용량 파일을 전체 메모리에 로드
- ❌ 파일 핸들을 닫지 않고 방치
- ❌ 에러 처리 없이 파싱
- ❌ 정규식 남용으로 인한 성능 저하

### 📈 성능 최적화 팁

| 기법 | 설명 | 적용 시점 |
|------|------|-----------|
| **제너레이터** | 한 줄씩 처리하여 메모리 절약 | 대용량 파일 |
| **청크 단위 처리** | 일정 크기씩 나누어 처리 | 중간 크기 파일 |
| **병렬 처리** | 멀티프로세싱 활용 | CPU 집약적 작업 |
| **정규식 컴파일** | 반복 사용 시 성능 향상 | 패턴이 고정된 경우 |
| **버퍼 크기 조절** | I/O 성능 최적화 | 파일 읽기/쓰기 빈번 |

```python
# 정규식 컴파일 예시
import re

# ❌ 비효율적
for line in lines:
    if re.search(r'\d{3}-\d{4}', line):
        print(line)

# ✅ 효율적
pattern = re.compile(r'\d{3}-\d{4}')
for line in lines:
    if pattern.search(line):
        print(line)
```

---

## ✅ 요약

| 구분 | 내용 |
|------|------|
| **기본 도구** | Python 내장 `open()`, `readline()`, `readlines()` |
| **고급 도구** | `re` (정규식), `chardet` (인코딩 감지), `pandas` (구조화 데이터) |
| **주요 기법** | 라인별 처리, 정규식 패턴 매칭, 구분자 기반 파싱 |
| **대용량 처리** | 제너레이터, 청크 처리, 병렬 처리 |
| **주의사항** | 인코딩 명시, 메모리 효율성, 에러 처리 |

---

## 📚 참고 자료

### Python 공식 문서
- [Built-in Functions - open()](https://docs.python.org/3/library/functions.html#open)
- [re - Regular expression operations](https://docs.python.org/3/library/re.html)
- [codecs - Codec registry and base classes](https://docs.python.org/3/library/codecs.html)

### 유용한 라이브러리
- **chardet**: 인코딩 자동 감지
- **pandas**: 구조화된 데이터 처리
- **regex**: 고급 정규식 기능
- **mmap**: 메모리 맵 파일 처리

### 온라인 리소스
- [Real Python - Reading and Writing Files](https://realpython.com/read-write-files-python/)
- [Python Regular Expression HOWTO](https://docs.python.org/3/howto/regex.html)

---

## 📌 다음 단계

TXT 파싱을 익혔다면, 다음은 **PDF 파싱**입니다.

**다음 문서**: [1.2.03. 데이터 전처리 - 파서(Parser) 만들기 (PDF)](./1.2.03.데이터%20전처리%20-파서(Parser)%20만들기%20(PDF).md)

---

**문서 버전**: 1.0
**최종 수정일**: 2025-10-23
**작성자**: Bern
