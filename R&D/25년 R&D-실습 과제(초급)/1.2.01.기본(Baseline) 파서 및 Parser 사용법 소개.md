# Baseline Parser 가이드

## 목차

1. [Baseline Parser의 개념](#1-baseline-parser의-개념)
2. [Baseline Parser의 역할](#2-baseline-parser의-역할)
3. [Baseline Parser의 사용법](#3-baseline-parser의-사용법)
4. [분야별 Baseline Parser 예시](#4-분야별-baseline-parser-예시)
5. [실습 예제](#5-실습-예제)
6. [성능 평가 지표](#6-성능-평가-지표)
7. [베스트 프랙티스](#7-베스트-프랙티스)

---

## 1. Baseline Parser의 개념

### 📘 정의

**Baseline Parser**란, 복잡한 모델이나 고급 알고리즘을 적용하기 전에 **"기본 동작이 잘 되는지 확인하기 위한 최소한의 구문 분석기(Parsing 모델)"**를 말합니다.

즉, 연구나 프로젝트 초기에 성능 비교의 **기준점(Baseline)** 역할을 하는 간단한 파서입니다.

### 🎯 핵심 특징

- **단순성**: 복잡한 알고리즘이나 딥러닝 모델을 사용하지 않음
- **재현성**: 동일한 입력에 대해 항상 동일한 결과를 보장
- **투명성**: 내부 동작 방식을 이해하기 쉬움
- **빠른 구현**: 짧은 시간 내에 구현 및 테스트 가능

### 📌 언제 사용하나요?

1. **새로운 프로젝트 시작 시**: 데이터와 문제를 이해하기 위한 첫 단계
2. **모델 성능 비교 시**: 고급 모델의 성능 개선 정도를 측정하는 기준점
3. **디버깅 시**: 데이터 포맷, 전처리 과정의 문제를 빠르게 발견
4. **실험 검증 시**: 복잡한 모델 도입 전 기본 접근법의 한계 파악

---

## 2. Baseline Parser의 역할

| 역할 | 설명 | 예시 |
|------|------|------|
| **비교 기준(Benchmark)** | 새로운 파서(예: Neural Parser, Transformer Parser)의 성능이 실제로 개선된 것인지 비교하기 위한 기준점 | Baseline UAS: 75% → Custom UAS: 92% (17%p 개선) |
| **디버깅 및 검증** | 데이터 포맷, 토큰화, 구문 트리 구조가 정상인지 테스트하기 위한 기본 모델 | 파싱 결과 시각화를 통한 데이터 품질 확인 |
| **단순 구현체** | 최소한의 기능으로 문법 규칙을 적용해 구조를 분석 | 규칙 기반 또는 단순 통계 기반 파싱 |
| **성능 하한선 설정** | 이 정도 성능은 최소한 나와야 한다는 기준 제시 | "랜덤 파서보다는 좋아야 함" |

### 📊 예시: 자연어 처리(문장 구조 파싱)

#### Baseline Parser

- 단순히 **빈도 기반** / **규칙 기반**으로 작동
- 예: "가장 자주 등장하는 품사 전이 패턴"을 기반으로 파싱 트리 구성

#### 고급 Parser와의 비교

이후 성능 향상을 위해 Neural Dependency Parser, BERT 기반 Parser 등을 도입하면 baseline 대비 성능이 얼마나 개선됐는지 비교 가능

```text
Baseline Parser (규칙 기반):     UAS 70%, LAS 65%
Neural Parser (BiLSTM):           UAS 85%, LAS 80%
Transformer Parser (BERT):        UAS 92%, LAS 88%
```

---

## 3. Baseline Parser의 사용법

다음은 **spaCy**나 **Stanza** 같은 NLP 툴을 기준으로, "Baseline Parser"를 잡고 비교 실험하는 흐름입니다.

### 🧪 예시 1: spaCy 기반 Dependency Parsing

#### 1️⃣ Baseline Parser 로드

```python
import spacy

# 영어 기본 파서 (Baseline)
nlp = spacy.load("en_core_web_sm")

doc = nlp("The quick brown fox jumps over the lazy dog.")

# 의존 관계 출력
for token in doc:
    print(f"{token.text:12} {token.dep_:10} {token.head.text}")

# 결과 예시:
# The          det        fox
# quick        amod       fox
# brown        amod       fox
# fox          nsubj      jumps
# jumps        ROOT       jumps
# over         prep       jumps
# the          det        dog
# lazy         amod       dog
# dog          pobj       over
# .            punct      jumps
```

> ✅ 여기서 spaCy의 기본 Dependency Parser가 **baseline 역할**을 합니다.

#### 2️⃣ 커스텀 모델과 비교

```python
# Baseline accuracy 측정
baseline_score = nlp.evaluate(dev_data)

print(f"Baseline UAS: {baseline_score['dep_uas']:.2%}")
print(f"Baseline LAS: {baseline_score['dep_las']:.2%}")

# Fine-tuned parser 로드
nlp_custom = spacy.load("model_finetuned")
custom_score = nlp_custom.evaluate(dev_data)

print(f"Custom UAS: {custom_score['dep_uas']:.2%}")
print(f"Custom LAS: {custom_score['dep_las']:.2%}")

# 개선 폭 계산
improvement_uas = custom_score['dep_uas'] - baseline_score['dep_uas']
print(f"개선율: {improvement_uas:.2%}p")
```

### 🧪 예시 2: 단순 규칙 기반 Parser

```python
import re

class SimpleBaselineParser:
    """
    단순 규칙 기반 Baseline Parser
    - 명사를 주어로 가정
    - 동사를 술어로 가정
    - 나머지는 수식어로 처리
    """

    def __init__(self):
        self.noun_tags = {'NN', 'NNS', 'NNP', 'NNPS'}
        self.verb_tags = {'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'}

    def parse(self, tokens_with_pos):
        """
        Args:
            tokens_with_pos: [(token, pos_tag), ...]
        Returns:
            parsed_structure: dict
        """
        subject = None
        predicate = None
        modifiers = []

        for token, pos in tokens_with_pos:
            if pos in self.noun_tags and subject is None:
                subject = token
            elif pos in self.verb_tags and predicate is None:
                predicate = token
            else:
                modifiers.append(token)

        return {
            'subject': subject,
            'predicate': predicate,
            'modifiers': modifiers
        }

# 사용 예시
parser = SimpleBaselineParser()
sentence = [
    ('The', 'DT'),
    ('cat', 'NN'),
    ('quickly', 'RB'),
    ('ran', 'VBD'),
    ('away', 'RB')
]

result = parser.parse(sentence)
print(result)
# 출력: {'subject': 'cat', 'predicate': 'ran', 'modifiers': ['The', 'quickly', 'away']}
```

---

## 4. 분야별 Baseline Parser 예시

| 분야 | 예시 | Baseline 방식 | 사용 사례 |
|------|------|---------------|-----------|
| **NLP (자연어 처리)** | Dependency Parser, Constituency Parser | 규칙 기반, 빈도 기반, POS 패턴 기반 | 문장 구조 분석, 품사 태깅 |
| **로그 파싱** | Drain, IPLoM, Spell | 정규식 기반, 템플릿 매칭 | 시스템 로그 분석, 이상 탐지 |
| **컴파일러 이론** | Recursive Descent Parser | 문법 규칙(BNF)에 따른 직접 구현 | 프로그래밍 언어 파싱 |
| **데이터 전처리** | JSON/CSV/XML Parser | 구조 검증용 단순 파싱 로직 | 데이터 품질 검증 |
| **웹 크롤링** | HTML Parser (BeautifulSoup) | DOM 트리 기반 파싱 | 웹 데이터 추출 |
| **바이오인포매틱스** | FASTA/GenBank Parser | 파일 포맷 규칙 기반 파싱 | 생물학적 시퀀스 분석 |

### 📋 로그 파싱 예시

```python
import re

class LogBaselineParser:
    """
    단순 정규식 기반 로그 파서
    """

    def __init__(self):
        # 일반적인 로그 패턴: [타임스탬프] [레벨] 메시지
        self.pattern = re.compile(
            r'\[(?P<timestamp>[\d\-\s:]+)\]\s*'
            r'\[(?P<level>\w+)\]\s*'
            r'(?P<message>.*)'
        )

    def parse(self, log_line):
        match = self.pattern.match(log_line)
        if match:
            return match.groupdict()
        return None

# 사용 예시
parser = LogBaselineParser()
log = "[2025-10-19 10:30:45] [ERROR] Database connection failed"
result = parser.parse(log)
print(result)
# 출력: {'timestamp': '2025-10-19 10:30:45', 'level': 'ERROR',
#       'message': 'Database connection failed'}
```

---

## 5. 실습 예제

실습 예제는 `실습/` 디렉토리에서 확인할 수 있습니다:

1. **[기본 파서 예제](실습/01_simple_parser.py)**: 단순 규칙 기반 문장 파서
2. **[로그 파서 예제](실습/02_log_parser.py)**: 정규식 기반 로그 분석
3. **[JSON 파서 예제](실습/03_json_parser.py)**: 데이터 검증용 파서
4. **[성능 비교 예제](실습/04_performance_comparison.py)**: Baseline vs Advanced Parser

### 테스트 데이터

- [샘플 문장 데이터](실습/data/sample_sentences.txt)
- [샘플 로그 데이터](실습/data/sample_logs.txt)
- [샘플 JSON 데이터](실습/data/sample_data.json)

---

## 6. 성능 평가 지표

### NLP Parser 평가 지표

| 지표 | 설명 | 계산 방법 |
|------|------|-----------|
| **UAS (Unlabeled Attachment Score)** | 의존 관계의 head를 올바르게 예측한 비율 | (올바른 head 수) / (전체 토큰 수) |
| **LAS (Labeled Attachment Score)** | head와 관계 레이블을 모두 올바르게 예측한 비율 | (올바른 head + label 수) / (전체 토큰 수) |
| **Precision** | 예측한 구조 중 올바른 비율 | TP / (TP + FP) |
| **Recall** | 실제 구조 중 올바르게 예측한 비율 | TP / (TP + FN) |
| **F1-Score** | Precision과 Recall의 조화 평균 | 2 × (P × R) / (P + R) |

### 로그 Parser 평가 지표

| 지표 | 설명 |
|------|------|
| **Parsing Accuracy** | 정확하게 파싱된 로그의 비율 |
| **Template Coverage** | 추출된 템플릿이 전체 로그를 얼마나 커버하는지 |
| **Runtime Performance** | 로그 라인당 평균 파싱 시간 |

---

## 7. 베스트 프랙티스

### ✅ Baseline Parser 설계 원칙

1. **단순하게 시작하기**
   - 가장 간단한 규칙부터 적용
   - 복잡한 예외 처리는 나중에 추가

2. **재현 가능성 보장**
   - 랜덤 요소 제거 (시드 고정)
   - 버전 관리 (라이브러리 버전 명시)

3. **명확한 문서화**
   - 어떤 규칙을 사용했는지 기록
   - 가정과 제약사항 명시

4. **빠른 실행 속도**
   - Baseline은 빠르게 실행되어야 함
   - 복잡한 연산은 최소화

5. **해석 가능성**
   - 왜 이런 결과가 나왔는지 설명 가능
   - 오류 분석이 용이한 구조

### 🚫 피해야 할 사항

- ❌ 너무 복잡한 Baseline 만들기
- ❌ 데이터에 과적합된 규칙 사용
- ❌ 실행 시간이 오래 걸리는 Baseline
- ❌ 블랙박스 모델을 Baseline으로 사용

### 📈 Baseline에서 고급 모델로의 발전 경로

```text
1. Simple Rule-based Parser (Baseline)
   ↓
2. Statistical Parser (빈도 기반)
   ↓
3. Machine Learning Parser (SVM, CRF)
   ↓
4. Neural Parser (LSTM, BiLSTM)
   ↓
5. Transformer-based Parser (BERT, GPT)
```

---

## ✅ 요약

| 구분 | 내용 |
|------|------|
| **정의** | 고급 모델과 비교하기 위한 "기본 구문 분석기" |
| **목적** | 성능 비교의 기준점(Baseline) 제공 |
| **특징** | 규칙 기반, 빈도 기반, 단순 통계 기반 등 최소 구현 |
| **장점** | 빠른 구현, 높은 해석 가능성, 재현성 보장 |
| **단점** | 낮은 성능, 제한적인 일반화 능력 |
| **활용 분야** | NLP, 로그 파싱, 코드 분석, 데이터 전처리 등 |
| **평가 지표** | UAS, LAS, F1-Score, Accuracy 등 |

---

## 📚 참고 자료

### 논문 및 서적

- Manning, C. D., & Schütze, H. (1999). *Foundations of Statistical Natural Language Processing*
- Jurafsky, D., & Martin, J. H. (2023). *Speech and Language Processing* (3rd ed.)

### 온라인 리소스

- [spaCy Documentation](https://spacy.io/usage/linguistic-features#dependency-parse)
- [Stanford NLP Group - Dependency Parsing](https://nlp.stanford.edu/software/nndep.html)
- [Universal Dependencies](https://universaldependencies.org/)

### 오픈소스 라이브러리

- **spaCy**: 산업용 NLP 라이브러리
- **NLTK**: 교육용 NLP 툴킷
- **Stanza**: Stanford NLP 그룹의 Python 라이브러리
- **Drain3**: 로그 파싱 라이브러리

---

**문서 버전**: 2.0
**최종 수정일**: 2025-10-19
**작성자**: Bern
