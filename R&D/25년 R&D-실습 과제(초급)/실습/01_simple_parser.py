"""
01. ë‹¨ìˆœ ê·œì¹™ ê¸°ë°˜ ë¬¸ì¥ íŒŒì„œ (Simple Baseline Parser)

ì´ ì˜ˆì œëŠ” NLTKì™€ spaCyë¥¼ ì‚¬ìš©í•œ ê·œì¹™ ê¸°ë°˜ íŒŒì„œë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.
- NLTK: í’ˆì‚¬ íƒœê¹…(POS tagging)ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
- spaCy: ê³ ê¸‰ NLP ê¸°ëŠ¥ì„ ì œê³µí•˜ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬
- ëª…ì‚¬ë¥¼ ì£¼ì–´(Subject)ë¡œ ê°€ì •
- ë™ì‚¬ë¥¼ ìˆ ì–´(Predicate)ë¡œ ê°€ì •
- ë‚˜ë¨¸ì§€ëŠ” ìˆ˜ì‹ì–´(Modifiers)ë¡œ ì²˜ë¦¬

ì„¤ì¹˜ í•„ìš”:
pip install nltk spacy
python -m spacy download en_core_web_sm
"""

import os
import re
from typing import List, Tuple, Dict

try:
    import nltk
    from nltk import pos_tag, word_tokenize
    NLTK_AVAILABLE = True
except ImportError:
    NLTK_AVAILABLE = False
    print("âš ï¸ NLTKê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. 'pip install nltk'ë¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.")

try:
    import spacy
    SPACY_AVAILABLE = True
except ImportError:
    SPACY_AVAILABLE = False
    print("âš ï¸ spaCyê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. 'pip install spacy'ë¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.")


class NLTKParser:
    """
    NLTK ê¸°ë°˜ Baseline Parser

    íŠ¹ì§•:
    - NLTKì˜ í’ˆì‚¬ íƒœê±°ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì¥ êµ¬ì¡° íŒŒì•…
    - ì²« ë²ˆì§¸ ëª…ì‚¬ë¥¼ ì£¼ì–´ë¡œ ê°„ì£¼
    - ì²« ë²ˆì§¸ ë™ì‚¬ë¥¼ ìˆ ì–´ë¡œ ê°„ì£¼
    """

    def __init__(self):
        # Penn Treebank í’ˆì‚¬ íƒœê·¸ ê¸°ì¤€
        self.noun_tags = {'NN', 'NNS', 'NNP', 'NNPS'}
        self.verb_tags = {'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'}
        self.adj_tags = {'JJ', 'JJR', 'JJS'}
        self.adv_tags = {'RB', 'RBR', 'RBS'}

        # NLTK ë°ì´í„° ë‹¤ìš´ë¡œë“œ (ìµœì´ˆ 1íšŒ)
        if NLTK_AVAILABLE:
            try:
                nltk.data.find('tokenizers/punkt')
                nltk.data.find('taggers/averaged_perceptron_tagger')
            except LookupError:
                print("ğŸ“¥ NLTK ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì¤‘...")
                nltk.download('punkt', quiet=True)
                nltk.download('averaged_perceptron_tagger', quiet=True)
                nltk.download('punkt_tab', quiet=True)
                nltk.download('averaged_perceptron_tagger_eng', quiet=True)

    def parse(self, tokens_with_pos: List[Tuple[str, str]]) -> Dict:
        """
        ë¬¸ì¥ì„ íŒŒì‹±í•˜ì—¬ êµ¬ì¡° ì¶”ì¶œ

        Args:
            tokens_with_pos: [(token, pos_tag), ...] í˜•ì‹ì˜ ë¦¬ìŠ¤íŠ¸

        Returns:
            parsed_structure: {
                'subject': str,
                'predicate': str,
                'modifiers': List[str],
                'adjectives': List[str],
                'adverbs': List[str]
            }
        """
        subject = None
        predicate = None
        modifiers = []
        adjectives = []
        adverbs = []

        for token, pos in tokens_with_pos:
            if pos in self.noun_tags and subject is None:
                subject = token
            elif pos in self.verb_tags and predicate is None:
                predicate = token
            elif pos in self.adj_tags:
                adjectives.append(token)
            elif pos in self.adv_tags:
                adverbs.append(token)
            else:
                if token not in ['.', ',', '!', '?']:  # êµ¬ë‘ì  ì œì™¸
                    modifiers.append(token)

        return {
            'subject': subject,
            'predicate': predicate,
            'modifiers': modifiers,
            'adjectives': adjectives,
            'adverbs': adverbs
        }

    def parse_sentence(self, sentence: str) -> Dict:
        """
        NLTKë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì¥ì„ íŒŒì‹±

        Args:
            sentence: íŒŒì‹±í•  ë¬¸ì¥

        Returns:
            parsed_structure: íŒŒì‹±ëœ ë¬¸ì¥ êµ¬ì¡°
        """
        if not NLTK_AVAILABLE:
            print("âš ï¸ NLTKê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return self._fallback_parse(sentence)

        # NLTKë¡œ í† í°í™” ë° í’ˆì‚¬ íƒœê¹…
        tokens = word_tokenize(sentence)
        pos_tags = pos_tag(tokens)

        return self.parse(pos_tags)

    def _fallback_parse(self, sentence: str) -> Dict:
        """
        ê°„ë‹¨í•œ ë¬¸ì¥ íŒŒì‹± (NLTK ì—†ì´ fallback)

        ì´ ë©”ì„œë“œëŠ” ì‹¤ì œ í’ˆì‚¬ íƒœê±° ì—†ì´ ë‹¨ìˆœ íœ´ë¦¬ìŠ¤í‹±ìœ¼ë¡œ ë™ì‘í•©ë‹ˆë‹¤.
        """
        words = sentence.lower().replace('.', '').replace('!', '').replace('?', '').split()

        # ê°„ë‹¨í•œ ë™ì‚¬ ë¦¬ìŠ¤íŠ¸
        common_verbs = {'is', 'are', 'was', 'were', 'run', 'ran', 'jump', 'jumped',
                       'walk', 'walked', 'eat', 'ate', 'sleep', 'slept', 'go', 'went',
                       'make', 'made', 'take', 'took', 'see', 'saw', 'come', 'came'}

        subject = words[0] if words else None
        predicate = None
        modifiers = []

        for word in words[1:]:
            if word in common_verbs and predicate is None:
                predicate = word
            else:
                modifiers.append(word)

        return {
            'subject': subject,
            'predicate': predicate,
            'modifiers': modifiers,
            'adjectives': [],
            'adverbs': []
        }


class SpacyParser:
    """
    spaCy ê¸°ë°˜ Parser

    íŠ¹ì§•:
    - spaCyì˜ ê³ ê¸‰ NLP íŒŒì´í”„ë¼ì¸ ì‚¬ìš©
    - ì˜ì¡´ì„± íŒŒì‹±(Dependency Parsing)ì„ í™œìš©
    - ë¬¸ì¥ì˜ ì£¼ì–´ì™€ ìˆ ì–´ë¥¼ ë” ì •í™•í•˜ê²Œ íŒŒì•…
    """

    def __init__(self, model_name='en_core_web_sm'):
        """
        Args:
            model_name: spaCy ëª¨ë¸ ì´ë¦„ (ê¸°ë³¸ê°’: en_core_web_sm)
        """
        if not SPACY_AVAILABLE:
            print("âš ï¸ spaCyë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            self.nlp = None
            return

        try:
            self.nlp = spacy.load(model_name)
        except OSError:
            print(f"âš ï¸ spaCy ëª¨ë¸ '{model_name}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            print(f"   ë‹¤ìŒ ëª…ë ¹ìœ¼ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”: python -m spacy download {model_name}")
            self.nlp = None

    def parse_sentence(self, sentence: str) -> Dict:
        """
        spaCyë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì¥ì„ íŒŒì‹±

        Args:
            sentence: íŒŒì‹±í•  ë¬¸ì¥

        Returns:
            parsed_structure: íŒŒì‹±ëœ ë¬¸ì¥ êµ¬ì¡°
        """
        if self.nlp is None:
            return {
                'subject': None,
                'predicate': None,
                'modifiers': [],
                'adjectives': [],
                'adverbs': [],
                'dependencies': []
            }

        doc = self.nlp(sentence)

        # ì£¼ì–´ì™€ ìˆ ì–´ ì°¾ê¸° (ì˜ì¡´ì„± íŒŒì‹± í™œìš©)
        subject = None
        predicate = None
        modifiers = []
        adjectives = []
        adverbs = []
        dependencies = []

        # ROOT ë™ì‚¬ ì°¾ê¸° (ì£¼ ë™ì‚¬)
        for token in doc:
            if token.dep_ == 'ROOT':
                predicate = token.text

            # ì£¼ì–´ ì°¾ê¸° (nsubj, nsubjpass)
            if token.dep_ in ('nsubj', 'nsubjpass') and subject is None:
                subject = token.text

            # í˜•ìš©ì‚¬
            if token.pos_ == 'ADJ':
                adjectives.append(token.text)

            # ë¶€ì‚¬
            if token.pos_ == 'ADV':
                adverbs.append(token.text)

            # ê¸°íƒ€ ìˆ˜ì‹ì–´ (ì „ì¹˜ì‚¬, ê´€ì‚¬ ë“±)
            if token.pos_ in ('ADP', 'DET', 'PRON') and token.text.lower() not in ['the', 'a', 'an']:
                modifiers.append(token.text)

            # ì˜ì¡´ì„± ê´€ê³„ ì €ì¥
            dependencies.append({
                'text': token.text,
                'pos': token.pos_,
                'dep': token.dep_,
                'head': token.head.text
            })

        return {
            'subject': subject,
            'predicate': predicate,
            'modifiers': modifiers,
            'adjectives': adjectives,
            'adverbs': adverbs,
            'dependencies': dependencies,
            'entities': [(ent.text, ent.label_) for ent in doc.ents]
        }


def demo_nltk_parser():
    """NLTK íŒŒì„œ ì˜ˆì œ"""
    print("=" * 60)
    print("ì˜ˆì œ 1: NLTKë¥¼ ì‚¬ìš©í•œ íŒŒì‹±")
    print("=" * 60)

    if not NLTK_AVAILABLE:
        print("\nâš ï¸ NLTKê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ ì´ ì˜ˆì œë¥¼ ì‹¤í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("   'pip install nltk'ë¡œ ì„¤ì¹˜ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
        return

    parser = NLTKParser()

    # ì˜ˆì œ ë¬¸ì¥ë“¤
    sentences = [
        "The cat quickly ran away.",
        "A beautiful bird sings sweetly in the tree.",
        "The old man carefully reads a book.",
        "Children play outside.",
        "The dog barks loudly."
    ]

    for i, sentence in enumerate(sentences, 1):
        print(f"\në¬¸ì¥ {i}: {sentence}")
        result = parser.parse_sentence(sentence)
        print(f"  ì£¼ì–´(Subject):      {result['subject']}")
        print(f"  ìˆ ì–´(Predicate):    {result['predicate']}")
        print(f"  í˜•ìš©ì‚¬(Adjectives): {', '.join(result['adjectives']) if result['adjectives'] else 'None'}")
        print(f"  ë¶€ì‚¬(Adverbs):      {', '.join(result['adverbs']) if result['adverbs'] else 'None'}")
        print(f"  ìˆ˜ì‹ì–´(Modifiers):  {', '.join(result['modifiers']) if result['modifiers'] else 'None'}")


def demo_spacy_parser():
    """spaCy íŒŒì„œ ì˜ˆì œ"""
    print("\n" + "=" * 60)
    print("ì˜ˆì œ 2: spaCyë¥¼ ì‚¬ìš©í•œ íŒŒì‹±")
    print("=" * 60)

    if not SPACY_AVAILABLE:
        print("\nâš ï¸ spaCyê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ ì´ ì˜ˆì œë¥¼ ì‹¤í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("   'pip install spacy && python -m spacy download en_core_web_sm'ë¡œ ì„¤ì¹˜ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
        return

    parser = SpacyParser()

    if parser.nlp is None:
        print("\nâš ï¸ spaCy ëª¨ë¸ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    sentences = [
        "The cat quickly ran away.",
        "A beautiful bird sings sweetly in the tree.",
        "The old man carefully reads a book.",
        "Children play outside happily.",
        "The dog barks loudly at strangers."
    ]

    for i, sentence in enumerate(sentences, 1):
        print(f"\në¬¸ì¥ {i}: {sentence}")
        result = parser.parse_sentence(sentence)
        print(f"  ì£¼ì–´(Subject):      {result['subject']}")
        print(f"  ìˆ ì–´(Predicate):    {result['predicate']}")
        print(f"  í˜•ìš©ì‚¬(Adjectives): {', '.join(result['adjectives']) if result['adjectives'] else 'None'}")
        print(f"  ë¶€ì‚¬(Adverbs):      {', '.join(result['adverbs']) if result['adverbs'] else 'None'}")
        print(f"  ìˆ˜ì‹ì–´(Modifiers):  {', '.join(result['modifiers']) if result['modifiers'] else 'None'}")

        # ì—”í‹°í‹°ê°€ ìˆìœ¼ë©´ ì¶œë ¥
        if result.get('entities'):
            entities_str = ', '.join([f"{text}({label})" for text, label in result['entities']])
            print(f"  ê°œì²´ëª…(Entities):   {entities_str}")


def demo_from_file():
    """íŒŒì¼ì—ì„œ ë¬¸ì¥ ì½ì–´ì„œ íŒŒì‹± (NLTK ì‚¬ìš©)"""
    print("\n" + "=" * 60)
    print("ì˜ˆì œ 3: íŒŒì¼ì—ì„œ ë¬¸ì¥ ì½ì–´ì„œ íŒŒì‹± (NLTK)")
    print("=" * 60)

    if not NLTK_AVAILABLE:
        print("\nâš ï¸ NLTKê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ ì´ ì˜ˆì œë¥¼ ì‹¤í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    parser = NLTKParser()

    try:
        # ìŠ¤í¬ë¦½íŠ¸ íŒŒì¼ì´ ìˆëŠ” ë””ë ‰í† ë¦¬ ê¸°ì¤€ìœ¼ë¡œ ê²½ë¡œ ì„¤ì •
        script_dir = os.path.dirname(os.path.abspath(__file__))
        data_file = os.path.join(script_dir, 'data', 'sample_sentences.txt')

        with open(data_file, 'r', encoding='utf-8') as f:
            sentences = [line.strip() for line in f if line.strip()]

        for i, sentence in enumerate(sentences, 1):
            print(f"\në¬¸ì¥ {i}: {sentence}")
            result = parser.parse_sentence(sentence)
            print(f"  ì£¼ì–´(Subject):      {result['subject']}")
            print(f"  ìˆ ì–´(Predicate):    {result['predicate']}")
            print(f"  í˜•ìš©ì‚¬(Adjectives): {', '.join(result['adjectives']) if result['adjectives'] else 'None'}")
            print(f"  ë¶€ì‚¬(Adverbs):      {', '.join(result['adverbs']) if result['adverbs'] else 'None'}")
            print(f"  ìˆ˜ì‹ì–´(Modifiers):  {', '.join(result['modifiers']) if result['modifiers'] else 'None'}")

    except FileNotFoundError:
        print("\nâš ï¸  data/sample_sentences.txt íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("   ë¨¼ì € í…ŒìŠ¤íŠ¸ ë°ì´í„° íŒŒì¼ì„ ìƒì„±í•´ì£¼ì„¸ìš”.")


if __name__ == "__main__":
    print("\n" + "ğŸ§© NLTK & spaCy ê¸°ë°˜ Baseline Parser ì˜ˆì œ")
    print("=" * 60)

    # ì˜ˆì œ ì‹¤í–‰
    demo_nltk_parser()
    demo_spacy_parser()
    demo_from_file()

    print("\n" + "=" * 60)
    print("âœ… ëª¨ë“  ì˜ˆì œ ì‹¤í–‰ ì™„ë£Œ!")
    print("=" * 60)
