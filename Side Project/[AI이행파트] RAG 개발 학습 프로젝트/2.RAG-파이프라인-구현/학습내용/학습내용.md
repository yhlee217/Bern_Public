# 2단계: RAG 파이프라인 구현 - 학습 내용

> **학습 기간**: 2025년 10월
> **소요 시간**: 16시간
> **학습 목표**: RAG 시스템의 핵심 파이프라인 이해 및 구현

---

## 📚 학습 주제

RAG 시스템의 전체 파이프라인을 7단계로 나누어 학습:

```
문서 수집 → Parser → Chunker → Metadata → Embedding → Vector Search → LLM
```

---

## 1. Parser (문서 파싱)

### 학습 내용

#### 1.1 Baseline Parser의 개념
**정의**: 복잡한 파서를 개발하기 전, 비교 기준이 되는 단순한 파서

**역할**:
- 성능 측정의 기준점
- 개선 효과 정량화
- 빠른 프로토타이핑

#### 1.2 Parser 종류

| Parser | 용도 | 라이브러리 |
|--------|------|-----------|
| **TXT Parser** | 텍스트 파일 읽기 | 내장 `open()` |
| **PDF Parser** | PDF 문서 추출 | PyPDF2, pdfplumber |
| **JSON Parser** | JSON 검증 | 내장 `json` |
| **Log Parser** | 로그 분석 | 정규식 `re` |

#### 1.3 PDF 파서 비교

| 라이브러리 | 장점 | 단점 |
|-----------|------|------|
| **PyPDF2** | 빠름, 가벼움 | 복잡한 PDF 처리 약함 |
| **pdfplumber** | 표 추출 우수 | 느림 |
| **PyMuPDF** | 가장 빠름 | 메모리 많이 사용 |

### 학습 포인트
- ✅ Baseline의 중요성 이해
- ✅ 파일 형식별 파싱 방법
- ✅ 성능과 정확도 트레이드오프

---

## 2. Chunker (텍스트 분할)

### 학습 내용

#### 2.1 Chunking이 필요한 이유

1. **LLM 토큰 제한**
   - GPT-4: 최대 8K~128K 토큰
   - 긴 문서는 한 번에 처리 불가

2. **검색 정확도 향상**
   - 작은 청크로 나누면 유사도 검색 정확도 ↑
   - 관련 없는 내용 혼재 방지

3. **처리 비용 절감**
   - 필요한 부분만 검색하여 LLM에 제공

#### 2.2 Chunking 기법 비교

| 기법 | 방법 | 장점 | 단점 |
|-----|------|------|------|
| **Fixed-size** | 고정 크기 분할 | 단순, 빠름 | 문맥 단절 |
| **Rule-based** | 문장/문단 단위 | 문맥 보존 | 청크 크기 불균일 |
| **Structure-aware** | 제목/섹션 인식 | 의미 보존 우수 | 구현 복잡 |
| **Semantic** | 의미 기반 분할 | 최고 품질 | 느림, 복잡 |

#### 2.3 청크 크기 권장사항

| 청크 크기 | 특징 | 적용 사례 |
|---------|------|----------|
| **100-300자** | 정밀 검색 | 짧은 Q&A |
| **300-800자** | 균형잡힘 (권장) | 일반 문서 |
| **800-1500자** | 문맥 중시 | 기술 문서, 논문 |

#### 2.4 Chunk Overlap

**개념**: 청크 간 중복을 두어 문맥 연속성 유지

```
[청크1: 문장1 문장2 문장3]
      [청크2: 문장2 문장3 문장4]
            [청크3: 문장3 문장4 문장5]
```

**오버랩 비율 권장**:
- 10-20%: 일반적 사용
- 30-50%: 문맥 중시
- 0%: 저장 공간 최소화 (비권장)

### 학습 포인트
- ✅ 청크 크기와 오버랩의 중요성
- ✅ 상황별 적합한 청킹 기법 선택
- ✅ 문맥 보존 vs 검색 정확도 균형

---

## 3. Metadata (메타데이터)

### 학습 내용

#### 3.1 Metadata의 역할
- 문서 추적 및 관리
- 필터링 조건 제공
- 품질 평가 기준

#### 3.2 Metadata 구조

```python
{
    'document_id': 'DOC_001',
    'text': '본문 내용...',
    'source': 'report.pdf',
    'category': 'research',
    'created_at': '2025-10-26',
    'char_count': 1523,
    'word_count': 245,
    'language': 'ko',
    'quality_score': 8.5
}
```

#### 3.3 품질 평가 기준

| 기준 | 점수 | 설명 |
|-----|------|------|
| 텍스트 길이 적정 | 2점 | 100-2000자 |
| 특수문자 비율 | 2점 | <5% |
| 단어 수 적정 | 2점 | >20단어 |
| 문장 완결성 | 2점 | 마침표 포함 |
| 숫자만 있지 않음 | 2점 | 의미 있는 텍스트 |

### 학습 포인트
- ✅ 메타데이터의 중요성 이해
- ✅ 품질 점수 계산 방법
- ✅ 필터링 조건 설정

---

## 4. Embedding (벡터화)

### 학습 내용

#### 4.1 Embedding이란?
**정의**: 텍스트를 수치 벡터로 변환하는 과정

**목적**: 컴퓨터가 텍스트의 의미를 계산할 수 있게 함

#### 4.2 Embedding 방법 비교

| 방법 | 특징 | 벡터 차원 | 속도 |
|-----|------|----------|------|
| **TF-IDF** | 통계 기반, 단순 | 수천~수만 | 빠름 |
| **Word2Vec** | 단어 임베딩 | 100-300 | 중간 |
| **Sentence-BERT** | 문장 임베딩 | 384-768 | 느림 |

#### 4.3 TF-IDF 원리

**TF (Term Frequency)**: 단어 빈도
```
TF = (특정 단어 출현 횟수) / (문서 내 전체 단어 수)
```

**IDF (Inverse Document Frequency)**: 역문서 빈도
```
IDF = log(전체 문서 수 / 해당 단어를 포함한 문서 수)
```

**TF-IDF**:
```
TF-IDF = TF × IDF
```

**특징**:
- 자주 나오지만 흔한 단어(예: 조사)는 낮은 점수
- 특정 문서에만 나오는 단어는 높은 점수

#### 4.4 Sentence Transformer (고급)

**모델**: `jhgan/ko-sroberta-multitask` (한국어)

**장점**:
- 문장 전체의 의미 포착
- 문맥 이해
- Pre-trained 모델 활용

### 학습 포인트
- ✅ 임베딩의 개념과 필요성
- ✅ TF-IDF vs Transformer 차이
- ✅ 벡터 차원과 성능 관계

---

## 5. Vector Search (벡터 검색)

### 학습 내용

#### 5.1 유사도 측정 방법

| 방법 | 수식 | 특징 |
|-----|------|------|
| **Cosine Similarity** | `cos(θ) = A·B / (‖A‖‖B‖)` | 방향 중시, -1~1 |
| **Euclidean Distance** | `√Σ(ai-bi)²` | 거리 중시, 0~∞ |
| **Dot Product** | `Σ(ai×bi)` | 빠름, 크기 영향 |

#### 5.2 Cosine Similarity 상세

**수식**:
```
similarity = (vec1 · vec2) / (||vec1|| × ||vec2||)
```

**값의 의미**:
- 1.0: 완전히 같음
- 0.0: 직교 (관련 없음)
- -1.0: 완전히 반대

**장점**:
- 벡터 크기에 영향 받지 않음
- 텍스트 유사도에 적합

#### 5.3 Top-K 검색

**개념**: 유사도가 가장 높은 K개 문서 검색

**구현**:
```python
1. 쿼리 벡터화
2. 모든 문서 벡터와 유사도 계산
3. 유사도 순 정렬
4. 상위 K개 반환
```

**최적화**:
- FAISS: Facebook의 벡터 검색 라이브러리
- Approximate Nearest Neighbors (ANN)

### 학습 포인트
- ✅ 유사도 측정 방법 이해
- ✅ Cosine Similarity 선택 이유
- ✅ Top-K 검색 알고리즘

---

## 6. Elasticsearch 연동

### 학습 내용

#### 6.1 Elasticsearch Vector Search

**dense_vector 타입**:
```json
{
  "mappings": {
    "properties": {
      "title": { "type": "text" },
      "vector": {
        "type": "dense_vector",
        "dims": 768
      }
    }
  }
}
```

#### 6.2 하이브리드 검색

**Keyword 검색 + Vector 검색**:
```json
{
  "query": {
    "bool": {
      "should": [
        { "match": { "title": "RAG 시스템" } },
        {
          "script_score": {
            "query": { "match_all": {} },
            "script": {
              "source": "cosineSimilarity(params.query_vector, 'vector') + 1.0",
              "params": { "query_vector": [0.1, 0.2, ...] }
            }
          }
        }
      ]
    }
  }
}
```

### 학습 포인트
- ✅ Elasticsearch의 Vector 지원
- ✅ 하이브리드 검색의 장점
- ✅ Python 클라이언트 사용법

---

## 7. LLM 호출

### 학습 내용

#### 7.1 OpenAI API 사용법

**기본 호출**:
```python
from openai import OpenAI

client = OpenAI(api_key="...")

response = client.chat.completions.create(
    model="gpt-3.5-turbo",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "What is RAG?"}
    ]
)
```

#### 7.2 Prompt Engineering 기법

| 기법 | 설명 | 예시 |
|-----|------|------|
| **Few-shot** | 예시 제공 | "Q: ... A: ..." 형식 |
| **Chain-of-Thought** | 사고 과정 유도 | "단계별로 생각해봅시다" |
| **Role Prompting** | 역할 부여 | "당신은 전문가입니다" |

#### 7.3 Temperature 파라미터

| 값 | 특성 | 용도 |
|----|------|------|
| **0.0** | 결정적, 일관성 | 정확한 답변 필요 시 |
| **0.7** | 균형 (기본값) | 일반적 사용 |
| **1.0** | 창의적, 다양성 | 창작, 브레인스토밍 |

### 학습 포인트
- ✅ OpenAI API 구조 이해
- ✅ Prompt Engineering 기법
- ✅ Temperature 활용법

---

## 💡 RAG 파이프라인 전체 흐름

### 통합 프로세스
```
1. 문서 수집
   ↓ (Parser)
2. 텍스트 추출: PDF → 텍스트
   ↓ (Chunker)
3. 텍스트 분할: 큰 문서 → 작은 청크
   ↓ (Metadata)
4. 메타데이터 추가: ID, 출처, 품질 점수
   ↓ (Embedding)
5. 벡터화: 텍스트 → 숫자 벡터
   ↓ (Elasticsearch)
6. 저장: Vector DB에 인덱싱
   ↓
7. 검색 요청
   ↓ (Vector Search)
8. 유사도 검색: Top-K 문서 검색
   ↓ (LLM)
9. 응답 생성: 검색 결과 + LLM → 최종 답변
```

### 실무 고려사항

1. **청크 크기**: 500자 + 50자 오버랩 (권장)
2. **임베딩 모델**: 한국어는 `jhgan/ko-sroberta-multitask`
3. **검색**: Top-5 검색 후 재랭킹
4. **LLM**: Temperature 0.7, max_tokens 500

---

## 📖 참고 자료

### 내부 문서 (R&D 폴더)
- 1.2.01 ~ 1.2.12: RAG 파이프라인 상세 가이드

### 외부 문서
- [Elasticsearch Vector Search](https://www.elastic.co/guide/en/elasticsearch/reference/current/dense-vector.html)
- [OpenAI API Documentation](https://platform.openai.com/docs/api-reference)
- [Sentence Transformers](https://www.sbert.net/)

---

**작성일**: 2025년 10월 26일
**학습 완료**: ✅
